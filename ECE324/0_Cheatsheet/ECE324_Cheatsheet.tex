\documentclass{article}
\usepackage{style}
\title{ECE324 Lectures}
\author{Hanhee Lee}
\lhead{ECE324}
\rhead{Hanhee Lee}

\begin{document}
\section{Problem 1: Description of the Problem, Subscribe a ML/NN Based Solution}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 2: Prescribe a strategy to optimize the NN.}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 3: Explain step by step inference for basic algorithms (MLP, CNN, GNN, attention mechanism) in terms of numpy or basic tensor operations.}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 4: Be able to explain why such a solution might work or fail.} 
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}
\newpage

\section{ML Concepts}
\subsection{Bias-Variance Tradeoff}
\begin{definition}
    \begin{itemize}
        \item \textbf{Bias (Underfitting):} Error due to overly simplistic assumptions in the model.
        \begin{itemize}
            \item A model with high bias makes strong assumptions about the data, leading to oversimplification.
            \item This results in poor training and test performance because the model fails to capture important patterns.
        \end{itemize}
        \item \textbf{Variance (Overfitting):} Error due to the model's sensitivity to fluctuations in the training data.
        \begin{itemize}
            \item A model with high variance is too complex and captures noise in the training data, rather than just the underlying pattern.
            \item This results in excellent training performance but poor generalization to new data.
        \end{itemize}
        \item \textbf{Low Bias, High Variance:} Overly complex models (e.g., deep neural networks with excessive parameters) suffer from overfitting, performing well on training data but poorly on test data.
        \item \textbf{High Bias, Low Variance:} Overly simplistic models (e.g., linear regression) suffer from underfitting, performing poorly on both training and test data.
    \end{itemize}
\end{definition}

\subsubsection{Solution to High Variance}
\begin{definition}
    \begin{itemize}
        \item Regularization
        \item Data Augmentation
        \item More trianing data 
        \item Simpler models
        \item Ensemble methods
    \end{itemize}
\end{definition}

\subsubsection{Solution to High Bias}
\begin{definition}
    \begin{itemize}
        \item More complex models
        \item Feature engineering
        \item Hyperparameter tuning
        \item Reduce regularization
    \end{itemize}
\end{definition}
\newpage

\section{Loss Functions}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Loss Fn} & \textbf{Equation} \\ 
            \toprule
            \textbf{Mean Squared Error (MSE)} & $\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Penalizes large errors.
            \end{itemize}} \\
            \midrule
            \textbf{Root Mean Squared Error (RMSE)} & $\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Error needs to be in the same units as the target.
            \end{itemize}} \\
            \midrule
            \textbf{Mean Absolute Error (MAE)} & $\frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Equal weighting of all errors (robust to outliers).
            \end{itemize}} \\
            \midrule
            \textbf{Binary Cross Entropy} & $- \frac{1}{n} \sum_{i=1}^{n} y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Binary classification problems.
            \end{itemize}} \\
            \midrule
            \textbf{Categorical Cross Entropy} & $- \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} y_{ij} \log(\hat{y}_{ij})$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Multi-class classification problems.
            \end{itemize}} \\
            \midrule
            \textbf{AE: Reconstruction Error} & Minimize reconstruction error. \\
            & $L_{AE} = \left\| \mathbf{x} - f_{\text{dec}}(f_{\text{enc}}(\mathbf{x})) \right\|^2 = \left\| \mathbf{x} - \hat{\mathbf{x}} \right\|^2$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item If our data is binary, what loss function should you use? Binary cross entropy.
            \end{itemize}} \\
            \midrule
            \textbf{VAE: Reconstruction and KL Divergence} & Balances reconstruction and latent space regularization. \\
            &  $L_{\text{VAE}} = L_{\text{AE}} (x, \hat{x}) + \text{KL} (q_\phi (\mathbf{z} \mid \mathbf{x}) \parallel p(\mathbf{z}))$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item For Gaussian Distribution: $KL(P \| Q) = KL\left(\mathcal{N}(\mu_1, \sigma_1^2) \| \mathcal{N}(\mu_2, \sigma_2^2)\right) = \frac{1}{2} \left[ \log\left(\frac{\sigma_2^2}{\sigma_1^2}\right) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{\sigma_2^2} - 1 \right]$ 
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\section{Algorithms}
\begin{summary}
    \begin{center}
        \begin{tabular}{llll}
            \toprule
            \textbf{Algorithm} & \textbf{Inputs} & \textbf{Outputs} & \textbf{Equations} \\
            \toprule
            PCA & $x$ & $x'$ & $x \underset{\text{Encode}}{\mapsto} z \underset{\text{Decode}}{\mapsto} x'$ \\
            & & & $\text{Encoder}(x) = Wx$, $\text{Decoder}(z) = W^{-1}z$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Overview:} Linear transformation for dimension reduction 
            \end{itemize}} \\
            \midrule
            AE & $x$ & $x'$ & $x \underset{\text{Encode}}{\mapsto} z \underset{\text{Decode}}{\mapsto} x'$ \\
            & & & $f_{\text{enc}}(x), \; \text{Encoder}(x) = \text{Neural network}$, $f_{\text{dec}}(z), \; \text{Decoder}(z) = \text{Neural network}$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Bottleneck (latent space) forces network to learn compressed representation
                \item Encoder maps input to lower dimension, decoder reconstructs input
            \end{itemize}} \\
            \midrule
            VAE & $x$ & $x'$ & $x \underset{\text{Encode}}{\mapsto} z \underset{\text{Decode}}{\mapsto} x'$ \\
            & & & $f_{\text{enc}, \phi}(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mathbf{z} \mid \boldsymbol{\mu}(\mathbf{x}), \sigma^2(\mathbf{x}) \mathbf{I})$, $f_{\text{dec}}(\mathbf{z})$\\ 
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Overview:} A probabilistic approach by learning smooth latent spaces by modelling probability distributions.
                \item $\boldsymbol{\mu}(\mathbf{x})$: Mean of the distribution, $\sigma^2(\mathbf{x})$: Variance of the distribution (independent)
                \item Encoder maps input to a distribution, decoder samples from it.
                \begin{itemize}
                    \item Encoder estimates parameters for a distribution in the latent space
                    \item Decoder samples from the distribution to generate an output sample
                \end{itemize}
                \item \textbf{Key:} Works on any data.
            \end{itemize}} \\
            \midrule
            RNN & $x_t,h_{t-1}$ & $y_{t},h_{t}$ & $h_t = \text{tanh}(\text{Linear} (h_{t-1}) + \text{Linear}(x_t))$ \\ 
            & & & $y_t = \text{MLP}(h_t)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $x_t$: Input, $h_t$: Hidden state, $y_t$: Output
            \end{itemize}} \\
            \midrule
            GRU & $x_t,h_{t-1}$ & $y_t,h_t$ & $z_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $r_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $\tilde{h}(t) = \text{tanh}(\text{Linear}(x_t) + \text{Linear}(r_t \odot h_{t-1}))$ \\
            & & & $h_t = (1-z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $z_t$: Update gate, $r_t$: Reset gate
                \item $h_t$: Hidden state
            \end{itemize}} \\
            \midrule
            LSTM & $x_t,h_{t-1}$ & $h_t,c_t$ & $f_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $i_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $o_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $\tilde{c}_t = \text{tanh}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$ \\
            & & & $h_t = o_t \odot \text{tanh}(c_t)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $f_t$: Forget gate, $i_t$: Input/Update gate, $o_t$: Output gate
                \item $h_t$: Hidden state, $c_t$: Cell state
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}

\subsection{Geometric DL Blueprint for NNs}
\begin{summary}
    Unify various networks around symmetry. 
    ADD IMAGE
\end{summary}
\newpage

\section{Intro to ML}
% \input{Lectures/L2.tex}
\newpage

\section{MLP}
\input{Lectures/L3.tex}
\newpage

\section{Neural Network Engineering}
\input{Lectures/L4.tex}
\newpage

\section{Hyperparameter Optimization}
\input{Lectures/L5.tex}
\newpage

\section{Representations and VAE}
\input{Lectures/L6.tex}
\newpage

\section{Software Development}
\input{Lectures/L7_8.tex}
\newpage

\section{Symmetries, Tabular Data, Sets}
\input{Lectures/L9.tex}
\newpage

\section{CNN}
\input{Lectures/L10.tex}
\newpage

\section{RNN}
\input{Lectures/L11.tex}
\newpage

\section{GNN}
\input{Lectures/L12.tex}

\end{document}