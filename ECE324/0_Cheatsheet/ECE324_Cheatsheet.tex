\documentclass{article}
\usepackage{style}
\title{ECE324 Cheatsheet}
\author{Hanhee Lee}
\lhead{ECE324}
\rhead{Hanhee Lee}

\begin{document}

\section{Problem 1: Description of the Problem, Subscribe a ML/NN Based Solution}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 2: Prescribe a strategy to optimize the NN.}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 3: Explain step by step inference for basic algorithms (MLP, CNN, GNN, attention mechanism) in terms of numpy or basic tensor operations.}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 4: Be able to explain why such a solution might work or fail.} 
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}
\newpage

\begin{center}
    \section*{L2: A tour of learning algorithms, representations, and neural networks}
\end{center}
\input{Lectures/L2.tex}
\newpage

\begin{center}
    \section*{L3: Multilayer Perceptrons (MLPs)}
\end{center}
\input{Lectures/L3.tex}
\newpage

\begin{center}
    \section*{L4: Neural Network Engineering}
\end{center}
\input{Lectures/L4.tex}
\newpage

\begin{center}
    \section*{L5 Optimizing Hyperparameters}
\end{center}
\input{Lectures/L5.tex}
\newpage

\begin{center}
    \section*{L6: Representation Learning and Variational Autoencoders}
\end{center}
\input{Lectures/L6.tex}
\newpage

% \subsection{Recap}
% \begin{summary}
%     \begin{itemize}
%         \item What is a MLP? Vector-in vector-out optimizable, learnable transformation of data.
%         \item What is an inductive bias and why might they be useful? Set of assumptions that the learner puts on a model for a task, makes an algorithm learn one pattern over another.
%         \begin{itemize}
%             \item Let certain patterns be learnable (restricting the hypothesis space).
%             \item Last layer in the GLM: Restricting output values to 0 and 1. 
%         \end{itemize}
%         \item What is the difference between hyperparameters and parameters? Hyperparameters are set before training (usually discrete), parameters are learned during training (continuous to learn).
%         \item How do we optimize all parameters in a model? Back propagation.
%     \end{itemize}
% \end{summary}

\begin{center}
    \section*{L7: Software Developemnt, Python Programming}
\end{center}
\input{Lectures/L7.tex}
\newpage

\begin{center}
    \section*{L8: Code to Paper}
\end{center}
\input{Lectures/L8.tex}
\newpage

No order and unique elements
\begin{itemize}
    \item No order: Permutation invariance 
    \item Algorithm: Deep sets (MLP)
\end{itemize}

MLPs seperate between linear and non-linear, pooling

Symmetry in the data. 

MLPs are equivariant symmetry operations

Geometric DL Blueprint

What's special about image data? Rotation, translation, scaling. 

\begin{summary}
    \begin{itemize}
        \item Invariance: Enforce symmetry
        \item Equivariance: 
    \end{itemize}
\end{summary}


\begin{center}
    \section*{L10: Convolutional Neural Networks}
\end{center}

\begin{summary}
    \begin{itemize}
        \item Deep Set: MLP $\rightarrow$ Pool $\rightarrow$ MLP.
    \end{itemize}
\end{summary}

\begin{example}
    \textbf{Given:}
    \textbf{Soln.}
    \begin{enumerate}
        \item Define the function w/ 3 arguments.
        \item Set size: Colllection of data. 
    \end{enumerate}

\end{example}



\end{document}