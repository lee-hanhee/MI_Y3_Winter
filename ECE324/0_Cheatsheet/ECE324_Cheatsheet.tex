\documentclass{article}
\usepackage{style}
\title{ECE324 Lectures}
\author{Hanhee Lee}
\lhead{ECE324}
\rhead{Hanhee Lee}

\begin{document}
\section{Problem 1: Description of the Problem, Subscribe a ML/NN Based Solution}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 2: Prescribe a strategy to optimize the NN.}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 3: Explain step by step inference for basic algorithms (MLP, CNN, GNN, attention mechanism) in terms of numpy or basic tensor operations.}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 4: Be able to explain why such a solution might work or fail.} 
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}
\newpage

\section{Loss Functions}
\begin{summary}
\end{summary}

\section{Algorithms}
\begin{summary}
    \begin{center}
        \begin{tabular}{llll}
            \toprule
            \textbf{Algorithm} & \textbf{Inputs} & \textbf{Outputs} & \textbf{Equations} \\
            \toprule
            RNN & $x_t,h_{t-1}$ & $y_{t},h_{t}$ & $h_t = \text{tanh}(\text{Linear} (h_{t-1}) + \text{Linear}(x_t))$ \\ 
            & & & $y_t = \text{MLP}(h_t)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $x_t$: Input, $h_t$: Hidden state, $y_t$: Output
            \end{itemize}} \\
            \midrule
            GRU & $x_t,h_{t-1}$ & $y_t,h_t$ & $z_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $r_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $\tilde{h}(t) = \text{tanh}(\text{Linear}(x_t) + \text{Linear}(r_t \odot h_{t-1}))$ \\
            & & & $h_t = (1-z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $z_t$: Update gate, $r_t$: Reset gate
                \item $h_t$: Hidden state
            \end{itemize}} \\
            \midrule
            LSTM & $x_t,h_{t-1}$ & $h_t,c_t$ & $f_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $i_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $o_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $\tilde{c}_t = \text{tanh}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$ \\
            & & & $h_t = o_t \odot \text{tanh}(c_t)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $f_t$: Forget gate, $i_t$: Input/Update gate, $o_t$: Output gate
                \item $h_t$: Hidden state, $c_t$: Cell state
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}

\subsection{Geometric DL Blueprint for NNs}
\begin{summary}
    Unify various networks around symmetry. 
    ADD IMAGE
\end{summary}
\newpage

\section{Intro to ML}
\input{Lectures/L2.tex}
\newpage

\section{Neural Network Engineering}
\input{Lectures/L4.tex}
\newpage

\section{Hyperparameter Optimization}
\input{Lectures/L5.tex}
\newpage

\section{Representations and VAE}
\input{Lectures/L6.tex}
\newpage

\section{Software Development}
\input{Lectures/L7.tex}
\newpage

\section{Code to Paper}
\input{Lectures/L8.tex}
\newpage

\section{Symmetries, Tabular Data, Sets}
\input{Lectures/L9.tex}
\newpage

\section{MLP}
\input{Lectures/L3.tex}
\newpage

\section{CNN}
\input{Lectures/L10.tex}
\newpage

\section{RNN}
\input{Lectures/L11.tex}
\newpage

\section{GNN}
\input{Lectures/L12.tex}

\end{document}