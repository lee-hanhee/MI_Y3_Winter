\documentclass{article}
\usepackage{style}
\title{ECE324 Lectures}
\author{Hanhee Lee}
\lhead{ECE324}
\rhead{Hanhee Lee}

\begin{document}
\section{Problem 1: Description of the Problem, Subscribe a ML/NN Based Solution}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 2: Prescribe a strategy to optimize the NN.}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 3: Explain step by step inference for basic algorithms (MLP, CNN, GNN, attention mechanism) in terms of numpy or basic tensor operations.}
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}

\section{Problem 4: Be able to explain why such a solution might work or fail.} 
\begin{process}

\end{process}

\begin{definition}
    
\end{definition}

\begin{example}
    
\end{example}
\newpage

\section{Loss Functions}
\begin{summary}
\end{summary}

\section{Algorithms}
\begin{summary}
    \begin{center}
        \begin{tabular}{llll}
            \toprule
            \textbf{Algorithm} & \textbf{Inputs} & \textbf{Outputs} & \textbf{Equations} \\
            \toprule
            PCA & $x$ & $x'$ & $x \underset{\text{Encode}}{\mapsto} z \underset{\text{Decode}}{\mapsto} x'$ \\
            & & & $\text{Encoder}(x) = Wx$, $\text{Decoder}(z) = W^{-1}z$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Overview:} Linear transformation for dimension reduction 
            \end{itemize}} \\
            \midrule
            AE & $x$ & $x'$ & $x \underset{\text{Encode}}{\mapsto} z \underset{\text{Decode}}{\mapsto} x'$ \\
            & & & $f_{\text{enc}}(x), \; \text{Encoder}(x) = \text{Neural network}$, $f_{\text{dec}}(z), \; \text{Decoder}(z) = \text{Neural network}$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Bottleneck (latent space) forces network to learn compressed representation
                \item Encoder maps input to lower dimension, decoder reconstructs input
            \end{itemize}} \\
            \midrule
            VAE & $x$ & $x'$ & $x \underset{\text{Encode}}{\mapsto} z \underset{\text{Decode}}{\mapsto} x'$ \\
            & & & $f_{\text{enc}, \phi}(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mathbf{z} \mid \boldsymbol{\mu}(\mathbf{x}), \sigma^2(\mathbf{x}) \mathbf{I})$, $f_{\text{dec}}(\mathbf{z})$\\ 
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Overview:} A probabilistic approach by learning smooth latent spaces by modelling probability distributions.
                \item $\boldsymbol{\mu}(\mathbf{x})$: Mean of the distribution, $\sigma^2(\mathbf{x})$: Variance of the distribution (independent)
                \item Encoder maps input to a distribution, decoder samples from it.
                \begin{itemize}
                    \item Encoder estimates parameters for a distribution in the latent space
                    \item Decoder samples from the distribution to generate an output sample
                \end{itemize}
                \item \textbf{Key:} Works on any data.
            \end{itemize}} \\
            \midrule
            RNN & $x_t,h_{t-1}$ & $y_{t},h_{t}$ & $h_t = \text{tanh}(\text{Linear} (h_{t-1}) + \text{Linear}(x_t))$ \\ 
            & & & $y_t = \text{MLP}(h_t)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $x_t$: Input, $h_t$: Hidden state, $y_t$: Output
            \end{itemize}} \\
            \midrule
            GRU & $x_t,h_{t-1}$ & $y_t,h_t$ & $z_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $r_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $\tilde{h}(t) = \text{tanh}(\text{Linear}(x_t) + \text{Linear}(r_t \odot h_{t-1}))$ \\
            & & & $h_t = (1-z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $z_t$: Update gate, $r_t$: Reset gate
                \item $h_t$: Hidden state
            \end{itemize}} \\
            \midrule
            LSTM & $x_t,h_{t-1}$ & $h_t,c_t$ & $f_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $i_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $o_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $\tilde{c}_t = \text{tanh}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$ \\
            & & & $h_t = o_t \odot \text{tanh}(c_t)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $f_t$: Forget gate, $i_t$: Input/Update gate, $o_t$: Output gate
                \item $h_t$: Hidden state, $c_t$: Cell state
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}

\subsection{Geometric DL Blueprint for NNs}
\begin{summary}
    Unify various networks around symmetry. 
    ADD IMAGE
\end{summary}
\newpage

\section{Intro to ML}
% \input{Lectures/L2.tex}
\newpage

\section{Neural Network Engineering}
% \input{Lectures/L4.tex}
\newpage

\section{Hyperparameter Optimization}
% \input{Lectures/L5.tex}
\newpage

\section{Representations and VAE}
\input{Lectures/L6.tex}
\newpage

\section{Software Development}
\input{Lectures/L7_8.tex}
\newpage

\section{Symmetries, Tabular Data, Sets}
\input{Lectures/L9.tex}
\newpage

\section{MLP}
\input{Lectures/L3.tex}
\newpage

\section{CNN}
\input{Lectures/L10.tex}
\newpage

\section{RNN}
\input{Lectures/L11.tex}
\newpage

\section{GNN}
\input{Lectures/L12.tex}

\end{document}