\begin{definition}
    Learnable \textbf{(optimizable)} transformations of data.
    \begin{equation}
        x \overset{\text{Model}}{\mapsto} y
    \end{equation}
\end{definition}

\subsection{Challenges in NN Training}
\begin{notes}
    \begin{itemize}
        \item Non-convex loss landscapes
        \item Vanishing/exploding gradients
        \item Overfitting to training data
        \item Computational cost
    \end{itemize}
\end{notes}
\newpage

\subsection{Data}
\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Techniques} \\
        \midrule
        \textbf{Data Exploration} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Initial data inspection informs modeling choices.
            \begin{itemize}
                \item Think in terms of "x" and "y".
                \item Visualizations (histograms, scatter plots)
                \item Summary statistics
                \item Identifying data imbalances
            \end{itemize}
            \item \textbf{Libraries:} Pandas, Matplotlib, Seaborn
        \end{itemize}} \\
        \midrule
        \textbf{Data Splitting} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Proper data partitioning prevents overly optimistic performance estimates.
            \customFigure[0.5]{../Images/L4_0.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Cross-Validation} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Provides more reliable performance estimates. 
            \item Assess model performance across multiple data partitions.
            \customFigure[0.5]{../Images/L4_1.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Techniques} \\
        \midrule
        \textbf{Stratified Splitting} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Maintains class proportions. 
            \item Ensure each data split reflects the original class distribution.
            \begin{itemize}
                \item Stratification will balance one classification label.  
                \item Iterative stratification algorithm balances multiple binary labels in datasets.
            \end{itemize}
            \customFigure[0.5]{../Images/L4_2.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Adversarial Splits} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Robustness. 
            \item Evaluate model performance under challenging, out-of-distribution scenarios. 
            \item Leave out: 
            \begin{itemize}
                \item Clusters
                \item Labels
                \item Temporal Data
            \end{itemize}
            \customFigure[0.5]{../Images/L4_3.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Techniques} \\
        \midrule
        \textbf{Data Augmentation} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Increases data variability. 
            \item Create new training examples by applying transformations to existing data.
            \begin{itemize}
                \item Transformations should be relevant to the data.
                \item Increase data size.
                \item Bake in "biases"/"priors" into your model.
            \end{itemize}
            \customFigure[0.5]{../Images/L4_4.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Image Augmentation} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Apply transformations like rotations, flips, and crops to images.
            \customFigure[0.5]{../Images/L4_5.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Evaluation}
\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Tips} \\
        \midrule
        \textbf{Evaluation Metrics Quantify Performance} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Choose appropriate metrics to assess model capabilities. 
            \item Regression
            \begin{itemize}
                \item \( R^2 \) and MAE, maybe Pearson \( r \)
            \end{itemize}
            \item Ranking
            \begin{itemize}
                \item Kendall Tau
            \end{itemize}
            \item Classification
            \begin{itemize}
                \item AUROC or AUPRC or AP
                \item A ``hard'' metrics (Accuracy, F1-Score, Recall)
            \end{itemize}
            \item Multitask (many tasks)
            \begin{itemize}
                \item Mean of metrics
                \item Holistic metric
                \item What is the most important thing?
            \end{itemize}
        \end{itemize}} \\
        \midrule
        \textbf{Adjusting Decision Thresholds} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Modifying the threshold impacts precision and recall. 
            \item How to interpret your probabiltiies is a modelling decision.
            \customFigure[0.4]{../Images/L4_6.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Calibrating Predicted Probabilities} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Predicted probabilities may not reflect true likelihood.
            \customFigure[0.4]{../Images/L4_7.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Tips} \\
        \midrule
        \textbf{AUROC: Area Under The Precision Recall-Curve} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Monotonic
            \item Ranking based
            \item \(1.0\) is perfect
            \item \(0.5\) is random
            \item AUPRC: Precision-Recall
            \item AP: Average Precision
            \customFigure[0.5]{../Images/L4_8.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Regression Correlation Metrics} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item $R^2$ and Pearson's $r$ quantify linear associations. 
            \begin{itemize}
                \item Pearson $r$ (linear correlation)
            \end{itemize}
            \item \( R^2 = 0 \): Represents the mean of the data.
            \item \( R^2 = 1 \): Indicates a perfect fit.
            \item \( R^2 \): Can be infinitely worse in some cases, context-dependent.
            \customFigure[0.5]{../Images/L4_9.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Tips} \\
        \midrule
        \textbf{Rank Correlation Metrics: Spearman and Kendall Tau} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Spearman and Kendall measure monotonic relationships.
            \begin{itemize}
                \item Magnitude does not matter.
                \item Spearman: Similar to Pearson correlation but applied to ranks.
                \item Kendall: Pair-based approach, determines if the order is correct or not.
                \item Applicable for regression or classification tasks.
            \end{itemize}
        \end{itemize}} \\
        \midrule
        \textbf{Quantifying Metric Uncertainty} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Confidence intervals provide a range of plausible values.
            \customFigure[0.5]{../Images/L4_10.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Bootstrapping for Robust Estimation} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Resampling with replacement estimates metric variability.
            \begin{itemize}
                \item \textbf{Resample:} Create many datasets by randomly picking data points with replacement from your original data.
                \item \textbf{Calculate:} Compute your metric of interest on each of these resampled datasets.
                \item \textbf{Estimate:} Use the distribution of these calculated metrics to estimate the uncertainty (e.g., confidence interval) of your original metric.
            \end{itemize}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Optimization / Training}
\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Techniques} \\
        \midrule
        \textbf{Gradient Descent} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Use Adam and finetune learning rate.
        \end{itemize}} \\
        \midrule
        \textbf{Batch Size} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item The number of samples per gradient update affects learning.
            \item The batch size governs the training speed and shouldn't be used to directly tune the validation set performance. Often, the ideal batch size will be the largest batch size supported by the available hardware.
        \end{itemize}} \\
        \midrule
        \textbf{Learning Rate} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Try a range of values. 
            \item \textbf{Tension:} High learning rate means faster training, possibility of not stabilizing (offshoot the minima), low learning rate is slow.  
            \item Dataset/model/batch dependent
            \item Typically in the $1e^{-1}$ to $1e^{-6}$ range, use \texttt{log10} scale, use low for fine-tuning, $1e^{-5} - 1e^{-6}$
        \end{itemize}} \\
        \midrule
        \textbf{Learning Rate Decay} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Reduces oscillations
            \item Dynamically adjust the learning rate during training. 
            \item Learning rate decay helps fine-tune the model in later stages of training, preventing overshooting the optimal parameters.
            \customFigure[0.3]{../Images/L4_11.png}{}
            \item Plateu: Learning rate decays only after hitting a plateau for a certain number of epochs.
            \customFigure[0.3]{../Images/L4_12.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Techniques} \\
        \midrule
        \textbf{Gradient Clipping} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Prevents exploding gradients.
            \item Limit the magnitude of gradients during training.
            \[
            g \leftarrow 
            \begin{cases} 
            \lambda \frac{g}{\|g\|} & \text{if } \|g\| > \lambda \\
            g & \text{otherwise}
            \end{cases}
            \]
            \begin{itemize}
                \item Typical values: 5 or 1.
            \end{itemize}
            \customFigure[0.5]{../Images/L4_13.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Early Stopping} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Monitor validation performance and stop training when it deteriorates.
            \begin{itemize}
                \item Stop after negligible improvements.
                \item Save model and restore.
                \item Patience (how much to wait before stopping).
            \end{itemize}
            \customFigure[0.5]{../Images/L4_14.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Loss Function Weighting} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Assign different weights to different classes during loss calculation.
            \[
            L_w(y, \hat{y}) = \frac{1}{\sum w_n} \sum_{n} w_n L(y_n, \hat{y}_n)
            \]
            \begin{itemize}
                \item Weights can be inverse class frequency.
            \end{itemize}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Techniques} \\
        \midrule
        \textbf{Focal Loss} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Focuses on hard examples by down-weighting the contribution of easy examples to the loss.
            \customFigure[0.5]{../Images/L4_15.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Regularization \& Modelling}
\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Tips} \\
        \midrule
        \textbf{Normalization Standardizes Feature Distributions} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Scale input features to have zero mean and unit variance.
            \begin{itemize}
                \item Stability
                \item Speed
                \item Prevents large scale features from dominating.
            \end{itemize}
            \customFigure[0.3]{../Images/L4_16.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Batch and Layer Normalization} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Normalize the activations across batch or features.
            \customFigure[0.3]{../Images/L4_17.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Residual Connections Facilitate Deeper Networks} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Add the input of a layer to its output.
            \customFigure[0.3]{../Images/L4_18.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Tips} \\
        \midrule
        \textbf{Dropout: Randomly Setting Activations Elements to Zero} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Reduce overfitting, create an ensemble of subnetworks implicitly.
            \customFigure[0.3]{../Images/L4_19.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Ensemble Methods Combine Multiple Models} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Improve predictive performance by aggregating predictions from diverse models.
            \begin{equation*}
                \hat{y} = \frac{1}{n} \sum_n^{\text{models}} f_n(x)
            \end{equation*}
            \item Always will give an equal or better model w.r.t. predictive performance.
        \end{itemize}} \\
        \midrule
        \textbf{Random Initialization Ensembles} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Diverse initial states improves ensemble diversity.
            \begin{itemize}
                \item Same model, different random seeds.
            \end{itemize}
            \begin{equation*}
                \hat{y} = \frac{1}{n} \sum_n^{\text{models}} f_n(x;\theta_n)
            \end{equation*}
        \end{itemize}} \\
        \midrule
        \textbf{Hyperparameter Ensembles} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Hyperparameter explorations give you more diverse ensembles.
            \[
            f_n \sim \text{comes from top-k models from a hyperparameter optimization, same model class}
            \]
            
            \[
            \hat{y} = \frac{1}{n} \sum_{n}^{\text{models}} f_n(x; \theta_n)
            \]
            \customFigure[0.3]{../Images/L4_20.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Tips} \\
        \midrule
        \textbf{Uncertainty Quantification: Variability in Prediction} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Uncertainty can represent the range of likely outcomes rather than relying solely on a single point prediction.
            \item Uncertainty $~\text{std}(f_1\ldots f_n)$
            \customFigure[0.5]{../Images/L4_21.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Not All Uncertainty is the Same} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Some models offer more reliable uncertainty estimates.
            \customFigure[0.5]{../Images/L4_22.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{L1 Regularization Encourages Sparsity} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Add the sum of absolute values of weights to the loss function:
            \begin{itemize}
                \item Encouraging sparse weights
                \item Feature selection
                \item Loss function with L1 penalty:
            \end{itemize}
            \[
            L(\theta) + \lambda \sum_i |\theta_i|
            \]
        \end{itemize}} \\
        \midrule
        \textbf{L2 Regularization Penalizes Large Weights} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Add the sum of squared values of weights to the loss function:
            \begin{itemize}
                \item Weight decay
                \item Preventing overfitting
                \item Loss function with L2 penalty:
            \end{itemize}
            \[
            L(\theta) + \lambda \sum_i \theta_i^2
            \]
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Experiments}
\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Tips} \\
        \midrule
        \textbf{Logging Metrics During Training} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Track relevant metrics to monitor progress and diagnose issues.
            \customFigure[0.5]{../Images/L4_23.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Setting Up Baselines Provides a Reference Point} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Establish a simple models to compare against.
            \begin{itemize}
                \item GLM model
                \item Random forest 
                \item XGBoost
                \item NGBoost (if you want uncertainty)
                \item SKlearn-like models
            \end{itemize}
        \end{itemize}} \\
        \midrule
        \textbf{Seed Setting} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Set random seeds for reproducibility of results.
            \customFigure[0.5]{../Images/L4_26.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{l}
        \toprule
        \textbf{Tips} \\
        \midrule
        \textbf{Ablation Studies Evaluate Systematically One Axis} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Assess impact by controlled single-parameter changes.
            \customFigure[0.5]{../Images/L4_24.png}{}
        \end{itemize}} \\
        \midrule
        \textbf{Context Matters: No Free Lunch Theorem} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Best tricks depend on the specific task and dataset. 
        \end{itemize}} \\
        \midrule
        \textbf{How You Tell The Story Matters} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item How you report number and values will change how you and others think about it. 
        \end{itemize}} \\
        \midrule
        \textbf{MLP Hyperparameter Space} \\
        \multicolumn{1}{p{\linewidth}}{
        \begin{itemize}
            \item Better to simplify modelling choices
            \customFigure[0.3]{../Images/L4_27.png}{}
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}