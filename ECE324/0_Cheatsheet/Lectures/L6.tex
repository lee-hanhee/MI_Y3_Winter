\subsection{PCA}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Limitations} & \\
            \toprule
            Capacity for representation & Finds linear correlations and can't capture non-guassian struct. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_0.png}{}
            \end{center}} \\
            \midrule 
            Interpretability & Non-negative factorization (tensor decomp.) are $\uparrow$ interpretable. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_1.png}{}
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Autoencoders}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Loss Function} & Minimize reconstruction error. \\
            & $L_{AE} = \left\| \mathbf{x} - f_{\text{dec}}(f_{\text{enc}}(\mathbf{x})) \right\|^2 = \left\| \mathbf{x} - \hat{\mathbf{x}} \right\|^2$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item If our data is binary, what loss function should you use? Binary cross entropy.
            \end{itemize}} \\
            \midrule 
            \textbf{Different Types of AEs} & \\
            \midrule
            Sparse Autoencoders (SAEs): Feature Selection & Encourage latent representations with only few active neurons. \\
            & $\Omega(\theta) = \lambda \sum_i \left| \theta_i \right| \rightarrow L_{AE} + \Omega(z, \theta)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item What regularization technique encourages sparsity? L1 regularization.
            \end{itemize}} \\
            \midrule 
            Denoising Autoencoders (DAEs): Robust Features & Enhance robustness by training on noisy inputs. \\
            & $\tilde{\mathbf{x}} = \mathbf{x} + \text{noise}$ \\
            & $L_{DAE} = \left\| \mathbf{x} - f_{\text{dec}}(f_{\text{enc}}(\tilde{\mathbf{x}})) \right\|^2$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_2.png}{}
            \end{center}} \\
            \midrule
            Anomaly Detection with AEs & Detect outliers by analyzing high reconstruction errors. \\
            & $A(\mathbf{x}) = \left\| \mathbf{x} - \hat{\mathbf{x}} \right\|^2$ s.t. $A(\mathbf{x}) > \text{Threshold}$ \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Latent Space of AEs} & Latent space of an autoencoder may not be continuous \\ 
            & Acts like hashmaps: Mapping data to a discrete index and back. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_3.png}{}
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsubsection{Reparameterization Trick}
\begin{motivation}
    \begin{enumerate}
        \item In this model, the encoder estimates parameters for a distribution in the latent space, from which the decoder samples to generate an output sample.

        \[
        f_{\text{enc}, \phi}(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mathbf{z} \mid \boldsymbol{\mu}(\mathbf{x}), \sigma^2(\mathbf{x}) \mathbf{I})
        \]        
        \customFigure[0.5]{../Images/L6_11.png}{}
        \item What might be a problem with this setup? Sampling is not differentiable.
    \end{enumerate}
\end{motivation}
\begin{definition}
    Reparameterization makes sampling differentiable for VAE training.
    \begin{itemize}
        \item \textbf{Sample (Non-differentiable):}
        \[
        \mathbf{z} \sim \mathcal{N}(\mu, \sigma^2 \mathbf{I})
        \]
        
        \item \textbf{Reparametrize (Differentiable):}
        \[
        \epsilon \sim \mathcal{N}(0, \mathbf{I}) \quad \text{Sample constant noise}
        \]
        \[
        \mathbf{z} = \mu + \sigma \odot \epsilon \quad \text{Compute } \mathbf{z} \text{ as a deterministic function}
        \]
    \end{itemize}
\end{definition}

\subsection{VAE's Loss Function: Reconstruction and KL Divergence}
\begin{definition}
    Balances reconstruction and latent space regularization.
    \begin{equation}
        L_{\text{VAE}} = L_{\text{AE}} (x, \hat{x}) + \text{KL} (q_\phi (\mathbf{z} \mid \mathbf{x}) \parallel p(\mathbf{z}))
    \end{equation}
    \begin{align*}
        KL(P \| Q) &= KL\left(\mathcal{N}(\mu_1, \sigma_1^2) \| \mathcal{N}(\mu_2, \sigma_2^2)\right) \\
        &= \frac{1}{2} \left[ \log\left(\frac{\sigma_2^2}{\sigma_1^2}\right) 
        + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{\sigma_2^2} - 1 \right]
    \end{align*}
\end{definition}

\subsection{VAE are generative models}
\begin{definition}
    VAE's latent space is continuous and allows sampling new data
    \customFigure[0.5]{../Images/L6_4.png}{}
\end{definition}
\newpage

\section{Representation Learning}

\subsection{Cosine Distance: A Metric for High Dimensions}
\begin{definition}
    Likely better suitan than Euclidean distance in high dimensions.
    \[
    d_{\text{cos}}(\mathbf{u}, \mathbf{v}) = 1 - \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \cdot \|\mathbf{v}\|}
    \]

    \[
    d_{\text{euclidean}}(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}
    \]
\end{definition}

\begin{warning}
    May be appear in A1.
\end{warning}

\subsection{Manifold Hypothesis: Data in Low Dimensions}
\begin{notes}
    Real-world data resides on lower-dimensionality manifolds.
    \customFigure[0.5]{../Images/L6_9.png}{1D String (Manifold) in 2D Space}
    \begin{itemize}
        \item What is the dimensionality of Fashion-MNIST? Higher bounded is 784 because it is a 28x28 image, but the lower bound is ill-definied (many answers)
        \item If it was a colour image, then it would be $3 \times 28 \times 28 = 2352$.
    \end{itemize}
\end{notes}

\subsection{Hierarchical Organization in NN Depth}
\begin{notes}
    Learning low to high level abstractions across different layers.
    \customFigure[0.5]{../Images/L6_10.png}{}
\end{notes}

\subsection{Contrastive Learning}
\begin{notes}
    Embed similar data close together. 
    \begin{itemize}
        \item \textbf{Positive Pairs:} Similar data points
        \item \textbf{Negative Pairs:} Dissimilar data points
    \end{itemize}
    \customFigure[0.5]{../Images/L6_12.png}{}
\end{notes}