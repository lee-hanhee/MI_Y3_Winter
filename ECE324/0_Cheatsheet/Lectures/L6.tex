\subsection{PCA}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Limitations} & \\
            \toprule
            Capacity for representation & Finds linear correlations and can't capture non-guassian struct. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_0.png}{}
            \end{center}} \\
            \midrule 
            Interpretability & Non-negative factorization (tensor decomp.) are $\uparrow$ interpretable. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_1.png}{}
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Autoencoders}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Loss Function} & Minimize reconstruction error. \\
            & $L_{AE} = \left\| \mathbf{x} - f_{\text{dec}}(f_{\text{enc}}(\mathbf{x})) \right\|^2 = \left\| \mathbf{x} - \hat{\mathbf{x}} \right\|^2$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item If our data is binary, what loss function should you use? Binary cross entropy.
            \end{itemize}} \\
            \midrule 
            \textbf{Different Types of AEs} & \\
            \midrule
            Sparse Autoencoders (SAEs): Feature Selection & Encourage latent representations with only few active neurons. \\
            & $\Omega(\theta) = \lambda \sum_i \left| \theta_i \right| \rightarrow L_{AE} + \Omega(z, \theta)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item What regularization technique encourages sparsity? L1 regularization.
            \end{itemize}} \\
            \midrule 
            Denoising Autoencoders (DAEs): Robust Features & Enhance robustness by training on noisy inputs. \\
            & $\tilde{\mathbf{x}} = \mathbf{x} + \text{noise}$ \\
            & $L_{DAE} = \left\| \mathbf{x} - f_{\text{dec}}(f_{\text{enc}}(\tilde{\mathbf{x}})) \right\|^2$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_2.png}{}
            \end{center}} \\
            \midrule
            Anomaly Detection with AEs & Detect outliers by analyzing high reconstruction errors. \\
            & $A(\mathbf{x}) = \left\| \mathbf{x} - \hat{\mathbf{x}} \right\|^2$ s.t. $A(\mathbf{x}) > \text{Threshold}$ \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Latent Space of AEs} & Latent space of an autoencoder may not be continuous \\ 
            & Acts like hashmaps: Mapping data to a discrete index and back. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_3.png}{}
                \vspace{-4em}
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Variational Autoencoders}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Reparameterization Trick} & Makes sampling differentiable for VAE training. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Motivation:} Sampling is not differentiable with the following image: 
                \begin{center}
                    \customFigure[0.25]{../Images/L6_11.png}{}
                    \vspace{-3em}
                \end{center}
                \item \textbf{Steps:}
                \begin{enumerate}
                    \item Sample (non-differentiable): $\mathbf{z} \sim \mathcal{N}(\mu, \sigma^2 \mathbf{I})$
                    \item Reparametrize (differentiable):
                    \begin{enumerate}
                        \item Sample constant noise: $\epsilon \sim \mathcal{N}(0, \mathbf{I})$
                        \item Compute $\mathbf{z}$ as a deterministic function: $\mathbf{z} = \mu + \sigma \odot \epsilon$
                    \end{enumerate}
                \end{enumerate}
            \end{itemize}} \\
            \midrule
            \textbf{Loss Fn: Reconstruction and KL Divergence} & Balances reconstruction and latent space regularization. \\
            &  $L_{\text{VAE}} = L_{\text{AE}} (x, \hat{x}) + \text{KL} (q_\phi (\mathbf{z} \mid \mathbf{x}) \parallel p(\mathbf{z}))$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item For Gaussian Distribution: $KL(P \| Q) = KL\left(\mathcal{N}(\mu_1, \sigma_1^2) \| \mathcal{N}(\mu_2, \sigma_2^2)\right) = \frac{1}{2} \left[ \log\left(\frac{\sigma_2^2}{\sigma_1^2}\right) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{\sigma_2^2} - 1 \right]$ 
            \end{itemize}} \\
            \midrule 
            \textbf{Generative Models} & VAE's latent space is cont. and allows sampling new data. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_4.png}{}
                \vspace{-4em}
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Representation Learning}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            Euclidean distance & A metric for low dimensions: $d_{\text{euclidean}}(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}$ \\
            \midrule
            Cosine Distance & A metric for high dimensions: $d_{\text{cos}}(\mathbf{u}, \mathbf{v}) = 1 - \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \cdot \|\mathbf{v}\|}$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Geometry of high-dim spaces are non-intuitive.
            \end{itemize}} \\
            \midrule 
            Manifold Hypothesis: Data in Low-Dim & Real-world data resides on lower-dimensionality manifolds. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_9.png}{1D String (Manifold) Embedded in 2D}
                \vspace{-4em}
            \end{center}} \\
            \midrule
            Hierarchical Organization in NN Depth & Learning low to high level abstractions across different layers. \\
            \midrule
            Contrastive Learning & Embed similar data close together. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Positive Pairs:} Similar data points
                \item \textbf{Negative Pairs:} Dissimilar data points
                \begin{center}
                    \customFigure[0.5]{../Images/L6_12.png}{}
                    \vspace{-4em}
                \end{center}
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}