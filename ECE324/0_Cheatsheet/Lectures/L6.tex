\subsection{Encoder-Decoder view}
\begin{definition}
    \begin{equation}
        x \overset{\text{Encoder}}{\mapsto} z \overset{\text{Decoder}}{\mapsto} y
    \end{equation}
    \begin{itemize}
        \item $z$: Embeddings, latent vectors, learned representations.
        \item \textbf{Latent Space:} Space of $z$.
    \end{itemize}
\end{definition}

\subsubsection{Supervised Learning}
\begin{example} 
    \begin{equation}
        x \overset{\text{Model}}{\mapsto} y
    \end{equation}
    \begin{itemize}
        \item $\mathrm{Model}(x) = \mathrm{Decoder}\bigl(\mathrm{Encoder}(x)\bigr)$
        \item $\mathrm{Decoder}(z) = \mathrm{Pred}(z)$
    \end{itemize}
    
\end{example}

\subsubsection{Unsupervised Learning (PCA)}
\begin{example} 
    \begin{enumerate}
        \item \textbf{Linear Transformation:} Encode and decode with PCA.
        \begin{equation}
            x \overset{\text{Encoder}}{\mapsto} z \overset{\text{Decoder}}{\mapsto} y
        \end{equation}
        \begin{itemize}
            \item $\text{Encoder}(x) = W \cdot x$
            \item $\text{Decoder}(z) = W^{-1} \cdot z$
        \end{itemize}
        \vspace{1em}
    
        \begin{itemize}
            \item \textcolor{teal}{\textbf{E?}} $x$
            \item \textcolor{purple}{\textbf{P?}} Reconstruction loss
            \item \textcolor{yellow}{\textbf{T?}} Reduce dimension
            \item \textcolor{orange}{\textbf{M?}} W
            \item \textcolor{red}{\textbf{learn?}} Eigendecompositions
        \end{itemize}
        \item \textbf{Neural Nets:} Replacing linear trans. in PCA w/ neural nets.
        \begin{equation}
            x \overset{\text{Encoder}}{\mapsto} z \overset{\text{Decoder}}{\mapsto} y
        \end{equation}
        \begin{itemize}
            \item $\text{Encoder}(x) = \text{Neural Network}$
            \item $\text{Decoder}(z) = \text{Neural Network}$
        \end{itemize}
    \end{enumerate}
\end{example}
\newpage

\subsection{Latent Space}
\begin{notes}
    \begin{itemize}
        \item \textbf{Encoding and Decoding:} Learning a continuous and reversible representation for molecules. 
        \customFigure[0.5]{../Images/L2_10.png}{}
        \item \textbf{Sampling:} Decoded latent vectors become molecules.
        \customFigure[0.5]{../Images/L2_11.png}{}
        \item \textbf{Optimizing:} Can perform any optimization in this new latent space.
        \customFigure[0.5]{../Images/L2_12.png}{}
        \item \textbf{Interpolating:} Connecting two latent vectors by smooth paths.
        \customFigure[0.5]{../Images/L2_13.png}{}
    \end{itemize}
\end{notes}
\newpage

\subsection{PCA}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Limitations} & \\
            \toprule
            Capacity for representation & Finds linear correlations and can't capture non-guassian struct. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_0.png}{}
                \vspace{-4em}
            \end{center}} \\
            \midrule 
            Interpretability & Non-negative factorization (tensor decomp.) are $\uparrow$ interpretable. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_1.png}{}
                \vspace{-4em}
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Autoencoders}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Different Types of AEs} & \\
            \midrule
            Sparse Autoencoders (SAEs): Feature Selection & Encourage latent representations w/ only few active neurons. \\
            & $\Omega(\theta) = \lambda \sum_i \left| \theta_i \right| \rightarrow L_{AE} + \Omega(z, \theta)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item What regularization technique encourages sparsity? L1 regularization.
            \end{itemize}} \\
            \midrule 
            Denoising Autoencoders (DAEs): Robust Features & Enhance robustness by training on noisy inputs. \\
            & $\tilde{\mathbf{x}} = \mathbf{x} + \text{noise}$ \\
            & $L_{DAE} = \left\| \mathbf{x} - f_{\text{dec}}(f_{\text{enc}}(\tilde{\mathbf{x}})) \right\|^2$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.2]{../Images/L6_2.png}{}
            \end{center}} \\
            \midrule
            Anomaly Detection with AEs & Detect outliers by analyzing high reconstruction errors. \\
            & $A(\mathbf{x}) = \left\| \mathbf{x} - \hat{\mathbf{x}} \right\|^2$ s.t. $A(\mathbf{x}) > \text{Threshold}$ \\
            \toprule
            \textbf{Latent Space of AEs} & Latent space of an autoencoder may not be continuous \\ 
            & Hashmaps: Mapping data to a discrete index and back. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.3]{../Images/L6_3.png}{}
                \vspace{-4em}
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Variational Autoencoders}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            \textbf{Reparameterization Trick} & Makes sampling differentiable for VAE training. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Motivation:} Sampling is not differentiable with the following image: 
                \begin{center}
                    \customFigure[0.25]{../Images/L6_11.png}{}
                    \vspace{-3em}
                \end{center}
                \item \textbf{Steps:}
                \begin{enumerate}
                    \item Sample (non-differentiable): $\mathbf{z} \sim \mathcal{N}(\mu, \sigma^2 \mathbf{I})$
                    \item Reparametrize (differentiable):
                    \begin{enumerate}
                        \item Sample constant noise: $\epsilon \sim \mathcal{N}(0, \mathbf{I})$
                        \item Compute $\mathbf{z}$ as a deterministic function: $\mathbf{z} = \mu + \sigma \odot \epsilon$
                    \end{enumerate}
                \end{enumerate}
            \end{itemize}} \\
            \midrule 
            \textbf{Generative Models} & VAE's latent space is cont. and allows sampling new data. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_4.png}{}
                \vspace{-4em}
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Representation Learning}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \toprule
            Euclidean distance & A metric for low dimensions: $d_{\text{euclidean}}(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}$ \\
            \midrule
            Cosine Distance & A metric for high dimensions: $d_{\text{cos}}(\mathbf{u}, \mathbf{v}) = 1 - \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \cdot \|\mathbf{v}\|}$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Geometry of high-dim spaces are non-intuitive.
            \end{itemize}} \\
            \midrule 
            Manifold Hypothesis: Data in Low-Dim & Real-world data resides on lower-dimensionality manifolds. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{center}
                \customFigure[0.5]{../Images/L6_9.png}{1D String (Manifold) Embedded in 2D}
                \vspace{-4em}
            \end{center}} \\
            \midrule
            Hierarchical Organization in NN Depth & Learning low to high level abstractions across different layers. \\
            \midrule
            Contrastive Learning & Embed similar data close together. \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Positive Pairs:} Similar data points
                \item \textbf{Negative Pairs:} Dissimilar data points
                \begin{center}
                    \customFigure[0.5]{../Images/L6_12.png}{}
                    \vspace{-4em}
                \end{center}
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}