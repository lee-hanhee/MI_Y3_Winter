\section{Math Symbols}
\begin{definition}
    \begin{itemize}
        \item \(x, y, b, h, z\) - Vectors (input, outputs, bias, hidden, latent)
        \item \(\hat{y}\) - Predictions
        \item \(\theta\) - Parameters (collections of tensors)
        \item \(f, f_i, f_j, C\) - Functions
        \item \(x_i\) - Scalar, \(i\)-indexed coordinate of \(x\)
        \item \(X, W\) - Matrices
        \item \(\frac{\partial f}{\partial x_i}\) - Partial derivative of \(f\) with respect to \(x_i\)
        \item \(\nabla f\) - Gradient of \(f\)
    \end{itemize}     
\end{definition}

\section{MLP Architecture}
\begin{definition}
    Feed-Forward NN (FFNN), Fully-Connected NN (FCNN)
    \begin{itemize}
        \item Vector-in, vector-out optimizable parametrized transformation. 
    \end{itemize}
\end{definition}

\subsection{Layers}
\begin{definition}
    \begin{equation*}
        f_{\text{layer}}(x) = \text{act}(W \cdot x + b)
    \end{equation*}
    \begin{equation*}
        \text{MLP}_2(x) = F_2(F_1(x)) = F_2(z_1)
    \end{equation*}
    \begin{equation*}
        \text{MLP}_n(x) = f_n(f_{n-1}(\dots f_1(x)))
    \end{equation*}

    \begin{itemize}
        \item Parameters: $f(x; \theta), \theta = \{W_1, \dots, W_n\} = \{w_1, \dots, w_N\}$
        \item Hyperparameters: 
        \begin{itemize}
            \item $n$: NN depth.
            \item hidden\_dim: NN width.
        \end{itemize}
    \end{itemize}
\end{definition}

\subsubsection{Hyperparameter Choices for a Single MLP}
\begin{notes}
    \customFigure[0.5]{../Images/L3_12.png}{}
\end{notes}

\subsection{Activation functions}
\begin{definition}
    Hyperparameter that 
    \begin{itemize}
        \item Introduces non-linearity.
        \item Tension: smoothness/compute/complexity.
    \end{itemize}
\end{definition}

\begin{warning}
    Choose ReLU or GeLU.
\end{warning}

\subsection{Outputs}
\begin{definition}
    Finding the "best" parameters by minimizing the error b/w predictions and actual values. 
    \begin{equation*}
        C(\theta) = \sqrt{\frac{1}{N} \sum_n (y_n - f(x_n; \theta))^2} = \text{RMSE}(y, f(x; \theta))
    \end{equation*}
\end{definition}

\subsubsection{Inductive Bias (Learning Bias)}
\begin{definition}
    Set of assumptions that the learner puts on a model for a task, makes an algorithm learn one pattern over another.
\end{definition}

\subsubsection{Maximum Likelihood Estimation (MLE)}
\begin{definition}
    Choose the parameters that maximize the likelihood (probability) of observing the data we actually have.
\end{definition}

\begin{warning}
    Minimizing negative log-likelihood leads us to loss functions.
\end{warning}

\subsubsection{Useful Trick in DL}
\begin{notes}
    Transform smoothly Gaussian-like data to Non-Gaussian data via Linear Transforms + Activations

    \begin{itemize}
        \item \textbf{Differentiable tools:}
        \begin{itemize}
            \item relu (e.g., max, min)
            \item sum, mean, var
            \item softplus
            \item sigmoid
            \item softmax
            \item masking
        \end{itemize}
    
        \item \textbf{Non differentiable:}
        \begin{itemize}
            \item argmax
            \item top-k
            \item binarize
        \end{itemize}
    \end{itemize}
    \customFigure[0.5]{../Images/L3_4.png}{}
\end{notes}

\begin{example}
    \begin{itemize}
        \item \textbf{Why should the transformation be differentiable?} 
        \begin{itemize}
            \item Smooth
        \end{itemize}
    \end{itemize}    
\end{example}

\subsubsection{Regression - Gaussian Distribution}
\begin{notes}
    \begin{itemize}
        \item \textbf{Likelihood:} The probability density of observing the actual output values given the inputs and our model parameters (weights, bias, and the variance of the noise).
        \item \textbf{MLE:} Minimizing negative Log-Likelihood leads to the Mean Squared Error:
        \[
        \frac{1}{n} \sum_n (y_n - \hat{y}_n)^2
        \]
        \item \textbf{Activation Function:} $\text{Act} = x$: Identity function.
    \end{itemize}
    \customFigure[0.5]{../Images/L3_4.png}{}
\end{notes}

\subsubsection{Masking: Omit data when missing or NA}
\begin{notes}
    Multiply loss function by a binary valued mask. 
    \begin{equation*}
        \frac{1}{\sum \text{mask}} \sum_n \text{mask}_n \cdot (y_n - \hat{y}_n)^2
    \end{equation*}
    \begin{itemize}
        \item Good for NaN, $-\infty$, high value outliers.
    \end{itemize}
    \customFigure[0.5]{../Images/L3_5.png}{}
\end{notes}

\subsubsection{Binary Classification - Binomial Distribution}
\begin{notes}
    \begin{itemize}
        \item \textbf{Likelihood:} The probability of observing the actual class labels (0 or 1) given the inputs and our model parameters.
        \item \textbf{MLE:} Minimizing negative Log-Likelihood leads to the Binary Cross Entropy loss (BCE):
        \[
        \frac{1}{n} \sum_n \left( y_n \log(\hat{y}_n) + (1 - y_n) \log(1 - \hat{y}_n) \right)
        \]
        \item \textbf{Activation Function:} $\text{Act} = \text{sigmoid}$: Squashes the output to $[0, 1].$
    \end{itemize}
    \customFigure[0.5]{../Images/L3_6.png}{}    
\end{notes}

\subsubsection{Multilabel Classification - N-dim Binomial Distribution}
\begin{notes}
    \begin{itemize}
        \item \textbf{Loss function:} \textbf{Sum} of Binary Cross Entropy (BCE) for each label:
        \[
        \frac{1}{n} \sum^{\text{labels}} \sum_n \left( y_n \log(\hat{y}_n) + (1 - y_n) \log(1 - \hat{y}_n) \right)
        \]
        \item \textbf{Activation Function:} $\text{Act} = \text{sigmoid}$: Squashes the output to $[0, 1].$
    \end{itemize}
    \customFigure[0.5]{../Images/L3_7.png}{}        
\end{notes}

\subsubsection{Multiclass Classification - Multinomial Distribution}
\begin{notes}
    \begin{itemize}
        \item \textbf{Loss function:} Cross entropy (CE):
        \[
        \sum_{\text{classes } i} y_i \log(\hat{y}_i)
        \]
        \item \textbf{Activation Function:} $\text{Act} = \text{softmax}$, where $\text{softmax}(z) = \frac{e^{z_i}}{\sum_j e^{z_j}}$
    \end{itemize}
    \customFigure[0.5]{../Images/L3_8.png}{}
\end{notes}

\subsubsection{Ordinal Classification}
\begin{notes}
    \begin{itemize}
        \item \textbf{Activation Function:} $\text{Act} = \text{sigmoid}$
    \end{itemize}
    \customFigure[0.5]{../Images/L3_9.png}{}
\end{notes}

\subsubsection{Zero-Inflated Distribution}
\begin{example}
    \[
    \text{Pred}(z) =
    \begin{cases}
        \text{active}: \text{sigmoid}(W_b \cdot z) \\
        \text{value}: W_r \cdot z
    \end{cases}
    \]

    \[
    \text{BCE}(\text{mask}, y_{\text{pred,binary}}) + \text{mask} \cdot * \text{MSE}(y_{\text{true}}, y_{\text{pred,reg}})
    \]
    \customFigure[0.75]{../Images/L3_11.png}{}
\end{example}



\section{Learning}
\subsection{Optimization}
\subsubsection{Gradient Descent: "Rolling Downhill"}
\begin{definition}
    Iteratively adjusting parameters to reduce the loss. 
    \begin{equation*}
        \theta_n = \theta_{n-1} - \alpha \nabla C(\theta_{n-1})
    \end{equation*}
    \customFigure[0.5]{../Images/L3_3.png}{}
    \begin{itemize}
        \item Multiple minima
        \item Gradient information:
        \begin{itemize}
            \item Sign (\textcolor{green}{+}, \textcolor{red}{-})
            \item Magnitude
        \end{itemize}
        \item Same idea for each parameter
        \item Initialization matters! (use default)
        \item \(\alpha\): Learning rate (how fast we get there) 
    \end{itemize}
\end{definition}

\subsection{Backpropagation: Forward and Backward Passes}
\begin{definition}
    Calculating predictions, keeping track operations and then gradients.
    \customFigure[0.5]{../Images/L3_13.png}{}
    \begin{itemize}
        \item $g \leftarrow \nabla_{\hat{y}} C = \nabla_{\hat{y}} L(\hat{y}, y)$: Compute gradient on output layer.
        \item $g \leftarrow \nabla_{h^{(k)}} C = g \odot \text{act}'(h^{(k)})$: Convert gradient on layer output into a gradient on the pre-nonlinearity activation.
        \item $\nabla_{b^{(k)}} C = g$: Compute gradient on biases.
        \item $\nabla_{W^{(k)}} C = g z^{(k-1)\top}$: Compute gradient on weights.
        \item $g \gets \nabla_{z^{(k-1)}} C = W^{(k)\top} g$: Propagate gradients w.r.t. the next lower-level activations.
    \end{itemize}
\end{definition}