\begin{summary}
    \begin{itemize}
        \item What does it mean when a computer program learns?
        \item What is a neural network?
        \item What is something that a AI model CANNOT do right now?
    \end{itemize}
\end{summary}

\section{Learning representations of data}
\subsubsection{AI/MI/ML/DL}
\begin{definition}
    \customFigure[0.5]{../Images/L2_0.png}{} 
\end{definition}
\subsection{Learning algorithms}
\begin{definition}
    \textquotedblleft A \colorbox{orange}{\textbf{computer program M}} is said to \colorbox{red}{\textbf{learn}} from \colorbox{cyan}{\textbf{experience E}} with respect to some class of \colorbox{yellow}{\textbf{tasks T}} and \colorbox{violet}{\textbf{performance measure P}}, if its performance at \colorbox{yellow}{\textbf{tasks in T}}, as measured by \colorbox{violet}{\textbf{P}}, improves with \colorbox{cyan}{\textbf{experience E}}\textquotedblright
    \vspace{1em}

    \begin{itemize}
        \item \textbf{\color{cyan}experience E} $\sim$ Data
        \item \textbf{\color{violet}performance measure P} $\sim$ Loss function, evaluation metric
        \item \textbf{\color{yellow}tasks T} $\sim$ ``Prediction problem''
        \item \textbf{\color{orange}computer program M} $\sim$ Model
        \item \textbf{\color{red}learn} $\sim$ Optimize
    \end{itemize}
\end{definition}

\subsection{Representing data as tensors}
\begin{definition}
    GLMs and Neural Nets work with tensor data.
\end{definition}

\subsection{Linear models}
\subsubsection{Linear Regression}
\begin{definition} 
    \begin{equation}
        W \cdot x = y
    \end{equation}
    \begin{itemize}
        \item \textcolor{teal}{\textbf{E?}} (x and y)
        \item \textcolor{purple}{\textbf{P?}} mean squared error
        \item \textcolor{yellow}{\textbf{T?}} Predict y from x
        \item \textcolor{orange}{\textbf{M?}} Linear model (W)
        \item \textcolor{red}{\textbf{learn?}} Analytical solution or gradient descent
    \end{itemize}
\end{definition}

\subsubsection{Generalized Linear Models}
\begin{definition} 
    Linear transformations warped to a prediction target.
    \begin{equation}
        \text{Link}(W \cdot x) = y
    \end{equation}
    \begin{itemize}
        \item $x$: Input features
        \item $W$: Linear transformation
        \item $y$: Output / target
        \item $\text{Link}(x)$: Warping function
    \end{itemize}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item If $x$ has dim $50$ and $W$ projects to dimension $100$, what is the shape of $W$?
        \begin{itemize}
            \item $W$ is a $100 \times 50$ matrix
        \end{itemize}
        \item If $W$ is learnable, how many parameters does $W$ have?
        \begin{itemize}
            \item $100 \times 50 = 5000$ parameters
        \end{itemize}
    \end{enumerate}
\end{example}

\begin{notes} \textbf{How does a generalized linear model make a prediction?}
    By either mapping to a line or separating data by a line (hyperplane)
    \customFigure[0.5]{../Images/L2_2.png}{}
\end{notes}

\begin{notes} \textbf{What can we do when the data cannot be separated by a line?}
    Resort to different decision surfaces.
    \customFigure[0.5]{../Images/L2_3.png}{}
\end{notes}

\subsection{Representations}
\begin{definition}
    \textbf{Representation} is a way of encoding data.
    \begin{equation}
        x \overset{\text{Representation}}{\mapsto} z
    \end{equation}
    \begin{itemize}
        \item $z$: Feature vectors, embeddings, latent codes, intermediate activations, etc.
    \end{itemize}
\end{definition}

\begin{notes}
    \begin{itemize}
        \item Learning representations of our data, optimized to a task.
    \end{itemize}
    \customFigure[0.5]{../Images/L2_9.png}{}
\end{notes}

\section{Neural networks}
\begin{definition}
    Learnable \textbf{(optimizable)} transformations of data.
    \begin{equation}
        x \overset{\text{Model}}{\mapsto} y
    \end{equation}
\end{definition}

\subsection{2-layer MLP}
\begin{definition} By stacking linear transforms with activation functions.
    \begin{equation}
        \text{Link}(W_2 \cdot \text{relu}(W_1 \cdot x)) = y
    \end{equation}
    \begin{itemize}
        \item $x$: Input features.
        \item $W_1, W_2$: Linear transformations or Weight Matrices.
        \item $\text{relu}(x) = \max(0,x)$: Non-linear activation function, s.t. $f'(x) = 1$ if $x > 0$ and $0$ otherwise.
        \item $y$: Output / target. 
    \end{itemize}
    \vspace{1em}

    \begin{itemize}
        \item \textcolor{teal}{\textbf{E?}} (x and y)
        \item \textcolor{purple}{\textbf{P?}} mean squared error
        \item \textcolor{yellow}{\textbf{T?}} Predict y from x
        \item \textcolor{orange}{\textbf{M?}} Neural net (W1, W2)
        \item \textcolor{red}{\textbf{learn?}} gradient descent
    \end{itemize}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item What purpose does relu serve? 
            \begin{itemize}
                \item Introduces non-linearity into the model, allowing it to learn more complex functions.
            \end{itemize}       
        \item If $x$ has dim $50$ and $y$ dim $10$, we have layer size of $50$, how many parameters do we have?
            \begin{itemize}
                \item $W_1$ is a $50 \times 50$ matrix, so it has $50 \times 50 = 2500$ parameters.
                \item $W_2$ is a $10 \times 50$ matrix, so it has $10 \times 50 = 500$ parameters.
                \item Total parameters: $2500 + 500 = 3000$ parameters. IS THIS CORRECT?
            \end{itemize} 
    \end{enumerate}
\end{example}

\subsection{Geometric intuition}
\subsubsection{Decision surfaces}
\begin{notes} 
    Different ways of cutting up space to make predictions.
    \customFigure[0.5]{../Images/L2_5.png}{}
\end{notes}

\subsubsection{Linear Transformation}
\begin{notes} 
    Transform data from one vector space to another
    \begin{equation}
        W \cdot x 
    \end{equation}
    \begin{itemize}
        \item $W$: Linear transformation
        \item $x$: Vector
    \end{itemize}
\end{notes}

\subsubsection{SVD of Linear Transformation}
\begin{notes}  
    Factorizing matrices into geometrical transformations.
    \begin{equation}
        W = U \Sigma V^T
    \end{equation}
    \begin{itemize}
        \item $U,V$: Rotation or reflection
        \item $\Sigma$: Scaling
    \end{itemize}
\end{notes}

\subsubsection{Affine Transformation}
\begin{notes} Transform data from one vector space to another.
    \begin{equation}
        W \cdot x + b
    \end{equation}
    \begin{itemize}
        \item $W$: Linear transformation
        \item $x$: Vector
        \item $b$: Bias vector
    \end{itemize}
    \vspace{1em}

    \begin{itemize}
        \item Translate (b)
        \item Rotate (W-SVD)
        \item Reflect (W-SVD)
        \item Scale (W-SVD)
        \item Project up or down (dimensionality of $W \mathbf{x}$)
    \end{itemize}
\end{notes}

\subsubsection{ReLU}
\begin{notes} Rectified linear unit, which has a geometric effect of "gating", some info passes, some doesn't.
    \begin{equation*}
        f(x) = \max(0,x)
    \end{equation*}
    \begin{equation*}
        f'(x) = \begin{cases}
            1 & \text{if } x > 0 \\
            0 & \text{if } x \leq 0
        \end{cases}
    \end{equation*}
    \customFigure[0.5]{../Images/L2_8.png}{}
\end{notes}

\subsubsection{Neural nets}
\begin{notes} Learn to warp space to make better predictions.
    \customFigure[0.5]{../Images/L2_6.png}{}
    \customFigure[0.5]{../Images/L2_7.png}{}
\end{notes}

\subsection{Encoder-Decoder view}
\begin{definition}
    \begin{equation}
        x \overset{\text{Encoder}}{\mapsto} z \overset{\text{Decoder}}{\mapsto} y
    \end{equation}
    \begin{itemize}
        \item $z$: Embeddings, latent vectors, learned representations.
        \item \textbf{Latent Space:} Space of $z$.
    \end{itemize}
\end{definition}

\subsubsection{Supervised Learning}
\begin{example} 
    \begin{equation}
        x \overset{\text{Model}}{\mapsto} y
    \end{equation}
    \begin{itemize}
        \item $\mathrm{Model}(x) = \mathrm{Decoder}\bigl(\mathrm{Encoder}(x)\bigr)$
        \item $\mathrm{Decoder}(z) = \mathrm{Pred}(z)$
    \end{itemize}
    
\end{example}

\subsubsection{Unsupervised Learning (PCA)}
\begin{example} 
    \begin{enumerate}
        \item \textbf{Linear Transformation}
        \begin{equation}
            x \overset{\text{Encoder}}{\mapsto} z \overset{\text{Decoder}}{\mapsto} y
        \end{equation}
        \begin{itemize}
            \item $\text{Encoder}(x) = W \cdot x$
            \item $\text{Decoder}(z) = W^{-1} \cdot z$
        \end{itemize}
        \vspace{1em}
    
        \begin{itemize}
            \item \textcolor{teal}{\textbf{E?}} $x$
            \item \textcolor{purple}{\textbf{P?}} Reconstruction loss
            \item \textcolor{yellow}{\textbf{T?}} Reduce dimension
            \item \textcolor{orange}{\textbf{M?}} W
            \item \textcolor{red}{\textbf{learn?}} Eigendecompositions
        \end{itemize}
        \item \textbf{Neural Nets}
        \begin{equation}
            x \overset{\text{Encoder}}{\mapsto} z \overset{\text{Decoder}}{\mapsto} y
        \end{equation}
        \begin{itemize}
            \item $\text{Encoder}(x) = \text{Neural Network}$
            \item $\text{Decoder}(z) = \text{Neural Network}$
        \end{itemize}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Latent Space}
\begin{notes}
    \begin{itemize}
        \item \textbf{Encoding and Decoding:} Learning a continuous and reversible representation for molecules. 
        \customFigure[0.5]{../Images/L2_10.png}{}
        \item \textbf{Sampling:} Decoded latent vectors become molecules.
        \customFigure[0.5]{../Images/L2_11.png}{}
        \item \textbf{Optimizing:} Can perform any optimization in this new latent space.
        \customFigure[0.5]{../Images/L2_12.png}{}
        \item \textbf{Interpolating:} Connecting two latent vectors by smooth paths.
        \customFigure[0.5]{../Images/L2_13.png}{}
    \end{itemize}
\end{notes}

\subsection{Typical ML Pipeline}
\begin{notes}
    \begin{itemize}
        \item Setup data $(x,y)$
        \item Define a model: $y = f(x,\theta)$
        \item Training algorithm to find $\theta$
        \item Evaluate the model.
    \end{itemize}
\end{notes}

