\subsection{Learning representations of data}
\subsubsection{AI/MI/ML/DL}
\begin{definition}
    \customFigure[0.5]{../Images/L2_0.png}{} 
\end{definition}
\subsubsection{Learning algorithms}
\begin{definition}
    \textquotedblleft A \colorbox{orange}{\textbf{computer program M}} is said to \colorbox{red}{\textbf{learn}} from \colorbox{cyan}{\textbf{experience E}} with respect to some class of \colorbox{yellow}{\textbf{tasks T}} and \colorbox{violet}{\textbf{performance measure P}}, if its performance at \colorbox{yellow}{\textbf{tasks in T}}, as measured by \colorbox{violet}{\textbf{P}}, improves with \colorbox{cyan}{\textbf{experience E}}\textquotedblright
    \vspace{1em}

    \begin{itemize}
        \item \textbf{\color{cyan}experience E} $\sim$ Data
        \item \textbf{\color{violet}performance measure P} $\sim$ Loss function, evaluation metric
        \item \textbf{\color{yellow}tasks T} $\sim$ ``Prediction problem''
        \item \textbf{\color{orange}computer program M} $\sim$ Model
        \item \textbf{\color{red}learn} $\sim$ Optimize
    \end{itemize}
\end{definition}

\subsubsection{Representing data as tensors}
\begin{definition}
    GLMs and Neural Nets work with tensor data.
\end{definition}
\newpage

\subsubsection{Linear models}
\begin{definition} \textbf{Linear Regression}
    \begin{equation}
        W \cdot x = y
    \end{equation}
    \begin{itemize}
        \item \textcolor{teal}{\textbf{E?}} (x and y)
        \item \textcolor{purple}{\textbf{P?}} mean squared error
        \item \textcolor{yellow}{\textbf{T?}} Predict y from x
        \item \textcolor{orange}{\textbf{M?}} Linear model (W)
        \item \textcolor{red}{\textbf{learn?}} Analytical solution or gradient descent
    \end{itemize}
\end{definition}

\begin{definition} \textbf{Generalized Linear Models:}
    Linear transformations warped to a prediction target.
    \begin{equation}
        \text{Link}(W \cdot x) = y
    \end{equation}
    \begin{itemize}
        \item $x$: Input features
        \item $W$: Linear transformation
        \item $y$: Output / target
        \item $\text{Link}(x)$: Warping function
    \end{itemize}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item If $x$ has dim $50$ and $W$ projects to dimension $100$, what is the shape of $W$?
        \begin{itemize}
            \item $W$ is a $100 \times 50$ matrix
        \end{itemize}
        \item If $W$ is learnable, how many parameters does $W$ have?
        \begin{itemize}
            \item $100 \times 50 = 5000$ parameters
        \end{itemize}
    \end{enumerate}
\end{example}

\begin{notes} \textbf{How does a generalized linear model make a prediction?}
    By either mapping to a line or separating data by a line (hyperplane)
    \customFigure[0.3]{../Images/L2_2.png}{}
\end{notes}

\begin{notes} \textbf{What can we do when the data cannot be separated by a line?}
    Resort to different decision surfaces.
    \customFigure[0.3]{../Images/L2_3.png}{}
\end{notes}
\newpage

\subsubsection{2-layer MLP}
\begin{definition} By stacking linear transforms with activation functions.
    \begin{equation}
        \text{Link}(W_2 \cdot \text{relu}(W_1 \cdot x)) = y
    \end{equation}
    \begin{itemize}
        \item $x$: Input features.
        \item $W_1, W_2$: Linear transformations or Weight Matrices.
        \item $\text{relu}(x) = \max(0,x)$: Non-linear activation function, s.t. $f'(x) = 1$ if $x > 0$ and $0$ otherwise.
        \item $y$: Output / target. 
    \end{itemize}
    \vspace{1em}

    \begin{itemize}
        \item \textcolor{teal}{\textbf{E?}} (x and y)
        \item \textcolor{purple}{\textbf{P?}} mean squared error
        \item \textcolor{yellow}{\textbf{T?}} Predict y from x
        \item \textcolor{orange}{\textbf{M?}} Neural net (W1, W2)
        \item \textcolor{red}{\textbf{learn?}} gradient descent
    \end{itemize}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item What purpose does relu serve? 
            \begin{itemize}
                \item Introduces non-linearity into the model, allowing it to learn more complex functions.
            \end{itemize}       
        \item If $x$ has dim $50$ and $y$ dim $10$, we have layer size of $50$, how many parameters do we have?
            \begin{itemize}
                \item $W_1$ is a $50 \times 50$ matrix, so it has $50 \times 50 = 2500$ parameters.
                \item $W_2$ is a $10 \times 50$ matrix, so it has $10 \times 50 = 500$ parameters.
                \item Total parameters: $2500 + 500 = 3000$ parameters. IS THIS CORRECT?
            \end{itemize} 
    \end{enumerate}
\end{example}

\subsection{Representations}
\begin{definition}
    \textbf{Representation} is a way of encoding data.
    \begin{equation}
        x \overset{\text{Representation}}{\mapsto} z
    \end{equation}
    \begin{itemize}
        \item $z$: Feature vectors, embeddings, latent codes, intermediate activations, etc.
    \end{itemize}
\end{definition}

\begin{notes}
    \begin{itemize}
        \item Learning representations of our data, optimized to a task.
    \end{itemize}
    \customFigure[0.5]{../Images/L2_9.png}{}
\end{notes}


