\subsection{Problem Setup}
\begin{definition}
    Given a Bayesian network, $\mathcal{B} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V} = \{X_1, \dots, X_{|\mathcal{V}|}\}$, we want to find the value of:
    \[
    \operatorname{pr}(\mathbf{Q} \mid \mathbf{E}) := \operatorname{pr}(Q_1, \dots, Q_{|\mathbf{Q}|} \mid E_1, \dots, E_{|\mathbf{E}|}) = \frac{\sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} p(X_1, \dots, X_{|\mathcal{V}|})}
    {\sum_{\mathcal{V} \setminus \mathbf{E}} p(X_1, \dots, X_{|\mathcal{V}|})}
    \]
    \[
    \operatorname{pr}(\mathbf{Q} \mid \mathbf{E}) \propto 
    \sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} 
    \left( p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i)) \right)
    \]


    \begin{itemize}
        \item $\mathbf{Q} = \{Q_1, \dots, Q_{|\mathbf{Q}|}\}$: Query variables
        \item $\mathbf{E} = \{E_1, \dots, E_{|\mathbf{E}|}\} \subseteq \mathcal{V}$: Evidence variables
        \item $\mathbf{Q} \cap \mathbf{E} = \emptyset$.
    \end{itemize}
\end{definition}

\subsection{Method 1: Bayesian Network Inference}

\subsubsection{Markov Blanket}
\begin{definition} 
    The \textbf{Markov blanket} of a variable $X$, denoted $\operatorname{mbk}(X)$, consists of the following variables:
    \begin{itemize}
        \item $X$'s children
        \item $X$'s parents
        \item The other parents of $X$'s children, excluding $X$ itself.
    \end{itemize}
    which is when a variable, $X$, is "eliminated", the resulting factor's scope is the Markov blanket of $X$.
\end{definition}

\subsubsection{Graphical Interpretation}
\begin{definition}
    Pictorially, eliminating $X$ is equivalent to replacing all hyper-edges that include $X$ with their union minus $X$, and then removing $X$.
\end{definition}

\subsubsection{Elimination Ordering}
\begin{definition}
    The order that the variables are eliminated.
\end{definition}

\subsubsection{Elimination Width}
\begin{definition}
    The \textbf{elimination width} of a sequence of hyper-graphs is the \# of variables in the hyper-edge within the sequence with the most variables.
\end{definition}

\subsubsection{Heuristics for Elimination Ordering}
\begin{definition}
    Choose the elimination ordering to minimize the elimination width using the following heuristics:
    \begin{enumerate}
        \item Eliminate variable with the fewest parents.
        \item Eliminate variable with the smallest domain for its parents, where
        \[
        |\operatorname{dom}(\operatorname{pts}(X))| = \prod_{Z \in \operatorname{pnt}(X)} |\operatorname{dom}(Z)|.
        \]
        \item Eliminate variable with the smallest Markov blanket.
        \item Eliminate variable with the smallest domain for its Markov blanket, where
        \[
        |\operatorname{dom}(\operatorname{mbk}(X))| = \prod_{Z \in \operatorname{embk}(X)} |\operatorname{dom}(Z)|.
        \]
    \end{enumerate}
\end{definition}
\newpage

\subsection{Method 2: Inference via Sampling}
\begin{definition}
    Generate a large \# of samples and then approximate as:
    \[
    p(\mathbf{Q} \mid \mathbf{E}) \approx \frac{\# \text{ of samples w/ } \mathbf{Q} \text{ and } \mathbf{E}}{\# \text{ of samples w/ } \mathbf{E}}.
    \]
    \begin{itemize}
        \item As $\# \text{ of samples} \to \infty$, the approximation becomes exact.
    \end{itemize}
\end{definition}

\subsubsection{Inference via Sampling with Likelihood Weighting}
\begin{motivation}
    Most of the samples are wasted since they are not consistent with the evidence.
\end{motivation}

\begin{definition}
    Generate a large \# of samples and then approximate as:
    \[
    p(\mathbf{Q} \mid \mathbf{E}) \approx \frac{\text{weight of samples w/ } \mathbf{Q} \text{ and } \mathbf{E}}{\text{weight of samples w/ } \mathbf{E}}.
    \]
    \begin{itemize}
        \item Weight for each sample: Probability of forcing the evidence, i.e. probability of the evidence given the sample.
    \end{itemize}
\end{definition}
\newpage

\subsection{Canonical Problems:}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Caveman is deciding whether to go hunt for meat. He must take into account several factors:
        \begin{itemize}
            \item Weather
            \item Possibility of over-exertion
            \item Possibility encountering lion
        \end{itemize}

        These factors can result in Cavemen's death. His decision will ultimately depend on the \textbf{chances} of his death.
        \item \textbf{Binary Variables:}
        \begin{itemize}
            \item $W = \{\text{Sun}, \text{Rainy}\}$: Weather
            \item $H$: Whether the Cavemen goes hunting or not.
            \item $L$: Whether the Cavemen encounters a lion or not.
            \item $T$: Whether the Cavement is tired or not.
            \item $D$: Whether the Cavemen dies or not
        \end{itemize}
        \item \textbf{Problem:} Cavemen must decide whether to go hunting or not. 
        \begin{itemize}
            \item He must consider the conditional probabilities (i.e. dependence) of each event.
        \end{itemize}
    \end{enumerate}
\end{example}

\begin{warning}
    Have to be discrete. 
\end{warning}
\newpage

\subsubsection{Path Blocked?}
\begin{process}
    \begin{itemize}
        \item Know when a path is blocked. More than one path b/w 2 varaibles, then all paths need to be blocked. 
    \end{itemize}
\end{process}

\begin{example}
    
\end{example}

\subsubsection{Independence}
\begin{process}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{process}

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Bayesian network.
        \customFigure[0.25]{../Images/L6_7.png}{}
        \item \textbf{Problem:} $A$ and $E$ are 
        \begin{itemize}
            \item independent if $\mathcal{K}=$
            \item not necessarily independent for $\mathcal{K}=$
        \end{itemize}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Hypergraph}
\begin{process}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{process}

\subsubsection{Bayesian Inference}
\begin{process}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{process}

\begin{example} 
    \begin{enumerate}
        \item \textbf{Given:}
        \customFigure[0.5]{../Images/L6_9.png}{}
        \item \textbf{Problem:}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Inference via Sampling}
\begin{process}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{process}

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:}
        \item \textbf{Problem:}
    \end{enumerate}
\end{example}