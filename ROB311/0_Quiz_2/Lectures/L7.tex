\subsection{Problem Setup}
\begin{definition}
    Given a Bayesian network, $\mathcal{B} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V} = \{X_1, \dots, X_{|\mathcal{V}|}\}$, we want to find the value of:
    \[
    \operatorname{pr}(\mathbf{Q} \mid \mathbf{E}) := \operatorname{pr}(Q_1, \dots, Q_{|\mathbf{Q}|} \mid E_1, \dots, E_{|\mathbf{E}|}) = \frac{\sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} p(X_1, \dots, X_{|\mathcal{V}|})}
    {\sum_{\mathcal{V} \setminus \mathbf{E}} p(X_1, \dots, X_{|\mathcal{V}|})}
    \]
    \[
    \operatorname{pr}(\mathbf{Q} \mid \mathbf{E}) \propto 
    \sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} 
    \left( p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i)) \right)
    \]


    \begin{itemize}
        \item $\mathbf{Q} = \{Q_1, \dots, Q_{|\mathbf{Q}|}\}$: Query variables
        \item $\mathbf{E} = \{E_1, \dots, E_{|\mathbf{E}|}\} \subseteq \mathcal{V}$: Evidence variables
        \item $\mathbf{Q} \cap \mathbf{E} = \emptyset$.
    \end{itemize}
\end{definition}

\subsection{Method 1: Bayesian Network Inference}

\subsubsection{Markov Blanket}
\begin{definition} 
    The \textbf{Markov blanket} of a variable $X$, denoted $\operatorname{mbk}(X)$, consists of the following variables:
    \begin{itemize}
        \item $X$'s children
        \item $X$'s parents
        \item The other parents of $X$'s children, excluding $X$ itself.
    \end{itemize}
    which is when a variable, $X$, is "eliminated", the resulting factor's scope is the Markov blanket of $X$.
\end{definition}

\subsubsection{Graphical Interpretation}
\begin{definition}
    Pictorially, eliminating $X$ is equivalent to replacing all hyper-edges that include $X$ with their union minus $X$, and then removing $X$.
\end{definition}

\subsubsection{Elimination Ordering}
\begin{definition}
    The order that the variables are eliminated.
\end{definition}

\subsubsection{Elimination Width}
\begin{definition}
    The \textbf{elimination width} of a sequence of hyper-graphs is the \# of variables in the hyper-edge within the sequence with the most variables.
\end{definition}

\subsubsection{Heuristics for Elimination Ordering}
\begin{definition}
    Choose the elimination ordering to minimize the elimination width using the following heuristics:
    \begin{enumerate}
        \item Eliminate variable with the fewest parents.
        \item Eliminate variable with the smallest domain for its parents, where
        \[
        |\operatorname{dom}(\operatorname{pts}(X))| = \prod_{Z \in \operatorname{pnt}(X)} |\operatorname{dom}(Z)|.
        \]
        \item Eliminate variable with the smallest Markov blanket.
        \item Eliminate variable with the smallest domain for its Markov blanket, where
        \[
        |\operatorname{dom}(\operatorname{mbk}(X))| = \prod_{Z \in \operatorname{embk}(X)} |\operatorname{dom}(Z)|.
        \]
    \end{enumerate}
\end{definition}
\newpage

\subsection{Method 2: Inference via Sampling}
\begin{definition}
    Generate a large \# of samples and then approximate as:
    \[
    p(\mathbf{Q} \mid \mathbf{E}) \approx \frac{\# \text{ of samples w/ } \mathbf{Q} \text{ and } \mathbf{E}}{\# \text{ of samples w/ } \mathbf{E}}.
    \]
    \begin{itemize}
        \item As $\# \text{ of samples} \to \infty$, the approximation becomes exact.
    \end{itemize}
\end{definition}

\subsubsection{Inference via Sampling with Likelihood Weighting}
\begin{motivation}
    Most of the samples are wasted since they are not consistent with the evidence.
\end{motivation}

\begin{definition}
    Generate a large \# of samples and then approximate as:
    \[
    p(\mathbf{Q} \mid \mathbf{E}) \approx \frac{\text{weight of samples w/ } \mathbf{Q} \text{ and } \mathbf{E}}{\text{weight of samples w/ } \mathbf{E}}.
    \]
    \begin{itemize}
        \item Weight for each sample: Probability of forcing the evidence, i.e. probability of the evidence given the sample.
    \end{itemize}
\end{definition}
\newpage

\subsection{Canonical Problems:}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Caveman is deciding whether to go hunt for meat. He must take into account several factors:
        \begin{itemize}
            \item Weather
            \item Possibility of over-exertion
            \item Possibility encountering lion
        \end{itemize}

        These factors can result in Cavemen's death. His decision will ultimately depend on the \textbf{chances} of his death.
        \item \textbf{Binary Variables:}
        \begin{itemize}
            \item $W = \{\text{Sun}, \text{Rainy}\}$: Weather
            \item $H$: Whether the Cavemen goes hunting or not.
            \item $L$: Whether the Cavemen encounters a lion or not.
            \item $T$: Whether the Cavement is tired or not.
            \item $D$: Whether the Cavemen dies or not
        \end{itemize}
        \item \textbf{Problem:} Cavemen must decide whether to go hunting or not. 
        \begin{itemize}
            \item He must consider the conditional probabilities (i.e. dependence) of each event.
        \end{itemize}
    \end{enumerate}
\end{example}

\begin{warning}
    Have to be discrete. 
\end{warning}
\newpage

\subsubsection{Undirected Path Blocked?}
\begin{process}
    \begin{enumerate}
        \item \textbf{Given:} Undirected path $p$ and $\mathcal{K}$
        \item Check if any of the junctions on the undirected path are blocked given $\mathcal{K}$.
        \begin{itemize}
            \item i.e. Check if $X_1$ and $X_3$ of the junction are independent given $\mathcal{K}$.
        \end{itemize}
    \end{enumerate}
\end{process}

\subsubsection{Independence}
\begin{process}
    \begin{enumerate}
        \item Given a Bayesian network w/ 2 variables to find independence.
        \item Find all undirected paths between the 2 variables in the Bayesian network.
        \item Identify a set of variables, $\mathcal{K}$, that blocks all undirected paths.
        \item If all undirected paths are blocked, then the 2 variables are independent given $\mathcal{K}$.
    \end{enumerate}
\end{process}

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Bayesian network.
        \customFigure[0.25]{../Images/L6_7.png}{}
        \item \textbf{Problem:} $A$ and $E$ are 
        \begin{itemize}
            \item independent if $\mathcal{K}=$
            \item not necessarily independent for $\mathcal{K}=$
        \end{itemize}
        \item \textbf{Soln:}
        \begin{enumerate}
            \item \textbf{Undirected Paths:}
            \begin{itemize}
                \item $A \to G \to H \to B \to E$
                \item $A \to G \to D \to B \to E$
                \item $A \to C \to F \to B \to E$
                \item $A \to C \to D \to B \to E$
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \textbf{Independent:}
    \begin{center}
        \begin{tabular}{c}
            \toprule
            $\mathcal{K}$ \\
            \midrule
            $\{G,C\}$ \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $A \iff G \iff H \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(A,G,H),(e_1,e_2)\}$ is blocked given $G$ since $A,H$ independent given $G$ (causal chain)
                    \item $A \iff G \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(A,G,D),(e_1,e_2)\}$ is blocked given $G$ since $A,D$ independent given $G$ (causal chain)
                    \item $A \iff C \iff F \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(A,C,F),(e_1,e_2)\}$ is blocked given $C$ since $A,F$ independent given $C$ (causal chain) 
                    \item $A \iff C \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(A,C,D),(e_1,e_2)\}$ is blocked given $C$ since $A,D$ independent given $C$ (causal chain) 
                \end{itemize}} \\
            \midrule
            $\{D,F\}$ \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $A \iff G \iff H \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(G,H,B),(e_1,e_2)\}$ is blocked NOT given $H$ since $G,B$ independent NOT given $H$ (common effect)
                    \item $A \iff G \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(G,D,B),(e_1,e_2)\}$ is blocked given $D$ since $G,B$ independent given $D$ (causal chain)
                    \item $A \iff C \iff F \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(C,F,B),(e_1,e_2)\}$ is blocked given $F$ since $C,B$ independent given $F$ (causal chain)
                    \item $A \iff C \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(C,D,B),(e_1,e_2)\}$ is blocked given $D$ since $C,B$ independent given $D$ (causal chain)
                \end{itemize}} \\
            \toprule
        \end{tabular}
    \end{center}
    \vspace{1em}

    \textbf{Not Necessarily Independent:}
    \begin{center}
        \begin{tabular}{c}
            \toprule
            $\mathcal{K}$ \\
            \midrule
            $\{H,D,F\}$ \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $A \iff G \iff H \iff B \iff E$ is unblocked given $\mathcal{K}$ since $\mathcal{J} = \{(G,H,B),(e_1,e_2)\}$ is unblocked given $H$ since $G,B$ not independent given $H$ (common effect)
                    \item $A \iff G \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(G,D,B),(e_1,e_2)\}$ is blocked given $D$ (causal chain) since $G,B$ independent given $D$ (causal chain)
                    \item $A \iff C \iff F \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(C,F,B),(e_1,e_2)\}$ is blocked given $F$ since $C,B$ independent given $F$ (causal chain)
                    \item $A \iff C \iff D \iff B \iff E$  is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(C,D,B),(e_1,e_2)\}$ is blocked given $D$ since $C,B$ independent given $D$ (causal chain)
                \end{itemize}} \\
            \midrule
            \toprule
        \end{tabular}
    \end{center}
\end{example}
\newpage

\subsubsection{Bayesian Inference}
\begin{process}
    \begin{enumerate}
        \item Given Bayesian network w/ variables and their conditional probabilities.
        \item Find the probability of the query variable given the evidence variable, $p(\mathbf{Q} \mid \mathbf{E})$.
        \item Use $p(\mathbf{Q} \mid \mathbf{E}) = \frac{\sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} p(X_1, \dots, X_{|\mathcal{V}|})}{\sum_{\mathcal{V} \setminus \mathbf{E}} p(X_1, \dots, X_{|\mathcal{V}|})}$.
        \item Determine $p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i))$ using the Bayesian network. 
        \item Write out the summation of the numerator in an order using heuristics to determine elimination ordering. 
        \item Start with inner summation and work outwards.
        \item Calculate the probability of the query variable(s) given the evidence variable(s).
    \end{enumerate}
\end{process}

\begin{example} 
    \begin{enumerate}
        \item \textbf{Given:}
        \customFigure[0.5]{../Images/L6_9.png}{}
        \begin{center}
            \begin{tabular}{ll}
                \toprule
                \textbf{Variables} & \textbf{Values} \\
                \midrule
                $W$ & $P(\text{Sunny}) = 0.5 \mid P(\text{Rainy}) = 0.5$ \\
                \midrule
                $H$ & $P(h) = 0.5 \mid P(\lnot h) = 0.5$ \\
                \midrule
                $T$ & $P(t \mid \text{Sunny}, h) = 1 \mid P(t \mid \text{Sunny}, \lnot h) = 0.5 \mid P(t \mid \text{Rainy}, h) = 0.25 \mid P(t \mid \text{Rainy}, \lnot h) = 0$ \\
                & $P(\lnot t \mid \text{Sunny}, h) = 0 \mid P(\lnot t \mid \text{Sunny}, \lnot h) = 0.5 \mid P(\lnot t \mid \text{Rainy}, h) = 0.75 \mid P(\lnot t \mid \text{Rainy}, \lnot h) = 1$ \\
                \midrule
                $L$ & $P(l \mid h) = 1 \mid P(l \mid \lnot h) = 0.75$ \\
                & $P(\lnot l \mid h) = 0 \mid P(\lnot l \mid \lnot h) = 0.25$ \\
                \midrule
                $D$ & $P(d \mid t,l) = 0.75 \mid P(d \mid t,\lnot l) = 1 \mid P(d \mid \lnot t,l) = 0 \mid P(d \mid \lnot t,\lnot l) = 0$ \\
                & $P(\lnot d \mid t,l) = 0.25 \mid P(\lnot d \mid t,\lnot l) = 0 \mid P(\lnot d \mid \lnot t,l) = 1 \mid P(\lnot d \mid \lnot t,\lnot l) = 1$ \\
                \bottomrule
            \end{tabular}
        \end{center}
        \item \textbf{Problem:} $p(d \mid h)$? 
        \item \textbf{Soln:}
        \begin{enumerate}
            \item $p(d \mid h) = \frac{p(d,h)}{p(h)} = \frac{\sum_{W,T,L} p(W,h,T,L,d)}{\sum_{W,T,L,D} p(W,h,T,L,d)}$ by definition of query and evidence equations.
            \item $p(W,h,T,L,D) = p(h) p(W) p(L \mid h) p(t \mid W,h) p(D \mid T,L)$ by Bayesian network and $p(X_1, \dots, X_{|\mathcal{V}|}) = p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i))$.
        \end{enumerate}
        \begin{center}
            \begin{tabular}{l}
                \toprule
                \textbf{Summation} \\
                \toprule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $\text{Numerator}: \underbrace{p(h) \sum_L p(L \mid h) \underbrace{\sum_T p(D \mid T,L) \underbrace{\sum_W p(W) p(T \mid W,h)}_{g_1(T)}}_{g_2(L,D)}}_{g_3(D)}$
                \end{center}} \\
                \toprule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $g_1(T) = p(\text{Sunny}) p(T \mid \text{Sunny},h) + p(\text{Rainy}) p(T \mid \text{Rainy},h)$
                \end{center}} \\
                $g_1(t) = p(\text{Sunny}) p(t \mid \text{Sunny},h) + p(\text{Rainy}) p(t \mid \text{Rainy},h) = 0.5 \cdot 1 + 0.5 \cdot 0.25 = 0.625$ \\
                $g_1(\lnot t) =  p(\text{Sunny}) p(\lnot t \mid \text{Sunny},h) + p(\text{Rainy}) p(\not t \mid \text{Rainy},h) = 0.5 \cdot 0 + 0.5 \cdot 0.75 = 0.375$ \\
                \midrule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $g_2(L,D) = p(D \mid t,L) g_1(t) + p(D \mid \lnot t, L) g_1(\lnot t)$ 
                \end{center}} \\
                $g_2(l,d) = p(d \mid t,l) g_1(t) + p(d \mid \lnot t, l) g_1(\lnot t) = 0.75 \cdot 0.625 + 0 \cdot 0.375 = 0.46875$ \\
                $g_2(l,\lnot d) = p(\lnot d \mid t,l) g_1(t) + p(\lnot d \mid \lnot t, l) g_1(\lnot t) = 0.25 \cdot 0.625 + 1 \cdot 0.375 = 0.53125$ \\
                $g_2(\lnot l,d) = p(d \mid t,\lnot l) g_1(t) + p(d \mid \lnot t, \lnot l) g_1(\lnot t) = 1 \cdot 0.625 + 0 \cdot 0.375 = 0.625$ \\
                $g_2(\lnot l,\lnot d) = p(\lnot d \mid t,\lnot l) g_1(t) + p(\lnot d \mid \lnot t, \lnot l) g_1(\lnot t) = 0 \cdot 0.625 + 1 \cdot 0.375 = 0.375$ \\
                \midrule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $g_3(D) = p(h) p(l \mid h) g_2(l,D) + p(h) p(\lnot l \mid h) g_2(\lnot l, D)$ 
                \end{center}} \\
                $g_3(d) = p(h) p(l \mid h) g_2(l,d) + p(h) p(\lnot l \mid h) g_2(\lnot l,d) = (0.5)(1)(0.46875) + (0.5)(0)(0.625) = 0.234375$ \\
                $g_3(\lnot d) = p(h) p(l \mid h) g_2(l,\lnot d) + p(h) p(\lnot l \mid h) g_2(\lnot l,\lnot d) = (0.5)(1)(0.53125) + (0.5)(0)(0.375) = 0.265625$ \\
                \bottomrule
            \end{tabular}
        \end{center}
        \vspace{1em}

        \begin{equation*}
            p(d \mid h) = \frac{g_3(d)}{g_3(d) + g_3(\lnot d)} = \frac{0.234375}{0.234375 + 0.265625} = \frac{0.234375}{0.5} = 0.46875
        \end{equation*}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Hypergraph}
\begin{process}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{process}

\subsubsection{Inference via Sampling}
\begin{process}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{process}

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:}
        \item \textbf{Problem:}
    \end{enumerate}
\end{example}