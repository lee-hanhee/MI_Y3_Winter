\subsection{Problem Setup}
\begin{definition}
    Given a Bayesian network, $\mathcal{B} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V} = \{X_1, \dots, X_{|\mathcal{V}|}\}$, we want to find the value of:
    \[
    \operatorname{pr}(\mathbf{Q} \mid \mathbf{E}) := \operatorname{pr}(Q_1, \dots, Q_{|\mathbf{Q}|} \mid E_1, \dots, E_{|\mathbf{E}|}) = \frac{\sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} p(X_1, \dots, X_{|\mathcal{V}|})}
    {\sum_{\mathcal{V} \setminus \mathbf{E}} p(X_1, \dots, X_{|\mathcal{V}|})}
    \]
    \[
    \operatorname{pr}(\mathbf{Q} \mid \mathbf{E}) \propto 
    \sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} 
    \left( p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i)) \right)
    \]


    \begin{itemize}
        \item $\mathbf{Q} = \{Q_1, \dots, Q_{|\mathbf{Q}|}\}$: Query variables
        \item $\mathbf{E} = \{E_1, \dots, E_{|\mathbf{E}|}\} \subseteq \mathcal{V}$: Evidence variables
        \item $\mathbf{Q} \cap \mathbf{E} = \emptyset$.
    \end{itemize}
\end{definition}

\begin{warning}
    \begin{itemize}
        \item Denominator: Normalization constant (assuming $\mathbf{E}$ is fixed)
        \item Therefore, only need to compute numerator (w/o specifying $\mathbf{Q}$), which we can then normalize w.r.t. $\mathbf{Q}$
    \end{itemize}
\end{warning}

\subsubsection{Joint Distribution in a Bayesian Network}
\begin{derivation}
    For any joint distribution, the following factorization holds:
    \begin{equation*}
        p(X_1,\ldots,X_{|p|})  = p(X_1) \prod_{i \neq 1} p(X_i \mid X_1,\ldots, X_{i-1})
    \end{equation*}

    \textbf{Bayesian Network Conditions:} If 
    \begin{itemize}
        \item at least 1 variable will be an orphan (i.e. no parents)
        \item no variable is both ancestor and descendant of another. 
    \end{itemize}
    \vspace{1em}

    then this allows us to order $X_1,\ldots,X_{|\mathcal{V}|}$,  so that if $X_j$ is a descendent of $X_i$, then for any $j > i$, 
    \begin{equation*}
        \text{pts}(X_i) \subseteq \{X_1,\ldots,X_{i-1}\} \text{ and } X_1,\ldots,X_{i-1} \notin \text{des}(X_i)
    \end{equation*}

    Therefore, using the consequence of dependence separation, then 
    \begin{equation*}
        p(X_1,\ldots,X_{|\mathcal{V}|}) = p(X_1) \prod_{i \neq 1} p(X_i \mid \text{pts}(X_i))
    \end{equation*}
\end{derivation}

\subsection{Method 1: Bayesian Network Inference}

\subsubsection{Markov Blanket}
\begin{definition} 
    The \textbf{Markov blanket} of a variable $X$, denoted $\operatorname{mbk}(X)$, consists of the following variables:
    \begin{itemize}
        \item $X$'s children
        \item $X$'s parents
        \item The other parents of $X$'s children, excluding $X$ itself.
    \end{itemize}
    which is when a variable, $X$, is "eliminated", the resulting factor's scope is the Markov blanket of $X$.
\end{definition}

\subsubsection{Graphical Interpretation}
\begin{notes}
    Pictorially, eliminating $X$ is equivalent to replacing all hyper-edges that include $X$ with their union minus $X$, and then removing $X$.
\end{notes}

\subsubsection{Elimination Ordering}
\begin{definition}
    The order that the variables are eliminated.
    \begin{itemize}
        \item This creates a sequence of hyper-graphs that depend on the elimination ordering.
    \end{itemize}
\end{definition}

\subsubsection{Elimination Width}
\begin{definition}
    The \textbf{elimination width} of a sequence of hyper-graphs is the \# of variables in the hyper-edge within the sequence with the most variables.
\end{definition}

\subsubsection{Heuristics for Elimination Ordering}
\begin{definition}
    Choose the elimination ordering to minimize the elimination width using the following heuristics:
    \begin{enumerate}
        \item Eliminate variable with the fewest parents.
        \item Eliminate variable with the smallest domain for its parents, where
        \[
        |\operatorname{dom}(\operatorname{pts}(X))| = \prod_{Z \in \operatorname{pnt}(X)} |\operatorname{dom}(Z)|.
        \]
        \item Eliminate variable with the smallest Markov blanket.
        \item Eliminate variable with the smallest domain for its Markov blanket, where
        \[
        |\operatorname{dom}(\operatorname{mbk}(X))| = \prod_{Z \in \operatorname{embk}(X)} |\operatorname{dom}(Z)|.
        \]
    \end{enumerate}
\end{definition}

\begin{warning}
    Choosing the variable with the smallest domain for its Markov blanket is the most effective heuristic.
\end{warning}
\newpage

\subsection{Method 2: Inference via Sampling}
\begin{definition}
    Generate a large \# of samples and then approximate as:
    \[
    p(\mathbf{Q} \mid \mathbf{E}) \approx \frac{\# \text{ of samples w/ } \mathbf{Q} \text{ and } \mathbf{E}}{\# \text{ of samples w/ } \mathbf{E}}.
    \]
    \begin{itemize}
        \item As $\# \text{ of samples} \to \infty$, the approximation becomes exact.
    \end{itemize}
\end{definition}

\subsubsection{Inference via Sampling with Likelihood Weighting}
\begin{motivation}
    Most of the samples are wasted since they are not consistent with the evidence.
\end{motivation}

\begin{definition}
    Generate a large \# of samples and then approximate as:
    \[
    p(\mathbf{Q} \mid \mathbf{E}) \approx \frac{\text{weight of samples w/ } \mathbf{Q} \text{ and } \mathbf{E}}{\text{weight of samples w/ } \mathbf{E}}.
    \]
    \begin{itemize}
        \item Weight for each sample: Probability of forcing the evidence, i.e. probability of the evidence given the sample.
    \end{itemize}
\end{definition}
\newpage

\subsection{Canonical Problems:}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Caveman is deciding whether to go hunt for meat. He must take into account several factors:
        \begin{itemize}
            \item Weather
            \item Possibility of over-exertion
            \item Possibility encountering lion
        \end{itemize}

        These factors can result in Cavemen's death. His decision will ultimately depend on the \textbf{chances} of his death.
        \item \textbf{Binary Variables:}
        \begin{itemize}
            \item $W = \{\text{Sun}, \text{Rainy}\}$: Weather
            \item $H$: Whether the Cavemen goes hunting or not.
            \item $L$: Whether the Cavemen encounters a lion or not.
            \item $T$: Whether the Cavement is tired or not.
            \item $D$: Whether the Cavemen dies or not
        \end{itemize}
        \item \textbf{Problem:} Cavemen must decide whether to go hunting or not. 
        \begin{itemize}
            \item He must consider the conditional probabilities (i.e. dependence) of each event.
        \end{itemize}
    \end{enumerate}
\end{example}

\begin{warning}
    Have to be discrete. 
\end{warning}
\newpage

\subsubsection{Undirected Path Blocked?}
\begin{process}
    \begin{enumerate}
        \item \textbf{Given:} Undirected path $p$ and $\mathcal{K}$
        \item Check if any of the junctions on the undirected path are blocked given $\mathcal{K}$.
        \begin{itemize}
            \item i.e. Check if $X_1$ and $X_3$ of the junction are independent given $\mathcal{K}$.
        \end{itemize}
    \end{enumerate}
\end{process}

\subsubsection{Independence}
\begin{process}
    \begin{enumerate}
        \item Given a Bayesian network w/ 2 variables to find independence.
        \item Find all undirected paths between the 2 variables in the Bayesian network.
        \item Identify a set of variables, $\mathcal{K}$, that block at least one junction in all undirected paths.
        \begin{itemize}
            \item Test a junction by seeing junction given relationships. 
        \end{itemize}
        \item If all undirected paths are blocked, then the 2 variables are independent given $\mathcal{K}$.
    \end{enumerate}
\end{process}

\begin{warning}
    \begin{itemize}
        \item Be careful of common effect, in which it is blocked by default. 
    \end{itemize}
\end{warning}

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Bayesian network.
        \customFigure[0.25]{../Images/L6_7.png}{}
        \item \textbf{Problem:} $A$ and $E$ are 
        \begin{itemize}
            \item independent if $\mathcal{K}=$
            \item not necessarily independent for $\mathcal{K}=$
        \end{itemize}
        \item \textbf{Soln:}
        \begin{enumerate}
            \item \textbf{Undirected Paths:}
            \begin{itemize}
                \item $A \to G \to H \to B \to E$
                \item $A \to G \to D \to B \to E$
                \item $A \to C \to F \to B \to E$
                \item $A \to C \to D \to B \to E$
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \textbf{Independent:}
    \begin{center}
        \begin{tabular}{c}
            \toprule
            $\mathcal{K}$ \\
            \midrule
            $\{G,C\}$ \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $A \iff G \iff H \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(A,G,H),(e_1,e_2)\}$ is blocked given $G$ since $A,H$ independent given $G$ (causal chain)
                    \item $A \iff G \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(A,G,D),(e_1,e_2)\}$ is blocked given $G$ since $A,D$ independent given $G$ (causal chain)
                    \item $A \iff C \iff F \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(A,C,F),(e_1,e_2)\}$ is blocked given $C$ since $A,F$ independent given $C$ (causal chain) 
                    \item $A \iff C \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(A,C,D),(e_1,e_2)\}$ is blocked given $C$ since $A,D$ independent given $C$ (causal chain) 
                \end{itemize}} \\
            \midrule
            $\{D,F\}$ \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $A \iff G \iff H \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(G,H,B),(e_1,e_2)\}$ is blocked NOT given $H$ since $G,B$ independent NOT given $H$ (common effect)
                    \item $A \iff G \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(G,D,B),(e_1,e_2)\}$ is blocked given $D$ since $G,B$ independent given $D$ (causal chain)
                    \item $A \iff C \iff F \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(C,F,B),(e_1,e_2)\}$ is blocked given $F$ since $C,B$ independent given $F$ (causal chain)
                    \item $A \iff C \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(C,D,B),(e_1,e_2)\}$ is blocked given $D$ since $C,B$ independent given $D$ (causal chain)
                \end{itemize}} \\
            \toprule
        \end{tabular}
    \end{center}
    \vspace{1em}

    \textbf{Not Necessarily Independent:}
    \begin{center}
        \begin{tabular}{c}
            \toprule
            $\mathcal{K}$ \\
            \midrule
            $\{H,D,F\}$ \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $A \iff G \iff H \iff B \iff E$ is unblocked given $\mathcal{K}$ since $\mathcal{J} = \{(G,H,B),(e_1,e_2)\}$ is unblocked given $H$ since $G,B$ not independent given $H$ (common effect)
                    \item $A \iff G \iff D \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(G,D,B),(e_1,e_2)\}$ is blocked given $D$ (causal chain) since $G,B$ independent given $D$ (causal chain)
                    \item $A \iff C \iff F \iff B \iff E$ is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(C,F,B),(e_1,e_2)\}$ is blocked given $F$ since $C,B$ independent given $F$ (causal chain)
                    \item $A \iff C \iff D \iff B \iff E$  is blocked given $\mathcal{K}$ since $\mathcal{J} = \{(C,D,B),(e_1,e_2)\}$ is blocked given $D$ since $C,B$ independent given $D$ (causal chain)
                \end{itemize}} \\
            \midrule
            \toprule
        \end{tabular}
    \end{center}
\end{example}
\newpage

\begin{example}
    Determine all subsets of $\{B,C,D,F,G,H\}$ for which $A$ and $E$ are independent. 
    \customFigure[0.25]{../Images/LR_1.png}{}
    \begin{enumerate}
        \item \textbf{Undirected Paths:}
        \begin{itemize}
            \item $A \to G \to H \to B \to E$
            \item $A \to G \to D \to B \to E$
            \item $A \to C \to F \to B \to E$
            \item $A \to C \to D \to B \to E$
        \end{itemize}
    \end{enumerate}
    \begin{center}
        \begin{tabular}{c}
            \toprule
            $\mathcal{K}$ \\
            \midrule
            $\{B\}$ (Any subset that contains $B$ will be independent) \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $AGHBE$ is b given $\mathcal{K}$ since $\mathcal{J} = \{(H,B,E),(e_1,e_2)\}$ is b since $H,E$ indep. given $B$ (causal chain)
                    \item $AGDBE$ is b given $\mathcal{K}$ since $\mathcal{J} = \{(D,B,E),(e_1,e_2)\}$ is b since $D,E$ indep. given $B$ (causal chain)
                    \item $ACFBE$ is b given $\mathcal{K}$ since $\mathcal{J} = \{(F,B,E),(e_1,e_2)\}$ is b since $F,E$ indep. given $B$ (causal chain) 
                    \item $ACDBE$ is b given $\mathcal{K}$ since $\mathcal{J} = \{(D,B,E),(e_1,e_2)\}$ is b since $D,E$ indep. given $B$ (causal chain) 
                \end{itemize}} \\
            \midrule
            $\{C\}$ (Not independent) \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $AGDBE$ is ub given $\mathcal{K}$ since $\forall \mathcal{J}$ on $p$, all are ub.
                \end{itemize}} \\
            \midrule
            $\{D\}$ (Not indepedent) \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $ACFBE$ is ub given $\mathcal{K}$ since $\forall \mathcal{J}$ on $p$, all are ub.
                \end{itemize}} \\
            \midrule
            $\{F\}$ (Not independent) \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $AGDBE$ is ub given $\mathcal{K}$ since $\forall \mathcal{J}$ on $p$, all are ub.
                \end{itemize}} \\
            \midrule
            $\{G\}$ (Not independent) \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $ACFBE$ is ub given $\mathcal{K}$ since $\forall \mathcal{J}$ on $p$, all are ub.
                \end{itemize}} \\
            \midrule
            $\{H\}$ (Not independent) \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $ACFBE$ is ub given $\mathcal{K}$ since $\forall \mathcal{J}$ on $p$, all are ub.
                \end{itemize}} \\
            \toprule
        \end{tabular}
    \end{center}
\end{example}
\newpage

\begin{example}
    \begin{center}
        \begin{tabular}{c}
            \toprule
            $\mathcal{K}$ \\
            \midrule
            $\{C,D\}$ (Any subset that contains $C$ and $D$ except $H$ will be independent) \\
            \multicolumn{1}{p{\linewidth}}{
                \begin{itemize}
                    \item $AGHBE$ is b given $\mathcal{K}$ since $\mathcal{J} = \{(G,H,B),(e_1,e_2)\}$ is b since $G,B$ indep. not given $H$ (causal effect)
                    \item $AGDBE$ is b given $\mathcal{K}$ since $\mathcal{J} = \{(G,D,B),(e_1,e_2)\}$ is b since $G,B$ indep. given $D$ (causal chain)
                    \item $ACFBE$ is b given $\mathcal{K}$ since $\mathcal{J} = \{(A,C,F),(e_1,e_2)\}$ is b since $A,F$ indep. given $C$ (causal chain)
                    \item $ACDBE$ is b given $\mathcal{K}$ since $\mathcal{J} = \{(A,C,D),(e_1,e_2)\}$ is b since $A,D$ indep. given $C$ (causal chain)
                \end{itemize}} \\
            \midrule
            $\ldots$ \\
            \toprule
        \end{tabular}
    \end{center}
\end{example}

\begin{example}
    Any set with 
    \begin{equation*}
        B \lor [(G \lor (D \land (\lnot H))) \land (C \lor F)]
    \end{equation*}
\end{example}
\newpage

\subsubsection{Bayesian Inference}
\begin{process}
    \begin{enumerate}
        \item Given Bayesian network w/ variables and their conditional probabilities.
        \item Find the probability of the query variable given the evidence variable, $p(\mathbf{Q} \mid \mathbf{E})$.
        \item Use $p(\mathbf{Q} \mid \mathbf{E}) = \frac{\sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} p(X_1, \dots, X_{|\mathcal{V}|})}{\sum_{\mathcal{V} \setminus \mathbf{E}} p(X_1, \dots, X_{|\mathcal{V}|})}$.
        \item Determine $p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i))$ using the Bayesian network. 
        \item Write out the summation of the numerator in an order using heuristics to determine elimination ordering. 
        \item Start with inner summation and work outwards.
        \item Calculate the probability of the query variable(s) given the evidence variable(s).
    \end{enumerate}
\end{process}

\begin{example} 
    \begin{enumerate}
        \item \textbf{Given:}
        \customFigure[0.5]{../Images/L6_9.png}{}
        \begin{center}
            \begin{tabular}{ll}
                \toprule
                \textbf{Variables} & \textbf{Values} \\
                \midrule
                $W$ & $P(\text{Sunny}) = 0.5 \mid P(\text{Rainy}) = 0.5$ \\
                \midrule
                $H$ & $P(h) = 0.5 \mid P(\lnot h) = 0.5$ \\
                \midrule
                $T$ & $P(t \mid \text{Sunny}, h) = 1 \mid P(t \mid \text{Sunny}, \lnot h) = 0.5 \mid P(t \mid \text{Rainy}, h) = 0.25 \mid P(t \mid \text{Rainy}, \lnot h) = 0$ \\
                & $P(\lnot t \mid \text{Sunny}, h) = 0 \mid P(\lnot t \mid \text{Sunny}, \lnot h) = 0.5 \mid P(\lnot t \mid \text{Rainy}, h) = 0.75 \mid P(\lnot t \mid \text{Rainy}, \lnot h) = 1$ \\
                \midrule
                $L$ & $P(l \mid h) = 1 \mid P(l \mid \lnot h) = 0.75$ \\
                & $P(\lnot l \mid h) = 0 \mid P(\lnot l \mid \lnot h) = 0.25$ \\
                \midrule
                $D$ & $P(d \mid t,l) = 0.75 \mid P(d \mid t,\lnot l) = 1 \mid P(d \mid \lnot t,l) = 0 \mid P(d \mid \lnot t,\lnot l) = 0$ \\
                & $P(\lnot d \mid t,l) = 0.25 \mid P(\lnot d \mid t,\lnot l) = 0 \mid P(\lnot d \mid \lnot t,l) = 1 \mid P(\lnot d \mid \lnot t,\lnot l) = 1$ \\
                \bottomrule
            \end{tabular}
        \end{center}
        \item \textbf{Problem:} $p(d \mid h)$? 
        \item \textbf{Soln:}
        \begin{enumerate}
            \item $p(d \mid h) = \frac{p(d,h)}{p(h)} = \frac{\sum_{W,T,L} p(W,h,T,L,d)}{\sum_{W,T,L,D} p(W,h,T,L,d)}$ by definition of query and evidence equations.
            \item $p(W,h,T,L,D) = p(h) p(W) p(L \mid h) p(t \mid W,h) p(D \mid T,L)$ by Bayesian network and $p(X_1, \dots, X_{|\mathcal{V}|}) = p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i))$.
        \end{enumerate}
        \begin{center}
            \begin{tabular}{l}
                \toprule
                \textbf{Summation} \\
                \toprule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $\text{Numerator}: \underbrace{p(h) \sum_L p(L \mid h) \underbrace{\sum_T p(D \mid T,L) \underbrace{\sum_W p(W) p(T \mid W,h)}_{g_1(T)}}_{g_2(L,D)}}_{g_3(D)}$
                \end{center}} \\
                \toprule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $g_1(T) = p(\text{Sunny}) p(T \mid \text{Sunny},h) + p(\text{Rainy}) p(T \mid \text{Rainy},h)$
                \end{center}} \\
                $g_1(t) = p(\text{Sunny}) p(t \mid \text{Sunny},h) + p(\text{Rainy}) p(t \mid \text{Rainy},h) = 0.5 \cdot 1 + 0.5 \cdot 0.25 = 0.625$ \\
                $g_1(\lnot t) =  p(\text{Sunny}) p(\lnot t \mid \text{Sunny},h) + p(\text{Rainy}) p(\not t \mid \text{Rainy},h) = 0.5 \cdot 0 + 0.5 \cdot 0.75 = 0.375$ \\
                \midrule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $g_2(L,D) = p(D \mid t,L) g_1(t) + p(D \mid \lnot t, L) g_1(\lnot t)$ 
                \end{center}} \\
                $g_2(l,d) = p(d \mid t,l) g_1(t) + p(d \mid \lnot t, l) g_1(\lnot t) = 0.75 \cdot 0.625 + 0 \cdot 0.375 = 0.46875$ \\
                $g_2(l,\lnot d) = p(\lnot d \mid t,l) g_1(t) + p(\lnot d \mid \lnot t, l) g_1(\lnot t) = 0.25 \cdot 0.625 + 1 \cdot 0.375 = 0.53125$ \\
                $g_2(\lnot l,d) = p(d \mid t,\lnot l) g_1(t) + p(d \mid \lnot t, \lnot l) g_1(\lnot t) = 1 \cdot 0.625 + 0 \cdot 0.375 = 0.625$ \\
                $g_2(\lnot l,\lnot d) = p(\lnot d \mid t,\lnot l) g_1(t) + p(\lnot d \mid \lnot t, \lnot l) g_1(\lnot t) = 0 \cdot 0.625 + 1 \cdot 0.375 = 0.375$ \\
                \midrule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $g_3(D) = p(h) p(l \mid h) g_2(l,D) + p(h) p(\lnot l \mid h) g_2(\lnot l, D)$ 
                \end{center}} \\
                $g_3(d) = p(h) p(l \mid h) g_2(l,d) + p(h) p(\lnot l \mid h) g_2(\lnot l,d) = (0.5)(1)(0.46875) + (0.5)(0)(0.625) = 0.234375$ \\
                $g_3(\lnot d) = p(h) p(l \mid h) g_2(l,\lnot d) + p(h) p(\lnot l \mid h) g_2(\lnot l,\lnot d) = (0.5)(1)(0.53125) + (0.5)(0)(0.375) = 0.265625$ \\
                \bottomrule
            \end{tabular}
        \end{center}
        \vspace{1em}

        \begin{equation*}
            p(d \mid h) = \frac{g_3(d)}{g_3(d) + g_3(\lnot d)} = \frac{0.234375}{0.234375 + 0.265625} = \frac{0.234375}{0.5} = 0.46875
        \end{equation*}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{center}
        \begin{tabular}{l}
            \toprule
            \textbf{Summation} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\text{Numerator}: \underbrace{p(h) \sum_L p(L \mid h) \underbrace{\sum_W p(W) \underbrace{\sum_T p(T \mid W,h) p(D \mid T,L)}_{g_1(W,D,L)}}_{g_2(D,L)}}_{g_3(D)}$
            \end{center}} \\
            \toprule 
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\text{Numerator}: \underbrace{p(h) \sum_W p(W) \underbrace{\sum_T p(T \mid W,h) \underbrace{\sum_L p(L \mid h) p(D \mid T,L)}_{g_1(D,T)}}_{g_2(D,W)}}_{g_3(D)}$
            \end{center}} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\text{Numerator}: \underbrace{p(h) \sum_W p(W) \underbrace{\sum_L p(L \mid h) \underbrace{\sum_T p(T \mid W,h) p(D \mid T,L)}_{g_1(W,D,L)}}_{g_2(W,D)}}_{g_3(D)}$
            \end{center}} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\text{Numerator}: \underbrace{p(h) \sum_T p(T \mid W,h) \underbrace{\sum_W p(W) \underbrace{\sum_L p(L \mid h) p(D \mid T,L)}_{g_1(D,T)}}_{g_2(D,T)}}_{g_3(D)}$
            \end{center}} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\ldots$
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:}
        \customFigure[0.5]{../Images/L7_1.png}{}
        \item \textbf{Problem:} Compute $\Pr(s = 1 \mid t = 1)$ if $\Pr(G = 1) = 0.3$, $\Pr(E = 1) = 0.4$, and the conditional probability tables for $S, Y,$ and $T$ are given below.
        \item \textbf{Solution:}
        \begin{enumerate}
            \item $p(s=1 \mid t=1) = \frac{p(s=1,t=1)}{p(t=1)} = \frac{\sum_{E,G,Y} p(E,G,Y,s=1,t=1)}{\sum_S p(t=1,S)}$ 
            \item $p(E,G,Y,s=1,t=1) = p(G) p(E) p(s=1 \mid G, E) p(t=1 \mid s = 1) p(Y \mid s = 1)$
            \begin{itemize}
                \item Conditional probability and individual probabilities come from Bayesian network, and set $t,s=1$ due to the query and evidence variables.
            \end{itemize}
        \end{enumerate}
    \end{enumerate}

    \begin{center}
        \begin{tabular}{l}
            \toprule
            \textbf{Summation} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\text{Numerator}: \underbrace{p(t=1 \mid s=1) \sum_E p(E) \underbrace{\sum_G p(G) p(s=1 \mid G,E) \underbrace{\sum_Y p(Y \mid s=1)}_{g_1}}_{g_2}}_{g_3}$
            \end{center}} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $g_1 = p(Y=1 \mid S=1) + p(Y=0 \mid S=1) = 0.9 + 0.1 = 1$
            \end{center}} \\
            \midrule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $g_2(E) = (p(g=1) p(s=1 \mid g=1, E) + p(g=0) p(s=1 \mid g=0, E)) g_1$
            \end{center}} \\
            $g_2(e=1,s=1) = 0.3(0.7) + 0.7(0.4) = 0.49$ \\
            $g_2(e=0,s=1) = 0.3(0.5) + 0.7(0.1) = 0.22$ \\
            $g_2(e=1,s=0) = 0.3(0.3) + 0.7(0.6) = 0.51$ \\
            $g_2(e=0,s=0) = 0.3(0.5) + 0.7(0.9) = 0.78$ \\
            \midrule
            \multicolumn{1}{p{\linewidth}}{
            \begin{itemize}
                \item $g_3(t=1 \mid s=1) = 0.9 p(e=1) g_2(e=1) + 0.9 p(e=0) g_2(e=0) = 0.9(0.4)(0.49) + 0.9(0.6)(0.22) = 0.2952$ 
                \item $g_3(t=1 \mid s=0) = 0.2 p(e=1) g_2(e=1) + 0.2 p(e=0) g_2(e=0) = 0.2(0.4)(0.51) + 0.2(0.6)(0.78) = 0.1344$
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}

    \begin{equation*}
        p(s=1 \mid t=1) = \frac{g_3}{\sum_S p(t=1,S)} = \frac{0.2952}{0.2952 + 0.1344} = \frac{0.2952}{0.4296} = 0.6875
    \end{equation*}
\end{example}
\newpage

\subsubsection{Hypergraph}
\begin{process} Process of eliminating a variable. 
    \begin{enumerate}
        \item Create a Hyper-graph by creating a node for each variable. 
        \item Create hyper-edges (factors) by circling the nodes that are probabilities in the joint distribution (i.e. numerator)
        \item Select a variable $v$ that we are summing over. 
        \begin{enumerate}
            \item Circle all the variables that have $v$ in their hyperedge into one big hyperedge (i.e. union of hyper-edges).
            \item Eliminate $v$ by removing the node. 
            \item Calculate the factor by determining the domain of each variable and summing their domains. 
        \end{enumerate}
        \item Reset the problem and repeat for all other $v$. 
        \item Select the smallest factor to eliminate first.
        \item Repeat until all variables are eliminated to determine the best ordering of elimination. 
        \begin{itemize}
            \item The first eliminated variable will be the inner sum. 
        \end{itemize}
    \end{enumerate}
\end{process}

\begin{example}
    \customFigure[0.5]{../Images/L6_11.png}{}
    \begin{itemize}
        \item Since these are all binary variables, we are selecting the factor with the least number of variables to eliminate first.
    \end{itemize}
    \customFigure[0.5]{../Images/L7_0.png}{}
\end{example}
\newpage

\subsubsection{Inference via Sampling}
\begin{process}
    \begin{enumerate}
        \item Given samples 
        \item Calculate number of samples w/ the query and evidence variables.
        \item Calculate number of samples w/ the evidence variables.
        \item Approximate the probability of the query variable given the evidence variable by dividing the \# of samples w/ the query and evidence variables by the \# of samples w/ the evidence variables.
    \end{enumerate}
\end{process}

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Samples 
        \customFigure[0.5]{../Images/L6_12.png}{}
        \item \textbf{Problem:} Find the probability of $p(d \mid h)$.
        \item \textbf{Soln:} $p(d \mid h) \approx \frac{\# \text{ of samples w/ } d \text{ and } h}{\# \text{ of samples w/ } h} = \frac{3}{5} = 0.6$.
    \end{enumerate}
\end{example}