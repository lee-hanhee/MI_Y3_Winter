\subsection{Probably Approximately Correct (PAC) Estimations}

\subsubsection{Hoeffding's Inequality}
\begin{definition}
    For any $\epsilon > 0$,
    \begin{equation}
        \mathbb{P}(|\nu - \mu| \geq \epsilon) \leq 2e^{-2\epsilon^2N}
    \end{equation}
    \begin{itemize}
        \item $\mu$: Probabillity of an event.
        \item $\nu$: Relative frequency in a sample size $N$.
        \item $\mu \overset{?}{\approx} \nu $: $\mu$ is probably approximately equal to $\nu$. As $N \rightarrow \infty$: $\nu \rightarrow \mu$
        \item $\epsilon$: Tolerance (i.e. how close we want $\nu$ to be to $\mu$).
        \begin{itemize}
            \item $\epsilon \rightarrow 0$: $\nu = \mu$
            \item $\epsilon \rightarrow \infty$: $\nu \rightarrow \mu$
        \end{itemize}
    \end{itemize}
\end{definition}

\begin{warning}
    We can approximate the true distribution with high probability by taking a large enough sample size, NOT guaranteeing that we can find the true distribution.
    \begin{itemize}
        \item Don't need to know where this theorem comes from.
    \end{itemize}
\end{warning}

Consider determining the class of a randomly chosen target point. If we ask a K-ary question about the points in $\mathcal{D}$

\subsection{PAC Learning}
\subsubsection{Error}
\begin{definition}
    \begin{itemize}
        \item \textbf{Out-Sample Error:}
        \begin{equation*}
            E_{\text{out}} = \mathbb{P}[f \neq h]
        \end{equation*}
        \item \textbf{In-Sample Error:}
        \begin{equation*}
            E_{\text{in}} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}[f(x^{(i)}) \neq h(x^{(i)})]
        \end{equation*}
    \end{itemize}
\end{definition}

\subsubsection{Two Conflicting Requirements}
\begin{definition}
    \begin{itemize}
        \item $E_{\text{in}}(h) \stackrel{?}{\approx} E_{\text{out}}$ requires small $|\mathcal{H}|$ (generalization)
        \item $E_{\text{in}}(h) \approx 0$ requires large $|\mathcal{H}|$ (discrimination)
    \end{itemize}
\end{definition}

\subsubsection{S}
\begin{definition}
    If we endow $\mathcal{F}$ w/ probability distribution, $P : \mathcal{X} \to [0,1]$, then $E_{\text{out}}$ is analogous to $\mu$ and $E_{\text{in}}(h)$ is analogous to $\nu$. If we then assume that $h$ is chosen from a set of hypotheses $\mathcal{H}$, we can derive a (loose) upper-bound on $|E_{\text{out}} - E_{\text{in}}|$:

    \begin{align*}
        \mathbb{P} \left[ \bigvee_{h \in \mathcal{H}} \left( |E_{\text{out}} - E_{\text{in}}(h)| > \varepsilon \right) \right]
        &\leq \sum_{h \in \mathcal{H}} \mathbb{P} \left[ |E_{\text{out}} - E_{\text{in}}(h)| > \varepsilon \right] \\
        &\leq \sum_{h \in \mathcal{H}} 2e^{-2\varepsilon^2 N} \\
        &= 2 |\mathcal{H}| e^{-2\varepsilon^2 N} 
    \end{align*}
\end{definition}

\begin{warning}
    If the events are highly correlated, then the union bound is not tight.
\end{warning}
\newpage

\begin{example}
    Take $N$ i.i.d. samples (i.e. take out a ball from an urn, record its color, and put it back in).
    \begin{itemize}
        \item $\nu \rightarrow \mu$ (empirical distribution $\rightarrow$ true distribution) as $N \rightarrow \infty$
    \end{itemize}
\end{example}



