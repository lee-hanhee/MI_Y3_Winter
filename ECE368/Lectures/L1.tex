\begin{summary}

\end{summary}

\begin{faq}
    \begin{itemize}
        \item How to study? Practice, practice.
        \item What textbooks? Use 2024 version of Murphy, Leon Garcia as main reference, Bishop, 4th textbook is intro.
        \item How is HW graded? Effort, and tutorials are used to explain soln. 
    \end{itemize}
\end{faq}

\subsection{Sample Space}
\begin{motivation}
    If you have 4 sheeps and a flea, the probability that starting from sheep 1, the flea will jump to sheep 4 in 10 steps is 0.2.
    \begin{itemize}
        \item Ambigious as there are 2 different interpretations for the sample space (i.e. space of probability is not clear):
        \begin{itemize}
            \item Set of sheeps
            \item Set of number of steps
        \end{itemize}
    \end{itemize}
\end{motivation}

\subsection{Probability Definitions}
\begin{definition}
    \begin{itemize}
        \item \textbf{Random Experiment:} An outcome (realization) for each run. 
        \item \textbf{Sample Space $\Omega$:} Set of all possible outcomes.
        \item \textbf{Events:} (measurable) subsets of $\Omega$.
        \item \textbf{Probability of Event A:} $P[A] \equiv P[\text{'outcome is in A'}]$.
    \end{itemize}
\end{definition}

\begin{example} \textbf{Roll Fair Die}
    \begin{itemize}
        \item $\Omega = \{1, 2, 3, 4, 5, 6\}$.
        \item $P[\text{'even number'}] = \frac{1}{2}$.
    \end{itemize}
\end{example}

\subsection{Axioms of Probability}
\begin{definition}
    \begin{enumerate}
        \item $P[A] \geq 0$ for all $A \in \Omega$.
        \item $P[\Omega] = 1$.
        \item If $A \cap B = \emptyset$, then $P[A \cup B] = P[A] + P[B]$ for all $A, B \in \Omega$.
        \customFigure[0.25]{Images/L1_0.png}{3rd Axiom}
    \end{enumerate}
\end{definition}

\subsection{Conditional Probability}
\begin{definition}
    \begin{equation}
        P[A|B] = \frac{P[A \cap B]}{P[B]}
    \end{equation}
    \begin{itemize}
        \item $|$: Given event (data/obs.).
    \end{itemize}
    \customFigure[0.25]{Images/L1_1.png}{Conditional Probability}
\end{definition}

\begin{notes}
    \begin{itemize}
        \item Changing sample space to $B$.
        \item Conditional probability satisfy the 3 axioms, can be viewed as probability measure on new sample space B.
    \end{itemize}
\end{notes}

\subsubsection{Consequences of Conditional Probability}
\begin{definition}
    \begin{equation}
        P[A \cap B] = P[A|B]P[B] = P[B|A]P[A]
    \end{equation}
\end{definition}

\subsubsection{Independence}
\begin{definition}
    $A$ and $B$ are independent iff 
    \begin{equation}
        P[A \cap B] = P[A]P[B] \iff P[A|B] = P[A] \iff P[B|A] = P[B]
    \end{equation}
\end{definition}

\subsubsection{Importance of Labelling}
\begin{example} \textbf{Toss 2 Fair Coins}
    \begin{enumerate}
        \item \textbf{Given:} Given that one of the coins is heads, what is the probability that the other coin is tails?
        \item \textbf{Wrong Solution:} $\frac{1}{2}$ since $\{HH, HT, TH, TT\}$, so $P[T|H] = \frac{1}{2}$, which assumes that the coins are distinguishable (i.e. coin \#1 is heads)
        \item \textbf{Correct Solution:} $\frac{2}{3}$ since $\{HH, HT, TH\}$ as we didn't specify which coin was heads, so $P[T|H] = \frac{2}{3}$, which assumes that the coins are indistinguishable.
    \end{enumerate}
    
\end{example}

\subsection{Total Probability}
\begin{definition}
    If $H_1,\ldots,H_n$ form a partition of $\Omega$, then
    \begin{equation}
        P[A] = \sum_{i=1}^{n} P[A|H_i]P[H_i]
    \end{equation}
    \customFigure[0.5]{Images/L1_2.png}{Total Probability}
\end{definition}

\subsection{Bayes' Rule}
\begin{definition}
    \begin{equation}
        P[H_k|A] = \frac{P[H_k \cap A]}{P[A]} = \frac{P[A|H_k]P[H_k]}{\sum_{i=1}^{n} P[A|H_i]P[H_i]}
    \end{equation}
\end{definition}

\subsubsection{Posteriori Probability, Priori Probability (Prior), Likelihood}
\begin{definition}
    \begin{itemize}
        \item \textbf{Posteriori:} $P[H_k|A]$.
        \item \textbf{Priori:} $P[H_k]$.
        \item \textbf{Likelihood:} $P[A|H_k]$.
    \end{itemize}
\end{definition}

\begin{example}
    Suppose a lie detector is 95\% accurate, i.e. $P[\text{'out=truth'}|\text{'in=truth'}] = 0.95$ and $P[\text{'out=lie'}|\text{'in=lie'}] = 0.95$. It says that Mr. Ernst is lying. What is the probability Mr. Ernst is actually lying.
    \begin{itemize}
        \item \textbf{Observation:} $A = \text{'out=lie'}$.
        \item \textbf{Hypothesis:} $H_0 = \text{'in=lie'}$ and $H_1 = \text{'in=truth'}$.
        \item \textbf{Solution:} $P[H_0|A] = \frac{P[A|H_0]P[H_0]}{P[A|H_0]P[H_0] + P[A|H_1]P[H_1]} = \frac{0.95 \times P[H_0]}{0.95 \times P[H_0] + 0.05 \times (1 - P[H_0])}$.
        \item \textbf{$H_0 = 0.01$:} i.e. 1\% of the population are liars, then $P[H_0|A] = \frac{0.95 \times 0.01}{0.95 \times 0.01 + 0.05 \times 0.99} = 0.16$.
    \end{itemize}
\end{example}

\begin{warning}
    Need to know priori probability.
\end{warning}

\subsubsection{Interpretation of Bayes' Rule}
\begin{definition}
    
\end{definition}

\subsection{Random Variables}
\begin{motivation} \textbf{Coin Toss}
    Mapping of each outcome to a real number, i.e. $w \in \Omega$ is the outcome of a coin toss, and $X(w) = 1$ if heads and $X(w) = 0$ if tails.
    \customFigure[0.5]{Images/L1_3.png}{Random Variables}
    \begin{itemize}
        \item Mapping is deterministic function. RV is not random or variable.
    \end{itemize}
\end{motivation}

\begin{definition}
    Mapping from $\Omega$ to $\mathbb{R}$.
\end{definition}

\subsubsection{Cumulative Distribution Function (CDF) of RV}
\begin{definition}
    \begin{equation}
        F_X(x) \equiv P[X \leq x]
    \end{equation}
\end{definition}

\subsubsection{Discrete RV PMF}
\begin{definition}
    \begin{equation}
        P_X(x_j) \equiv P[X=x_j] \quad j=1,2,3,\ldots 
    \end{equation}
\end{definition}

\begin{example} \textbf{Binonmial RV w/ $(n,p)$}
    \begin{equation}
        P_X(k) = \binom{n}{k} p^k (1-p)^{n-k}
    \end{equation}
    \customFigure[0.5]{Images/L1_4.png}{Binomial RV}
\end{example}

\subsubsection{Continuous RV PDF}
\begin{definition}
    \begin{equation}
        f_X(x) \equiv \frac{d}{dx} F_X(x)
    \end{equation}
    \begin{equation}
        P[x < X < x+dx] = f_X (x) dx
    \end{equation}
\end{definition}

\begin{example} \textbf{Gaussian RV w/ ($\mu,\sigma^2$)}
    \begin{equation}
        f_X (x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
    \end{equation}
    \customFigure[0.5]{Images/L1_5.png}{Gaussian RV}
    \begin{itemize}
        \item $p[X \in A] = \int_{A} f_X(x) dx$.
        \item \textbf{Note:} Discrete RV has pdf w/ $\delta$ functions.
    \end{itemize}
\end{example}

\subsubsection{Conditional PMF/PDF}
\begin{definition}
    \begin{equation}
        P_X (x|A) 
    \end{equation}
    \begin{equation}
        f_X (x|A)
    \end{equation}
\end{definition}

\begin{example} \textbf{Continuous}
    \begin{equation}
        f(x|X>a) = \begin{cases}
            \frac{f_X(x)}{P[X>a]} & \text{if } x > a \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation}
\end{example}

\begin{example} \textbf{Geometric RV}
    Geometric RV $X$ w/ success probability $p$ 
    \begin{equation}
        P_X(k) = (1-p)^{k-1}p
    \end{equation}

    \begin{itemize}
        \item \textbf{Memoryless Property:} $P_X[k|X > m] = \frac{p(1-p)^{k-1}}{(1-p)^m} = p(1-p)^{k-m-1}$. So it only cares about the additional trials. 
    \end{itemize}
    
\end{example}

\subsection{Expected Values}
\begin{definition}
    \begin{equation}
        E[X] = \int_{-\infty}^{\infty} x f_X(x) dx \overset{\text{If int. values}}{=} \sum_{k} k f_X(k)
    \end{equation}
    \begin{equation}
        E[h(X)] = \int_{-\infty}^{\infty} h(x) f_X(x) dx \overset{\text{If int. values}}{=} \sum_{k} h(k) f_X(k)
    \end{equation}
    \begin{equation}
        \text{Var}[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2
    \end{equation}
    \begin{equation}
        E[X|A] = \int_{-\infty}^{\infty} x f_{X}(x|A) dx
    \end{equation}
\end{definition}

\begin{example} \textbf{Lottery Ticket}
    \begin{enumerate}
        \item \textbf{Given:} Buying one lottery ticket per week
        \begin{itemize}
            \item Each ticket has $10^{-7} = p$ chance of winning the jackpot.
            \item $X = \text{'\# of weeks to win jackpot'}$.
        \end{itemize}
        \item \textbf{Problem:} What is the expected number of weeks to win the jackpot?
        \item \textbf{Solution:} $E[X] = \sum_{k=1}^{\infty} k(1 - p)^{k-1}p = \ldots = \frac{1}{p} = 10^7$ weeks.
        \item \textbf{Extension (Memoryless Property):} If I have already played for 999999 weeks, what is the expected number of weeks to win the jackpot? $E[X-999999|X>999999] = E[X] = 10^7$ weeks.
    \end{enumerate}
    
\end{example}


