\documentclass[5pt]{extarticle} % Note the extarticle document class
\usepackage[margin=0.2in]{geometry} % Set 0.3-inch margins
\usepackage{multicol}              % For multi-column support
\usepackage{lipsum}                % Dummy text generator (optional)
\usepackage{amssymb} % math symbols
\setlength{\parskip}{0ex}
\setlength\parindent{0pt} % no indent
%% optional packages -- documentation at ctan.org
\usepackage{graphicx}  % image handline
\usepackage{amsmath}   % enhanced equation environments
\usepackage{tikz}      % block diagrams
\usetikzlibrary{positioning}  % allow relative positioning of tikz elements
\usepackage{pgfplots}  % package for plots, based on tikz
\usepackage{hyperref}
\usepackage{paracol}             % Import paracol package

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0} % Define the dark green color

\begin{document}

\begin{paracol}{3}
    {\tiny
    \textcolor{red}{\textbf{Intro:}} \textcolor{blue}{\textbf{Random Experiment:}} An outcome for each run.  

    \textcolor{blue}{\textbf{Sample Space $\Omega$:}} Set of all possible outcomes.

    \textcolor{blue}{\textbf{Event:}} Subsets of $\Omega$.

    \textcolor{blue}{\textbf{Prob. of Event A:}} $P(A) = \frac{\text{Number of outcomes in A}}{\text{Number of outcomes in } \Omega}$

    \textcolor{blue}{\textbf{Axioms:}} $P(A) \geq 0 \; \forall A \in \Omega$, $P(\Omega) = 1$, \\
    If $A \cap B = \emptyset$, then $P(A \cup B) = P(A) + P(B) \; \forall A, B \in \Omega$

    \textcolor{blue}{\textbf{Cond. Prob.}} $P(A|B) = \frac{P(A \cap B)}{P(B)} \\
    *P(A \cap B) = P(A|B) P(B) = P(B|A) P(A)$

    \textcolor{darkgreen}{\textbf{Independence:}} $P(A|B) = P(A) \Leftrightarrow P(A \cap B) = P(A) P(B)$

    \textcolor{blue}{\textbf{Total Prob. Thm:}} If $H_1, H_2, \ldots, H_n$ form a partition of $\Omega$, then $P(A) = \sum_{i=1}^n P(A|H_i) P(H_i)$.

    \textcolor{blue}{\textbf{Bayes' Rule:}} $P(H_k|A) = \frac{P(H_k \cap A)}{P(A)} = \frac{P(A|H_k) P(H_k)}{\sum_{i=1}^n P(A|H_i) P(H_i)}$ \\
    *Posteriori: $P(H_k|A)$, Likelihood: $P(A|H_k)$, Prior: $P(H_k)$ 

    \textcolor{red}{\textbf{1 RV:}} \textcolor{blue}{\textbf{CDF:}} $F_X(x) = P[X \leq x]$ 

    \textcolor{blue}{\textbf{PMF:}} $P_X(x_j) = P[X = x_j] \; j=1,2,\ldots$

    \textcolor{blue}{\textbf{PDF:}} $f_X(x) = \frac{d}{dx} F_X(x)$ \\
    *$P[a \leq X \leq b] = \int_a^b f_X(x) \, dx$ IS THIS CORRECT?

    \textcolor{blue}{\textbf{Cond. PMF:}} $P_X(x|A) = P[X = x|A] = \frac{P[X = x, A]}{P[A]}$ IS THIS CORRECT?

    \textcolor{blue}{\textbf{Cond. PDF:}} $f_{X}(x|A) = \frac{f_{X,A}(x, a)}{f_A(a)}$ IS THIS CORRECT?

    \textcolor{blue}{\textbf{Exp.:}} $E[h(X)] \text{=} \int_{-\infty}^{\infty} h(x) f_X(x) dx \mid \sum_{k=-\infty}^{\infty} h(k) P_X(x_i \text{=} k)$ \\

    \textcolor{darkgreen}{\textbf{Variance:}} $\sigma_X^2 = \text{Var}[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2$

    \textcolor{darkgreen}{\textbf{Cond. Exp.:}} $E[X|A] = \int_{-\infty}^{\infty} x f_X(x|A) \, dx$ 

    \textcolor{red}{\textbf{2 RVs:}}  \textcolor{blue}{\textbf{Joint PMF:}} $P_{X,Y}(x, y) = P[X = x, Y = y]$

    \textcolor{blue}{\textbf{Joint PDF:}} $f_{X,Y}(x, y) = \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x, y)$ \\
    *$P[(X, Y) \in A] = \int \int_{(x, y) \in A} f_{X,Y}(x, y) \, dx \, dy$

    \textcolor{blue}{\textbf{Expectation:}} $E[g(X, Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f_{X,Y}(x, y) \, dx \, dy$

    \textcolor{darkgreen}{\textbf{Correlation:}} $E[XY]$

    \textcolor{darkgreen}{\textbf{Covariance:}} $\text{Cov}[X, Y] \text{=} E[(X - \mu_X)(Y - \mu_Y)] \text{=} E[XY] - E[X] E[Y]$

    \textcolor{darkgreen}{\textbf{Correlation Coeff.:}} $\rho_{X,Y} \text{=} E\left[ \left( \frac{X - \mu_X}{\sigma_X} \right) \left( \frac{Y - \mu_Y}{\sigma_Y} \right) \right] = \frac{\text{Cov}[X, Y]}{\sigma_X \sigma_Y}$

    \textcolor{blue}{\textbf{Marginal PMF:}} $P_X(x) = \sum_{j=1}^\infty P_{X,Y}(x, y_j)$

    \textcolor{blue}{\textbf{Marginal PDF:}} $f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, dy$

    \textcolor{blue}{\textbf{Conditional PMF:}} $P_{X|Y}(x|Y) = P[X = x|Y = y] = \frac{P_{X,Y}(x, y)}{P_Y(y)}$

    \textcolor{blue}{\textbf{Conditional PDF:}} $f_{X|Y}(x|y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}$

    \textcolor{blue}{\textbf{Bayes' Rule}} $f_{Y|X}(y|x) = \frac{f_{X,Y} (x,y)}{f_X (x)} = \frac{f_{X|Y} (x|y) f_Y (y)}{\int_{-\infty}^{\infty} f_{X|Y} (x|y') f_Y (y') \, dy'}$ \\
    *$P_{Y|X}(y|x) = \frac{P_{X,Y}(x, y)}{P_X(x)} = \frac{P_{X|Y}(x|y) P_Y(y)}{\sum_{j=1}^\infty P_{X|Y}(x|y_j) P_Y(y_j)}$

    \textcolor{blue}{\textbf{Independent:}} $f_{X|Y}(x|y) = f_X(x) \; \forall y \Leftrightarrow f_{X,Y}(x, y) = f_X(x) f_Y(y) $ 
    *If independent, then uncorrelated.

    \textcolor{blue}{\textbf{Uncorrelated:}} $\text{Cov}[X, Y] = 0 \Leftrightarrow \rho_{X,Y} = 0$

    \textcolor{blue}{\textbf{Orthogonal:}} $E[XY] = 0$

    \textcolor{blue}{\textbf{Conditional Expectation:}} $E[Y] = E[E[Y|X]]$ or $E[E[h(Y)|X]]$ \\
    *$E[E[Y|X]]$ w.r.t. $X \mid E[Y|X]$ w.r.t. $Y$. 

    \textcolor{red}{\textbf{Estimation:}} Estimate unknown parameter $\theta$ from $n$ i.i.d. measurements $X_1, X_2, \ldots, X_n$, $\hat{\Theta}(\underline{X}) = g(X_1, X_2, \ldots, X_n)$

    \textcolor{blue}{\textbf{Estimation Error:}} $\hat{\Theta}(\underline{X}) - \theta$. 

    \textcolor{blue}{\textbf{Unbiased:}} $\hat{\Theta}(\underline{X})$ is unbiased if $E[\hat{\Theta}(\underline{X})] = \theta$. \\
    *\textbf{Asymptotically unbiased:} $\lim_{n \to \infty} E[\hat{\Theta}(\underline{X})] = \theta$.

    \textcolor{blue}{\textbf{Consistent:}} $\hat{\Theta}(\underline{X})$ is consistent if $\hat{\Theta}(\underline{X}) \rightarrow \theta$ as $n \to \infty$ or $\forall \epsilon >0, \lim_{n \to \infty} P[|\hat{\Theta}(\underline{X}) - \theta| < \epsilon] \rightarrow 1$.

    \textcolor{darkgreen}{\textbf{Sample Mean:}} $M_n = \frac{1}{n} S_n = \frac{1}{n} \sum_{i=1}^n X_i$. \\
    *Given a sequence of i.i.d. RVs, $X_1, X_2, \ldots, X_n$, $M_n$ is unbiased and consistent.

    \textcolor{blue}{\textbf{Chebychev's Inequality:}} $P[|X - E[X]| \geq \epsilon] \leq \frac{\text{Var}[X]}{\epsilon^2}$

    \textcolor{blue}{\textbf{Weak Law of Large \#s:}} $\lim_{n \to \infty} P[|M_n - \mu| < \epsilon] = 1 \; \forall \epsilon > 0$.

    \textcolor{red}{\textbf{Maximum Likelihood Estimation:}} Choose parameter $\theta$ that is most likely to generate the obs. $x_1, x_2, \ldots, x_n$. \\
    *Disc: $\hat{\Theta} = \arg \max_\theta P_{\underline{X}} (\underline{x} | \theta) \overset{\text{log}}{\rightarrow} \hat{\theta} = \arg \max_\theta \sum_{i=1}^n \log P_X(x_i | \theta)$ \\
    *Cont: $\hat{\Theta} = \arg \max_\theta f_{\underline{X}} (\underline{x} | \theta) \overset{\text{log}}{\rightarrow} \hat{\theta} = \arg \max_\theta \sum_{i=1}^n \log f_X(x_i | \theta)$

    \textcolor{blue}{\textbf{Maximum A Posteriori (MAP) Estimation:}} \\
    *Disc: $\hat{\theta} = \arg \max_\theta P_{\Theta | \underline{X}} (\theta | \underline{x}) = \arg \max_\theta P_{\underline{X} | \Theta} (\underline{x} | \theta) P_\Theta (\theta)$\\
    *Cont: $\hat{\theta} = \arg \max_\theta f_{\Theta | \underline{X}} (\theta | \underline{x}) = \arg \max_\theta f_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta)$

    \textcolor{darkgreen}{\textbf{Bayes' Rule:}} $P_{\Theta | \underline{X}} (\theta | \underline{x}) = \begin{cases}
        \frac{P_{\underline{X} | \Theta} (\underline{x} | \theta) P_\Theta (\theta)}{P_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ disc.} \\
        \frac{f_{\underline{X} | \Theta} (\underline{x} | \theta) P_\Theta (\theta)}{f_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ cont.}
    \end{cases}$ \\
    $f_{\Theta | \underline{X}} (\theta | \underline{x}) = \begin{cases}
        \frac{P_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta)}{P_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ disc.} \\
        \frac{f_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta)}{f_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ cont.} 
    \end{cases}$ \\
    *Independent of $\theta$: $f_{\underline{X}} (\underline{x}) = \int_{-\infty}^{\infty} f_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta) \, d\theta$ \\

    \textcolor{blue}{\textbf{Beta Prior}} $\Theta$ is a Beta R.V. w/ $\alpha,\beta>0$\\
    $f_\Theta (\theta) = \begin{cases}
        \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1} & \text{if } 0 < \theta < 1 \\
        0 & \text{otherwise}
    \end{cases}$ \\
    *$\Gamma(x) = \int_{0}^{\infty} t^{x-1} e^{-t} \, dt$

    \textcolor{darkgreen}{\textbf{Properties:}} 1. $\Gamma(x+1) = x \Gamma(x)$. For $m \in \mathbb{Z}^+$, $\Gamma(m+1) = m!$. \\
    2. $\beta(\alpha,\beta) = \frac{(\alpha + \beta -1)!}{(\alpha - 1)! (\beta - 1)!} = \beta \binom{\alpha + \beta - 1}{\alpha - 1}$ \\
    3. Expected Value: $E[\Theta] = \frac{\alpha}{\alpha + \beta}$ \\
    4. Mode (max of PDF): $\frac{\alpha - 1}{\alpha + \beta - 2}$ 

    \textcolor{blue}{\textbf{Least Mean Squares (LMS) Estimation:}} Assume prior $P_\Theta (\theta)$ or $f_\Theta (\theta)$ w/ obs. $\underline{X} = \underline{x}$. \\
    *$\hat{\theta} = g(\underline{x}) = \mathbb{E} [\Theta | \underline{X} = \underline{x}] \; \mid \; \hat{\Theta} = g(\underline{X}) = \mathbb{E} [\Theta | \underline{X}]$ \\

    }
\end{paracol}

\end{document}