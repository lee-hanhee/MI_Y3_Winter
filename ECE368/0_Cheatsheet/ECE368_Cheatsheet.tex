\documentclass[5pt]{extarticle} % Note the extarticle document class
\usepackage[margin=0.2in]{geometry} % Set 0.3-inch margins
\usepackage{multicol}              % For multi-column support
\usepackage{lipsum}                % Dummy text generator (optional)
\usepackage{amssymb} % math symbols
\setlength{\parskip}{0ex}
\setlength\parindent{0pt} % no indent
%% optional packages -- documentation at ctan.org
\usepackage{graphicx}  % image handline
\usepackage{amsmath}   % enhanced equation environments
\usepackage{tikz}      % block diagrams
\usetikzlibrary{positioning}  % allow relative positioning of tikz elements
\usepackage{pgfplots}  % package for plots, based on tikz
\usepackage{hyperref}
\usepackage{paracol}             % Import paracol package
\usepackage{float} % for H in figure

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0} % Define the dark green color

\newcommand{\customFigure}[3][]{
    \vspace{-2em}
    \begin{figure}[H]
        \centering
        \includegraphics[width=#1\textwidth]{#2}
    \end{figure}
    \vspace{-2em}
}

\begin{document}

\begin{paracol}{3}
    {\tiny
    \textcolor{red}{\textbf{Intro:}} \textcolor{blue}{\textbf{Random Experiment:}} An outcome for each run.  

    \textcolor{blue}{\textbf{Sample Space $\Omega$:}} Set of all possible outcomes.

    \textcolor{blue}{\textbf{Event:}} Subsets of $\Omega$.

    \textcolor{blue}{\textbf{Prob. of Event A:}} $P(A) = \frac{\text{Number of outcomes in A}}{\text{Number of outcomes in } \Omega}$

    \textcolor{blue}{\textbf{Axioms:}} $P(A) \geq 0 \; \forall A \in \Omega$, $P(\Omega) = 1$, \\
    If $A \cap B = \emptyset$, then $P(A \cup B) = P(A) + P(B) \; \forall A, B \in \Omega$

    \textcolor{blue}{\textbf{Cond. Prob.}} $P(A|B) = \frac{P(A \cap B)}{P(B)} \\
    *P(A \cap B) = P(A|B) P(B) = P(B|A) P(A)$

    \textcolor{darkgreen}{\textbf{Independence:}} $P(A|B) = P(A) \Leftrightarrow P(A \cap B) = P(A) P(B)$

    \textcolor{blue}{\textbf{Total Prob. Thm:}} If $H_1, H_2, \ldots, H_n$ form a partition of $\Omega$, then $P(A) = \sum_{i=1}^n P(A|H_i) P(H_i)$.

    \textcolor{blue}{\textbf{Bayes' Rule:}} $P(H_k|A) \text{=} \frac{P(H_k \cap A)}{P(A)} \text{=} \frac{P(A|H_k) P(H_k)}{\sum_{i=1}^n P(A|H_i) P(H_i)}$ \\
    *Posteriori: $P(H_k|A)$, Likelihood: $P(A|H_k)$, Prior: $P(H_k)$ 

    \textcolor{red}{\textbf{1 RV:}} \textcolor{blue}{\textbf{CDF:}} $F_X(x) = P[X \leq x]$ 

    \textcolor{blue}{\textbf{PMF:}} $P_X(x_j) = P[X = x_j] \; j=1,2,\ldots$

    \textcolor{blue}{\textbf{PDF:}} $f_X(x) = \frac{d}{dx} F_X(x)$ \\
    *$P[a \leq X \leq b] = \int_a^b f_X(x) \, dx$ IS THIS CORRECT?

    \textcolor{blue}{\textbf{Cond. PMF:}} $P_X(x|A) = P[X = x|A] = \frac{P[X = x, A]}{P[A]}$ IS THIS CORRECT?

    \textcolor{blue}{\textbf{Cond. PDF:}} $f_{X}(x|A) = \frac{f_{X,A}(x, a)}{f_A(a)}$ IS THIS CORRECT?

    \textcolor{blue}{\textbf{Exp.:}} $E[h(X)] \text{=} \int_{-\infty}^{\infty} h(x) f_X(x) dx \mid \sum_{k=-\infty}^{\infty} h(k) P_X(x_i \text{=} k)$ \\

    \textcolor{darkgreen}{\textbf{Variance:}} $\sigma_X^2 = \text{Var}[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2$

    \textcolor{darkgreen}{\textbf{Cond. Exp.:}} $E[X|A] = \int_{-\infty}^{\infty} x f_X(x|A) \, dx$ 

    \textcolor{red}{\textbf{2 RVs:}}  \textcolor{blue}{\textbf{Joint PMF:}} $P_{X,Y}(x, y) = P[X = x, Y = y]$

    \textcolor{blue}{\textbf{Joint PDF:}} $f_{X,Y}(x, y) = \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x, y)$ \\
    *$P[(X, Y) \in A] = \int \int_{(x, y) \in A} f_{X,Y}(x, y) \, dx \, dy$

    \textcolor{blue}{\textbf{Exp.:}} $E[g(X, Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f_{X,Y}(x, y) \, dx \, dy$

    \textcolor{darkgreen}{\textbf{Correlation (Corr.):}} $E[XY]$

    \textcolor{darkgreen}{\textbf{Covar.:}} $\text{Cov}[X, Y] \text{=} E[(X - \mu_X)(Y - \mu_Y)] \text{=} E[XY] - E[X] E[Y]$

    \textcolor{darkgreen}{\textbf{Corr. Coeff.:}} $\rho_{X,Y} \text{=} E\left[ \left( \frac{X - \mu_X}{\sigma_X} \right) \left( \frac{Y - \mu_Y}{\sigma_Y} \right) \right] = \frac{\text{Cov}[X, Y]}{\sigma_X \sigma_Y}$

    \textcolor{blue}{\textbf{Marginal PMF:}} $P_X(x) = \sum_{j=1}^\infty P_{X,Y}(x, y_j)$

    \textcolor{blue}{\textbf{Marginal PDF:}} $f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, dy$

    \textcolor{blue}{\textbf{Cond. PMF:}} $P_{X|Y}(x|Y) = P[X = x|Y = y] = \frac{P_{X,Y}(x, y)}{P_Y(y)}$

    \textcolor{blue}{\textbf{Cond. PDF:}} $f_{X|Y}(x|y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}$

    \textcolor{blue}{\textbf{Bayes' Rule}} \\ 
    $f_{Y|X}(y|x) \text{=} \frac{f_{X,Y} (x,y)}{f_X (x)} \text{=} \frac{f_{X|Y} (x|y) f_Y (y)}{\int_{-\infty}^{\infty} f_{X|Y} (x|y') f_Y (y') \, dy'}$ \\
    *$P_{Y|X}(y|x) = \frac{P_{X,Y}(x, y)}{P_X(x)} = \frac{P_{X|Y}(x|y) P_Y(y)}{\sum_{j=1}^\infty P_{X|Y}(x|y_j) P_Y(y_j)}$

    \textcolor{blue}{\textbf{Ind.:}} $f_{X|Y}(x|y) = f_X(x) \; \forall y \Leftrightarrow f_{X,Y}(x, y) = f_X(x) f_Y(y) $ 
    *If independent, then uncorrelated.

    \textcolor{blue}{\textbf{Uncorrelated:}} $\text{Cov}[X, Y] = 0 \Leftrightarrow \rho_{X,Y} = 0$

    \textcolor{blue}{\textbf{Orthogonal:}} $E[XY] = 0$

    \textcolor{blue}{\textbf{Cond. Exp.:}} $E[Y] = E[E[Y|X]]$ or $E[E[h(Y)|X]]$ \\
    *$E[E[Y|X]]$ w.r.t. $X \mid E[Y|X]$ w.r.t. $Y$. 

    \textcolor{red}{\textbf{Estimation:}} Estimate unknown parameter $\theta$ from $n$ i.i.d. measurements $X_1, X_2, \ldots, X_n$, $\hat{\Theta}(\underline{X}) = g(X_1, X_2, \ldots, X_n)$

    \textcolor{blue}{\textbf{Estimation Error:}} $\hat{\Theta}(\underline{X}) - \theta$. 

    \textcolor{blue}{\textbf{Unbiased:}} $\hat{\Theta}(\underline{X})$ is unbiased if $E[\hat{\Theta}(\underline{X})] = \theta$. \\
    *\textbf{Asymptotically unbiased:} $\lim_{n \to \infty} E[\hat{\Theta}(\underline{X})] = \theta$.

    \textcolor{blue}{\textbf{Consistent:}} $\hat{\Theta}(\underline{X})$ is consistent if $\hat{\Theta}(\underline{X}) \rightarrow \theta$ as $n \to \infty$ or $\forall \epsilon >0, \lim_{n \to \infty} P[|\hat{\Theta}(\underline{X}) - \theta| < \epsilon] \rightarrow 1$.

    \textcolor{darkgreen}{\textbf{Sample Mean:}} $M_n = \frac{1}{n} S_n = \frac{1}{n} \sum_{i=1}^n X_i$. \\
    *Given a sequence of i.i.d. RVs, $X_1, X_2, \ldots, X_n$, $M_n$ is unbiased and consistent.

    \textcolor{blue}{\textbf{Chebychev's Inequality:}} $P[|X - E[X]| \geq \epsilon] \leq \frac{\text{Var}[X]}{\epsilon^2}$

    \textcolor{blue}{\textbf{Weak Law of Large \#s:}} $\lim_{n \to \infty} P[|M_n - \mu| < \epsilon] = 1 \; \forall \epsilon > 0$.

    \textcolor{red}{\textbf{ML Estimation:}} Choose parameter $\theta$ that is most likely to generate the obs. $x_1, x_2, \ldots, x_n$. \\
    *Disc: $\hat{\Theta} = \arg \max_\theta P_{\underline{X}} (\underline{x} | \theta) \overset{\text{log}}{\rightarrow} \hat{\theta} = \arg \max_\theta \sum_{i=1}^n \log P_X(x_i | \theta)$ \\
    *Cont: $\hat{\Theta} = \arg \max_\theta f_{\underline{X}} (\underline{x} | \theta) \overset{\text{log}}{\rightarrow} \hat{\theta} = \arg \max_\theta \sum_{i=1}^n \log f_X(x_i | \theta)$

    \textcolor{blue}{\textbf{Maximum A Posteriori (MAP) Estimation:}} \\
    *Disc: $\hat{\theta} = \arg \max_\theta P_{\Theta | \underline{X}} (\theta | \underline{x}) = \arg \max_\theta P_{\underline{X} | \Theta} (\underline{x} | \theta) P_\Theta (\theta)$\\
    *Cont: $\hat{\theta} = \arg \max_\theta f_{\Theta | \underline{X}} (\theta | \underline{x}) = \arg \max_\theta f_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta)$ \\
    *$f_{\Theta | \underline{X}} (\theta | \underline{x})$: Posteriori, $f_{\underline{X} | \Theta} (\underline{x} | \theta)$: Likelihood, $f_\Theta (\theta)$: Prior

    \textcolor{darkgreen}{\textbf{Bayes' Rule:}} $P_{\Theta | \underline{X}} (\theta | \underline{x}) = \begin{cases}
        \frac{P_{\underline{X} | \Theta} (\underline{x} | \theta) P_\Theta (\theta)}{P_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ disc.} \\
        \frac{f_{\underline{X} | \Theta} (\underline{x} | \theta) P_\Theta (\theta)}{f_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ cont.}
    \end{cases}$ \\
    $f_{\Theta | \underline{X}} (\theta | \underline{x}) = \begin{cases}
        \frac{P_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta)}{P_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ disc.} \\
        \frac{f_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta)}{f_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ cont.} 
    \end{cases}$ \\
    *Independent of $\theta$: $f_{\underline{X}} (\underline{x}) = \int_{-\infty}^{\infty} f_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta) \, d\theta$ \\

    \textcolor{blue}{\textbf{Beta Prior}} $\Theta$ is a Beta R.V. w/ $\alpha,\beta>0$\\
    $f_\Theta (\theta) = \begin{cases}
        \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1} & \text{if } 0 < \theta < 1 \\
        0 & \text{otherwise}
    \end{cases}$ \\
    *$\Gamma(x) = \int_{0}^{\infty} t^{x-1} e^{-t} \, dt$

    \textcolor{darkgreen}{\textbf{Prop.:}} 1. $\Gamma(x+1) = x \Gamma(x)$. For $m \in \mathbb{Z}^+$, $\Gamma(m+1) = m!$. \\
    2. $\beta(\alpha,\beta) = \frac{(\alpha + \beta -1)!}{(\alpha - 1)! (\beta - 1)!} = \beta \binom{\alpha + \beta - 1}{\alpha - 1}$ \\
    3. Expected Value: $E[\Theta] = \frac{\alpha}{\alpha + \beta}$ for $\alpha, \beta > 0$ \\
    4. Mode (max of PDF): $\frac{\alpha - 1}{\alpha + \beta - 2}$ for $\alpha, \beta > 1$ \\

    \textcolor{orange}{\textbf{Drawing Beta Dist.}} 1. Label $x$-axis from 0 to 1. 2. Identify mode. \\ 
    3. Determine shape based on $\alpha$ and $\beta$: $\alpha = \beta = 1$ (uniform), $\alpha = \beta > 1$ (bell-shaped, peak at 0.5), $\alpha = \beta < 1$ (U-shaped w/ high density near 0 and 1), $\alpha > \beta$ (left-skewed), $\alpha < \beta$ (right-skewed).

    \textcolor{blue}{\textbf{Least Mean Squares (LMS) Estimation:}} Assume prior $P_\Theta (\theta)$ or $f_\Theta (\theta)$ w/ obs. $\underline{X} = \underline{x}$. \\
    *$\hat{\theta} = g(\underline{x}) = \mathbb{E} [\Theta | \underline{X} = \underline{x}] \; \mid \; \hat{\Theta} = g(\underline{X}) = \mathbb{E} [\Theta | \underline{X}]$ \\

    \textcolor{blue}{\textbf{Uniform PDF}} $f_X(x) = \begin{cases}
        \frac{1}{b - a} & \text{if } a \leq x \leq b \\
        0 & \text{otherwise}
    \end{cases}$ \\
    *$E[X] = \frac{a + b}{2}$, $\text{Var}[X] = \frac{(b - a)^2}{12}$

    \textcolor{blue}{\textbf{Conditional Exp.}} $E[X|Y] = \int_{-\infty}^{\infty} x f_{X|Y} (x|y) \, dx$

    \textcolor{red}{\textbf{Binary Hyp. Testing:}} $H_0$: Null Hyp., $H_1$: Alt. Hyp. \\

    \textcolor{blue}{\textbf{TI Err. (False Rejection):}} Reject $H_0$ when $H_0$ is true. \\
    *$\alpha(R) = P[\underline{X} \in R \mid H_0]$ 

    \textcolor{blue}{\textbf{TII Err. (False Accept.):}} Accept $H_0$ when $H_1$ is true. \\
    *$\beta(R) = P[\underline{X} \in R^c \mid H_1]$ \\

    \customFigure[0.11]{../Images/TB_1.png}

    \textcolor{blue}{\textbf{Likelihood Ratio Test:}} For each value of $\underline{x}$, \\
    *$L(\underline{x}) = \frac{P_{\underline{X}} (\underline{x}|H_1)}{P_{\underline{X}} (\underline{x}|H_0)} \overset{H_1}{\underset{H_0}{\gtrless}} 1 \text{ or } \xi$ \\
    *\textbf{MLT:} $1$, \textbf{LRT:} $\xi$

    \textcolor{blue}{\textbf{Neyman-Pearson Lemma:}} Given a false rejection prob. ($\alpha$), the LRT offers the smallest possible false accept. prob. ($\beta$), and vice versa. \\
    *LRT produces ($\alpha,\beta$) pairs that lie on the efficient frontier.

    \customFigure[0.1]{../Images/L10_0.png}{}
    
    % Given $L(X), \xi$ so that \\ 
    % $P[L(X) > \xi \mid H_0] = \alpha$ and $P[L(X) \leq \xi \mid H_1] = \beta$,
    % then for any other test (rejection region) w/ $P[X \in R \mid H_0] \leq \alpha$, then $P[X \notin R \mid H_1] \geq \beta$.

    % \textcolor{darkgreen}{\textbf{Sig. Testing:}} Given $X_1, \ldots, X_n$, find a rejection reg. so a level of T1 err. is achieved: $P[\text{Reject } H_0 \mid H_0] = \alpha$. \\
    % *$\alpha$: Significance level, $1 - \alpha$: Confidence level.

    \textcolor{blue}{\textbf{Bayesian Hyp. Testing:}} \textcolor{darkgreen}{\textbf{MAP Rule:}} \\ 
    $L(\underline{x}) = \frac{p_{\underline{X}} (\underline{x} | H_1)}{p_{\underline{X}} (\underline{x} | H_0)} \overset{H_1}{\underset{H_0}{\gtrless}} \frac{P[H_0]}{P[H_1]}$ 
    
    % Selects hyp. w/ higher a posteriori prob, reject $H_0$ if: \\
    % $p(H_1 \mid \underline{x}) \overset{H_1}{\underset{H_0}{\gtrless}} p(H_0 \mid \underline{x}) \; \mid \; f(H_1 \mid \underline{x}) \overset{H_1}{\underset{H_0}{\gtrless}} f(H_0 \mid \underline{x})$ \\
    % $p_{\underline{X}} (\underline{x} | H_1) P[H_1] \overset{H_1}{\underset{H_0}{\gtrless}} p_{\underline{X}} (\underline{x} | H_0) P[H_0]$ \\
    
    % $p(\underline{x} \mid H_1)\pi_j \overset{H_1}{\underset{H_0}{\gtrless}} p(\underline{x} \mid H_0)\pi_0 \; \mid \; f(\underline{x} \mid H_1)\pi_j \overset{H_1}{\underset{H_0}{\gtrless}} f(\underline{x} \mid H_0)\,\pi_0$ \\
    % *$p(H_j \mid \underline{x}) \text{=} \frac{p_{\underline{X}} (\underline{x} \mid H_j) P [H_j]}{p_{\underline{X}} (\underline{x} \mid H_0) P[H_0] + p_{\underline{X}} (\underline{x} \mid H_1) P[H_1]}$: A posteriori 

    \textcolor{darkgreen}{\textbf{Min. Cost Bayes' Dec. Rule:}} $C_{ij}$ is cost of choosing $H_j$ when $H_i$ is true. Given obs. $\underline{X} = \underline{x}$, the exp. cost of choosing \\ 
    $H_j$ is $A_j (\underline{x}) = \sum_{i=0}^1 C_{ij} \, P[H_i | \underline{X} = \underline{x}]$. \\
    
    % $\text{Min. Cost Detection} = \sum_{i=0}^1 \sum_{j=0}^1 C_{ij} \, P[\text{decide } j \mid H_i] \, \pi_i$ \\
    % *$j=0$: Accept $H_0$, $j=1$: Reject $H_0$

    \textcolor{darkgreen}{\textbf{Min. Cost Dec. Rule:}} 
    $L(\underline{x}) = \frac{P_{\underline{X}} (\underline{x} \mid H_1)}{P_{\underline{X}} (\underline{x} \mid H_0)}\overset{H_1}{\underset{H_0}{\gtrless}} \frac{(C_{01} - C_{00}) P[H_0]}{(C_{10} - C_{11})P[H_1]}$. \\
    *$C_{01}$: False accept. cost, $C_{10}$: False reject. cost.

    \textcolor{blue}{\textbf{Naive Bayes Assumption:}} Assume $X_1 \ldots, X_n$ (features) are ind., then $p_{\underline{X} \mid \Theta} (\underline{x} \mid \theta) \Pi_{i=1}^n p_X (x_i \mid \theta)$.

    \textbf{Notation:} $P_{\underline{X} | \Theta} (\underline{x} | \theta)$, only put RVs in subscript, not values. $P_{\underline{X}} (\underline{x} | H_i)$, didn't put $H$ in subscript b/c it's not a RV.

    \textbf{Binomial} \# of successes in $n$ trials, each w/ prob. $p$ \\
    $ b(x \mid n, p) = \binom{n}{x} p^x (1 - p)^{n - x}, x = 0, 1, 2, \dots $ \\
    *$ E[X] = \mu = np \; \mid \; Var(X) = \sigma^2 = np(1 - p) $

    \textbf{Multinomial} \# of $x_i$ successes in $n$ trials, each w/ prob. $p_i$ \\
    $ f(x_i \mid p_i \forall i, n) = \frac{n!}{x_1! \dots x_m!} p_1^{x_1} \dots p_m^{x_m} $  \\
    *$ \sum_{i} x_i = n $, and $ \sum_{i=1}^{m} p_i = 1 $ \\
    *$ E[X_i] = \mu_i = np_i \; \mid \; Var(X_i) = \sigma^2_i = np_i(1 - p_i) $

    \textbf{Hypergeometric} \# of successes in $n$ draws from $N$ items, $k$ of which are successes \\
    $ h(x \mid N, n, k) = \frac{\binom{k}{x} \binom{N-k}{n-x}}{\binom{N}{n}}$ \\
    *$\max\{0, n - (N - k)\} \leq x \leq \min\{n, k\} $ \\
    *$E[X] = \mu = \frac{nk}{N} \; \mid \; Var(X) = \sigma^2 = \frac{N-n}{N-1} \cdot n \cdot \frac{k}{N} \cdot \left(1 - \frac{k}{N} \right) $

    \textbf{Negative Binomial} \# of trials until $k$ successes, each w/ prob. $p$ \\
    $ b^*(x \mid k, p) = \binom{x-1}{k-1} p^k (1 - p)^{x - k}$ \\
    *$x \geq k, x = k, k+1, \dots $ \\
    *$ E[X] = \mu = \frac{k}{p} \; \mid \; Var(X) = \sigma^2 = \frac{k(1 - p)}{p^2} $

    \textbf{Geometric} \# of trials until 1st success, each w/ prob. $p$ \\
    $ g(x \mid p) = p(1 - p)^{x - 1}$ \\
    *$x \geq 1, x = 1, 2, 3, \dots $ \\
    *$ E[X] = \mu = \frac{1}{p} \; \mid \; Var(X) = \sigma^2 = \frac{1 - p}{p^2} $

    \textbf{Poisson} \# of events in a fixed interval w/ rate $\lambda$ \\
    $ p(x \mid \lambda t) = \frac{e^{-\lambda t} (\lambda t)^x}{x!}$ \\
    *$x \geq 0, x = 0, 1, 2, \dots $ \\
    *$ E[X] = \mu = \lambda t \; \mid \; Var(X) = \sigma^2 = \lambda t $

    \textcolor{orange}{\textbf{Gaussian to Q Fcn:}} 1. Find $Q(x) = \frac{1}{\sqrt{2\pi}} \int_x^\infty e^{-t^2/2} \, dt$. \\
    2. Use table to find $Q(x)$ for $x \geq 0$. 

    \textcolor{red}{\textbf{Random Vector:}} $\underline{X} = [X_1,\ldots,X_n]^T$ 

    \textcolor{blue}{\textbf{Mean Vector:}} $\underline{m}_{\underline{X}} = E[\underline{X}] = [\mu_1, \ldots, \mu_n]^T$ \\

    \textcolor{blue}{\textbf{Corr. Mat.:}} $R_{\underline{X}} =
    \begin{bmatrix}
    E[X_1^2] & E[X_1 X_2] & \cdots & E[X_1 X_n] \\
    E[X_2 X_1] & E[X_2^2] & \cdots & E[X_2 X_n] \\
    \vdots & \vdots & \ddots & \vdots \\
    E[X_n X_1] & E[X_n X_2] & \cdots & E[X_n^2]
    \end{bmatrix}$ \\
    *$R$ is real, symmetric, and PSD ($\forall \underline{a}, \underline{a}^T R \underline{a} \geq 0$).

    \textcolor{blue}{\textbf{Covar. Mat.:}} $K_{\underline{X}} =
    \begin{bmatrix}
    \operatorname{Var}[X_1] & \cdots & \operatorname{Cov}[X_1, X_n] \\
    \operatorname{Cov}[X_2, X_1] & \cdots & \operatorname{Cov}[X_2, X_n] \\
    \vdots & \ddots & \vdots \\
    \operatorname{Cov}[X_n, X_1] & \cdots & \operatorname{Var}[X_n]
    \end{bmatrix}$ \\
    *$K_{\underline{X}} = R_{\underline{X}} - \underline{m}_{\underline{X}} = R_{\underline{X}} - \underline{m} \underline{m}^T$ \\
    *Diagonal $K_{\underline{X}} \iff X_1, \ldots, X_n$ are (mutually) uncorrelated.

    \textcolor{red}{\textbf{Lin. Trans.}} $\underline{Y} = A \underline{X}$ (A rotates and stretches $\underline{X}$)

    \textcolor{darkgreen}{\textbf{Mean:}} $E[\underline{Y}] = A \underline{m}_{\underline{X}}$

    \textcolor{darkgreen}{\textbf{Covar. Mat.:}} $K_{\underline{Y}} = A K_{\underline{X}} A^T$

    \textcolor{darkgreen}{\textbf{Diag. Covar. Mat.:}} For any $\underline{X}$, if $\underline{Y} = P^T \underline{X}$, then \\
    $K_{\underline{Y}} = P^T K_{\underline{X}} P = \Lambda$ (i.e. $\underline{Y}$ is uncorrelated) \\
    *$K_{\underline{X}} = P \Lambda P^T \; \mid \; P = [\underline{e}_1, \ldots, \underline{e}_n]$ of $K_{\underline{X}}$ \\

    \textcolor{orange}{\textbf{Find $K_{\underline{Y}}$}} 1. Find eigenvalues, norm. eigenvectors of $K_{\underline{X}}$. \\
    2. Set $\underline{Y} = P^T \underline{X}$, $K_{\underline{Y}} = \Lambda$.

    \textcolor{darkgreen}{\textbf{PDF of L.T.}} If $\underline{Y} = A \underline{X}$ w/ $A$ not singular, then \\
    $f_{\underline{Y}} (\underline{y}) = \frac{f_{\underline{X}} (\underline{x})}{|\det A|} \Big|_{\underline{x} = A^{-1} \underline{y}}$

    \textcolor{orange}{\textbf{Find $f_{\underline{Y}} (\underline{y})$}} 1. Given $f_{\underline{X}} (\underline{x})$, define transformation $A$ \\
    2. Determine $|\det A|$, $A^{-1}$, then $f_{\underline{Y}} (\underline{y})$.

    \textcolor{red}{\textbf{Gaussian RVs:}} \textcolor{blue}{\textbf{Analytic Tractability:}} PDF of jointly Gaussian $X_1, \ldots, X_n$ is Guassian vector. \\
    *$f_{\underline{X}} (\underline{x}) = \frac{1}{(2\pi)^{n/2} |\det \Sigma|^{1/2}} e^{-\frac{1}{2} (\underline{x} - \underline{\mu})^T \Sigma^{-1} (\underline{x} - \underline{\mu})}$ \\ 
    *$\underline{\mu} = \underline{m}_{\underline{X}}$, $\Sigma = K_{\underline{X}}$ (if $\Sigma$ is not singular)

    \textcolor{blue}{\textbf{Properties of Guassian Vector:}} $\underline{X} \sim \mathcal{N} (\underline{\mu}, \Sigma)$ \\
    1. PDF is completely determined by $\underline{\mu}$, $\Sigma$. \\
    2. $\underline{X}$ uncorrelated $\implies$ $\underline{X}$ independent. \\
    3. Any L.T. $\underline{Y} = A \underline{X} + \underline{b}$ is Gaussian vector w/ $\underline{\mu}_{\underline{Y}} = A \underline{\mu}_{\underline{X}}$, $\Sigma_{\underline{Y}} = A \Sigma_{\underline{X}} A^T$. \\
    4. Any subset of $\{X_i\}$ are jointly Gaussian. \\
    5. Any cond. PDF of a subset of $\{X_i\}$ given the other elements is Gaussian. \\

    \textcolor{blue}{\textbf{Diag. of Guassian Covar.}} Eigen decomp. of $\Sigma_{\underline{X}}$: $\{\lambda_i\}, \{e_i\}$ \\
    $A = [\underline{e}_1, \ldots, \underline{e}_n]^T$, then $\underline{Y} = A \underline{X}$ has $\Sigma_{\underline{Y}} = \Lambda$. \\
    *$\underline{Y}$: Vector of indep. Gaussian RVs.

    \textcolor{orange}{\textbf{How to go from $Y$ to $X$?}} 1. Given, $\underline{X} \sim \mathcal{N}(\underline{\mu}, \Sigma)$, then find $\Sigma = P \Lambda P^T$. \\
    2. $\underline{V} \sim \mathcal{N} (\underline{0}, I)$ 3. $\underline{W} = \sqrt{\Lambda} \underline{V}$ 4. $\underline{Y} = P \underline{W}$ 4. $\underline{X} = \underline{Y} + \underline{\mu}$

    \textcolor{blue}{\textbf{Guassian Discriminant Analysis:}} Obs: $\underline{X} = \underline{x} = (x_1,\ldots,x_D)$ \\
    Hyp: $\underline{x}$ is gen. by $\mathcal{N} (\underline{\mu}_c, \Sigma_c), c \in C$ \\
    Dec: Which "Guassian bump" generated $\underline{x}$? \\
    Prior: $P[C = c] = \pi_c$  (Gaussian Mixture Model) \\

    \textcolor{darkgreen}{\textbf{MAP Rule:}} $\hat{c} = \arg \max_c P_C[c | \underline{X} = \underline{x}] = \arg \max_c f_{\underline{X} \mid C} (\underline{x} \mid c) \pi_c$ \\

    \textcolor{darkgreen}{\textbf{LGD:}} $\Sigma_c = \Sigma \; \forall c$, find $c$ w/ best $\underline{\mu}_c$ \\
    $\hat{c} = \arg \max_c \underline{\beta}_c^T \underline{x} + \gamma_c$ \\
    *$\underline{\beta}_c^T = \underline{\mu}_c^T \Sigma^{-1} \underline{x} \; \mid \; \gamma_c = \log \pi_c - \frac{1}{2} \underline{\mu}_c^T \Sigma^{-1} \underline{\mu}_c$ \\
    *Bin. hyp. dec. boundary: $\underline{\beta}_0^T \underline{x} + \gamma_0 = \underline{\beta}_1^T \underline{x} + \gamma_1$ (lin. in space of $\underline{x}$) 

    \textcolor{darkgreen}{\textbf{QGD:}} $\Sigma_c$ are diff., find $c$ w/ best $\underline{\mu}_c$, $\Sigma_c$ \\
    *Bin. hyp. dec. boundary: Quadratic in space of $\underline{x}$

    \textcolor{orange}{\textbf{How to find $\underline{\pi}_c, \underline{\mu}_c, \Sigma_c$:}} Given $n$ points gen. by GMM, then $n_c$ points $\{\underline{x}_1^c, \ldots, \underline{x}_{n_c}^c\}$ come from $\mathcal{N} (\underline{\mu}_c, \Sigma_c)$ \\
    $\hat{\pi}_c = \frac{n_c}{n}$, $\hat{\mu}_c = \frac{1}{n_c} \sum_{i=1}^{n} \underline{x}_i^c$, \\
    $\Sigma_c = \frac{1}{n_c} \sum_{i=1}^{n_c} (x_i^c - \hat{\mu}_c)(x_i^c - \hat{\mu}_c)^T$

    \textcolor{red}{\textbf{Guassian Estimation}} \textcolor{blue}{\textbf{ML Estimator for $\theta$:}} \\ 
    $\underline{X} \text{=} \{X_1,\ldots,X_n\}$, $X_i \text{=} \theta + Z_i$, $Z_i \sim \mathcal{N}(0, \sigma_i^2)$ (indep not iid) \\
    $\hat{\theta}_{\text{ML}} = \frac{\sum_{i=1}^n \frac{x_i}{\sigma_i^2}}{\sum_{i=1}^n \frac{1}{\sigma_i^2}}$ (weighted avg. of $\underline{x}$) \\
    *$\frac{1}{\sigma_i^2}$: Precision of $X_i$ (i.e. weight) \\
    *Larger $\sigma_i^2 \implies$ less weight on $X_i$ (less reliable measurement) \\
    *If $\sigma_i^2 = \sigma^2 \; \forall i$ (iid), then $\hat{\theta}_{\text{ML}} = \frac{1}{n} \sum_{i=1}^n x_i$ (sample mean) 

    \textcolor{blue}{\textbf{MAP Estimator for $\theta$:}} Prior $\Theta \sim \mathcal{N} (x_0, \sigma_0^2)$, indep. $\underline{Z}$ \\
    $\hat{\theta}_{\text{MAP}} = \frac{\sum_{i=0}^n \frac{x_i}{\sigma_i^2}}{\sum_{i=1}^n \frac{1}{\sigma_i^2}} = \frac{\frac{1}{\sigma_0^2}}{\frac{1}{\sigma_0^2} + \sum_{i=1}^n \frac{1}{\sigma_i^2}} x_0 + \frac{\sum_{i=1}^n \frac{1}{\sigma_i^2}}{\frac{1}{\sigma_0^2} + \sum_{i=1}^n \frac{1}{\sigma_i^2}} \hat{\theta}_{\text{ML}}$ \\
    *Gaussian prior $f_\Theta$ is equiv. to a prior meas. $x_0$ w/ $\sigma_0^2$. \\
    *As $n \rightarrow \infty$, $\hat{\theta}_{\text{MAP}} \rightarrow \hat{\theta}_{\text{ML}}$. As $\sigma_0^2 \rightarrow \infty$, $\hat{\theta}_{\text{MAP}} \rightarrow \hat{\theta}_{\text{ML}}$ 

    \textcolor{blue}{\textbf{SC MAP Estimator for $\underline{X}$ Given $\underline{Y}$:}} $\underline{W} = (\underline{X},\underline{Y}) \sim \mathcal{N} (\underline{\mu}, \Sigma)$ \\
    $\hat{\underline{x}}_{\text{MAP}}(\underline{y}) = \hat{\underline{x}}_{\text{LMS}}(\underline{y}) = \underline{\mu}_{\underline{X} \mid \underline{Y}}= \underline{\mu}_{\underline{X}} + \Sigma_{\underline{X} \underline{Y}} \Sigma_{\underline{Y} \underline{Y}}^{-1} (\underline{y} - \underline{\mu}_Y)$ \\
    *$\hat{\underline{x}}_{\text{MAP/LMS}}$: Linear fcn of $\underline{y}$ 

    \textcolor{darkgreen}{\textbf{Covar. Matrices:}} \\
    *$\Sigma_{\underline{X} \underline{X}} = \Sigma_{\underline{X}} = E\left[(\underline{X} - \underline{\mu}_{\underline{X}})(\underline{X} - \underline{\mu}_{\underline{X}})^T\right] \mid \Sigma_{\underline{Y} \underline{Y}} = \Sigma_{\underline{Y}}$ \\ 
    *$\Sigma_{\underline{X} \underline{Y}} = E\left[(\underline{X} - \underline{\mu}_{\underline{X}})(\underline{Y} - \underline{\mu}_{\underline{Y}})^T\right] \mid \Sigma_{\underline{Y} \underline{X}} = \Sigma_{\underline{X} \underline{Y}}^T$ \\
    \textcolor{darkgreen}{\textbf{Mean and Covar. Mat. of $\underline{X}$ Given $\underline{Y}$:}} \\
    *$\underline{\mu}_{\underline{X} \mid \underline{Y}} = E[\underline{X} \mid \underline{Y} = \underline{y}]$ \\
    *$\Sigma_{\underline{X} \mid \underline{Y}} = E\left[(\underline{X} - \underline{\mu}_{\underline{X} \mid \underline{Y}})(\underline{X} - \underline{\mu}_{\underline{X} \mid \underline{Y}})^T \mid \underline{Y} = \underline{y} \right] \\
    = \Sigma_{\underline{X}} - \Sigma_{\underline{X} \underline{Y}} \Sigma_{\underline{Y} \underline{Y}}^{-1} \Sigma_{\underline{Y} \underline{X}}$ \\ 
    *Since 2nd term is PDF, therefore, given obs. $\underline{Y} = \underline{y}$, we are always reducing uncertainty in $\underline{X}$.

    \textcolor{blue}{\textbf{LMMSE Estimator for $\underline{X}$ Given $\underline{Y}$:}} For non-Guassian $\underline{X}$, $\underline{Y}$, \\
    $\hat{\underline{x}}_{\text{LMMSE}}(\underline{y}) = \underline{\mu}_{\underline{X}} + \Sigma_{\underline{X} \underline{Y}} \Sigma_{\underline{Y} \underline{Y}}^{-1} (\underline{y} - \underline{\mu}_Y)$  

    \textcolor{red}{\textbf{Linear Guassian System:}} $\underline{Y} = A \underline{X} + \underline{b} + \underline{Z}$ \\
    *$A\underline{X} + \underline{b}$: channel distortion, $\underline{Z}$: Noise 
    
    \textcolor{blue}{\textbf{MAP/LMS Estimator for $\underline{X}$ Given $\underline{Y}$:}} \\
    $\hat{x}_{\text{MAP/LMS}} \text{=} \underline{\mu}_{\underline{X}} + \Sigma_{\underline{X}} A^T (A \Sigma_{\underline{X}} A^T \text{+} \Sigma_{\underline{Z}})^{-1} (\underline{y} - A \underline{\mu}_{\underline{X}} - \underline{b})$ \\
    $\hat{x}_{\text{MAP/LMS}} \text{=} \left(\Sigma_{\underline{X}}^{-1} + A^T \Sigma_{\underline{Z}}^{-1} A \right)^{-1} \left(A^T \Sigma_{\underline{Z}}^{-1} (\underline{y} - \underline{b}) + \Sigma_{\underline{X}}^{-1} \underline{\mu}_{\underline{X}} \right)$ 

    \textcolor{darkgreen}{\textbf{Covar. Mat of $\underline{X}$ Given $\underline{Y}=\underline{y}$:}} $\Sigma_{\underline{X} \mid \underline{y}} = \left(\Sigma_{\underline{X}}^{-1} + A^T \Sigma_{\underline{Z}}^{-1} A \right)^{-1}$



    }
\end{paracol}

\end{document}