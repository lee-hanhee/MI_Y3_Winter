\documentclass[5pt]{extarticle} % Note the extarticle document class
\usepackage[margin=0.2in]{geometry} % Set 0.3-inch margins
\usepackage{multicol}              % For multi-column support
\usepackage{lipsum}                % Dummy text generator (optional)
\usepackage{amssymb} % math symbols
\setlength{\parskip}{0ex}
\setlength\parindent{0pt} % no indent
%% optional packages -- documentation at ctan.org
\usepackage{graphicx}  % image handline
\usepackage{amsmath}   % enhanced equation environments
\usepackage{tikz}      % block diagrams
\usetikzlibrary{positioning}  % allow relative positioning of tikz elements
\usepackage{pgfplots}  % package for plots, based on tikz
\usepackage{hyperref}
\usepackage{paracol}             % Import paracol package
\usepackage{float} % for H in figure

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0} % Define the dark green color

\newcommand{\customFigure}[3][]{
    \vspace{-2em}
    \begin{figure}[H]
        \centering
        \includegraphics[width=#1\textwidth]{#2}
    \end{figure}
    \vspace{-2em}
}

\begin{document}

\begin{paracol}{3}
    {\tiny
    \textcolor{red}{\textbf{Intro:}} \textcolor{blue}{\textbf{Random Experiment:}} An outcome for each run.  

    \textcolor{blue}{\textbf{Sample Space $\Omega$:}} Set of all possible outcomes.

    \textcolor{blue}{\textbf{Event:}} Subsets of $\Omega$.

    \textcolor{blue}{\textbf{Prob. of Event A:}} $P(A) = \frac{\text{Number of outcomes in A}}{\text{Number of outcomes in } \Omega}$

    \textcolor{blue}{\textbf{Axioms:}} $P(A) \geq 0 \; \forall A \in \Omega$, $P(\Omega) = 1$, \\
    If $A \cap B = \emptyset$, then $P(A \cup B) = P(A) + P(B) \; \forall A, B \in \Omega$

    \textcolor{blue}{\textbf{Cond. Prob.}} $P(A|B) = \frac{P(A \cap B)}{P(B)} \\
    *P(A \cap B) = P(A|B) P(B) = P(B|A) P(A)$

    \textcolor{darkgreen}{\textbf{Independence:}} $P(A|B) = P(A) \Leftrightarrow P(A \cap B) = P(A) P(B)$

    \textcolor{blue}{\textbf{Total Prob. Thm:}} If $H_1, H_2, \ldots, H_n$ form a partition of $\Omega$, then $P(A) = \sum_{i=1}^n P(A|H_i) P(H_i)$.

    \textcolor{blue}{\textbf{Bayes' Rule:}} $P(H_k|A) \text{=} \frac{P(H_k \cap A)}{P(A)} \text{=} \frac{P(A|H_k) P(H_k)}{\sum_{i=1}^n P(A|H_i) P(H_i)}$ \\
    *Posteriori: $P(H_k|A)$, Likelihood: $P(A|H_k)$, Prior: $P(H_k)$ 

    \textcolor{red}{\textbf{1 RV:}} \textcolor{blue}{\textbf{CDF:}} $F_X(x) = P[X \leq x]$ 

    \textcolor{blue}{\textbf{PMF:}} $P_X(x_j) = P[X = x_j] \; j=1,2,\ldots$

    \textcolor{blue}{\textbf{PDF:}} $f_X(x) = \frac{d}{dx} F_X(x)$ \\
    *$P[a \leq X \leq b] = \int_a^b f_X(x) \, dx$ IS THIS CORRECT?

    \textcolor{blue}{\textbf{Cond. PMF:}} $P_X(x|A) = P[X = x|A] = \frac{P[X = x, A]}{P[A]}$ IS THIS CORRECT?

    \textcolor{blue}{\textbf{Cond. PDF:}} $f_{X}(x|A) = \frac{f_{X,A}(x, a)}{f_A(a)}$ IS THIS CORRECT?

    \textcolor{blue}{\textbf{Exp.:}} $E[h(X)] \text{=} \int_{-\infty}^{\infty} h(x) f_X(x) dx \mid \sum_{k=-\infty}^{\infty} h(k) P_X(x_i \text{=} k)$ \\

    \textcolor{darkgreen}{\textbf{Variance:}} $\sigma_X^2 = \text{Var}[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2$

    \textcolor{darkgreen}{\textbf{Cond. Exp.:}} $E[X|A] = \int_{-\infty}^{\infty} x f_X(x|A) \, dx$ 

    \textcolor{red}{\textbf{2 RVs:}}  \textcolor{blue}{\textbf{Joint PMF:}} $P_{X,Y}(x, y) = P[X = x, Y = y]$

    \textcolor{blue}{\textbf{Joint PDF:}} $f_{X,Y}(x, y) = \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x, y)$ \\
    *$P[(X, Y) \in A] = \int \int_{(x, y) \in A} f_{X,Y}(x, y) \, dx \, dy$

    \textcolor{blue}{\textbf{Exp.:}} $E[g(X, Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f_{X,Y}(x, y) \, dx \, dy$

    \textcolor{darkgreen}{\textbf{Correlation (Corr.):}} $E[XY]$

    \textcolor{darkgreen}{\textbf{Covar.:}} $\text{Cov}[X, Y] \text{=} E[(X - \mu_X)(Y - \mu_Y)] \text{=} E[XY] - E[X] E[Y]$

    \textcolor{darkgreen}{\textbf{Corr. Coeff.:}} $\rho_{X,Y} \text{=} E\left[ \left( \frac{X - \mu_X}{\sigma_X} \right) \left( \frac{Y - \mu_Y}{\sigma_Y} \right) \right] = \frac{\text{Cov}[X, Y]}{\sigma_X \sigma_Y}$

    \textcolor{blue}{\textbf{Marginal PMF:}} $P_X(x) = \sum_{j=1}^\infty P_{X,Y}(x, y_j)$

    \textcolor{blue}{\textbf{Marginal PDF:}} $f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, dy$

    \textcolor{blue}{\textbf{Cond. PMF:}} $P_{X|Y}(x|Y) = P[X = x|Y = y] = \frac{P_{X,Y}(x, y)}{P_Y(y)}$

    \textcolor{blue}{\textbf{Cond. PDF:}} $f_{X|Y}(x|y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}$

    \textcolor{blue}{\textbf{Bayes' Rule}} \\ 
    $f_{Y|X}(y|x) \text{=} \frac{f_{X,Y} (x,y)}{f_X (x)} \text{=} \frac{f_{X|Y} (x|y) f_Y (y)}{\int_{-\infty}^{\infty} f_{X|Y} (x|y') f_Y (y') \, dy'}$ \\
    *$P_{Y|X}(y|x) = \frac{P_{X,Y}(x, y)}{P_X(x)} = \frac{P_{X|Y}(x|y) P_Y(y)}{\sum_{j=1}^\infty P_{X|Y}(x|y_j) P_Y(y_j)}$

    \textcolor{blue}{\textbf{Ind.:}} $f_{X|Y}(x|y) = f_X(x) \; \forall y \Leftrightarrow f_{X,Y}(x, y) = f_X(x) f_Y(y) $ 
    *If independent, then uncorrelated.

    \textcolor{blue}{\textbf{Uncorrelated:}} $\text{Cov}[X, Y] = 0 \Leftrightarrow \rho_{X,Y} = 0$

    \textcolor{blue}{\textbf{Orthogonal:}} $E[XY] = 0$

    \textcolor{blue}{\textbf{Cond. Exp.:}} $E[Y] = E[E[Y|X]]$ or $E[E[h(Y)|X]]$ \\
    *$E[E[Y|X]]$ w.r.t. $X \mid E[Y|X]$ w.r.t. $Y$. 

    \textcolor{red}{\textbf{Estimation:}} Estimate unknown parameter $\theta$ from $n$ i.i.d. measurements $X_1, X_2, \ldots, X_n$, $\hat{\Theta}(\underline{X}) = g(X_1, X_2, \ldots, X_n)$

    \textcolor{blue}{\textbf{Estimation Error:}} $\hat{\Theta}(\underline{X}) - \theta$. 

    \textcolor{blue}{\textbf{Unbiased:}} $\hat{\Theta}(\underline{X})$ is unbiased if $E[\hat{\Theta}(\underline{X})] = \theta$. \\
    *\textbf{Asymptotically unbiased:} $\lim_{n \to \infty} E[\hat{\Theta}(\underline{X})] = \theta$.

    \textcolor{blue}{\textbf{Consistent:}} $\hat{\Theta}(\underline{X})$ is consistent if $\hat{\Theta}(\underline{X}) \rightarrow \theta$ as $n \to \infty$ or $\forall \epsilon >0, \lim_{n \to \infty} P[|\hat{\Theta}(\underline{X}) - \theta| < \epsilon] \rightarrow 1$.

    \textcolor{darkgreen}{\textbf{Sample Mean:}} $M_n = \frac{1}{n} S_n = \frac{1}{n} \sum_{i=1}^n X_i$. \\
    *Given a sequence of i.i.d. RVs, $X_1, X_2, \ldots, X_n$, $M_n$ is unbiased and consistent.

    \textcolor{blue}{\textbf{Chebychev's Inequality:}} $P[|X - E[X]| \geq \epsilon] \leq \frac{\text{Var}[X]}{\epsilon^2}$

    \textcolor{blue}{\textbf{Weak Law of Large \#s:}} $\lim_{n \to \infty} P[|M_n - \mu| < \epsilon] = 1 \; \forall \epsilon > 0$.

    \textcolor{red}{\textbf{ML Estimation:}} Choose parameter $\theta$ that is most likely to generate the obs. $x_1, x_2, \ldots, x_n$. \\
    *Disc: $\hat{\Theta} = \arg \max_\theta P_{\underline{X}} (\underline{x} | \theta) \overset{\text{log}}{\rightarrow} \hat{\theta} = \arg \max_\theta \sum_{i=1}^n \log P_X(x_i | \theta)$ \\
    *Cont: $\hat{\Theta} = \arg \max_\theta f_{\underline{X}} (\underline{x} | \theta) \overset{\text{log}}{\rightarrow} \hat{\theta} = \arg \max_\theta \sum_{i=1}^n \log f_X(x_i | \theta)$

    \textcolor{blue}{\textbf{Maximum A Posteriori (MAP) Estimation:}} \\
    *Disc: $\hat{\theta} = \arg \max_\theta P_{\Theta | \underline{X}} (\theta | \underline{x}) = \arg \max_\theta P_{\underline{X} | \Theta} (\underline{x} | \theta) P_\Theta (\theta)$\\
    *Cont: $\hat{\theta} = \arg \max_\theta f_{\Theta | \underline{X}} (\theta | \underline{x}) = \arg \max_\theta f_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta)$ \\
    *$f_{\Theta | \underline{X}} (\theta | \underline{x})$: Posteriori, $f_{\underline{X} | \Theta} (\underline{x} | \theta)$: Likelihood, $f_\Theta (\theta)$: Prior

    \textcolor{darkgreen}{\textbf{Bayes' Rule:}} $P_{\Theta | \underline{X}} (\theta | \underline{x}) = \begin{cases}
        \frac{P_{\underline{X} | \Theta} (\underline{x} | \theta) P_\Theta (\theta)}{P_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ disc.} \\
        \frac{f_{\underline{X} | \Theta} (\underline{x} | \theta) P_\Theta (\theta)}{f_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ cont.}
    \end{cases}$ \\
    $f_{\Theta | \underline{X}} (\theta | \underline{x}) = \begin{cases}
        \frac{P_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta)}{P_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ disc.} \\
        \frac{f_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta)}{f_{\underline{X}} (\underline{x})} & \text{if } \underline{X} \text{ cont.} 
    \end{cases}$ \\
    *Independent of $\theta$: $f_{\underline{X}} (\underline{x}) = \int_{-\infty}^{\infty} f_{\underline{X} | \Theta} (\underline{x} | \theta) f_\Theta (\theta) \, d\theta$ \\

    \textcolor{blue}{\textbf{Beta Prior}} $\Theta$ is a Beta R.V. w/ $\alpha,\beta>0$\\
    $f_\Theta (\theta) = \begin{cases}
        \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1} & \text{if } 0 < \theta < 1 \\
        0 & \text{otherwise}
    \end{cases}$ \\
    *$\Gamma(x) = \int_{0}^{\infty} t^{x-1} e^{-t} \, dt$

    \textcolor{darkgreen}{\textbf{Prop.:}} 1. $\Gamma(x+1) = x \Gamma(x)$. For $m \in \mathbb{Z}^+$, $\Gamma(m+1) = m!$. \\
    2. $\beta(\alpha,\beta) = \frac{(\alpha + \beta -1)!}{(\alpha - 1)! (\beta - 1)!} = \beta \binom{\alpha + \beta - 1}{\alpha - 1}$ \\
    3. Expected Value: $E[\Theta] = \frac{\alpha}{\alpha + \beta}$ for $\alpha, \beta > 0$ \\
    4. Mode (max of PDF): $\frac{\alpha - 1}{\alpha + \beta - 2}$ for $\alpha, \beta > 1$ \\

    \textcolor{orange}{\textbf{Drawing Beta Dist.}} 1. Label $x$-axis from 0 to 1. 2. Identify mode. \\ 
    3. Determine shape based on $\alpha$ and $\beta$: $\alpha = \beta = 1$ (uniform), $\alpha = \beta > 1$ (bell-shaped, peak at 0.5), $\alpha = \beta < 1$ (U-shaped w/ high density near 0 and 1), $\alpha > \beta$ (left-skewed), $\alpha < \beta$ (right-skewed).

    \textcolor{blue}{\textbf{Least Mean Squares (LMS) Estimation:}} Assume prior $P_\Theta (\theta)$ or $f_\Theta (\theta)$ w/ obs. $\underline{X} = \underline{x}$. \\
    *$\hat{\theta} = g(\underline{x}) = \mathbb{E} [\Theta | \underline{X} = \underline{x}] \; \mid \; \hat{\Theta} = g(\underline{X}) = \mathbb{E} [\Theta | \underline{X}]$ \\

    \textcolor{blue}{\textbf{Uniform PDF}} $f_X(x) = \begin{cases}
        \frac{1}{b - a} & \text{if } a \leq x \leq b \\
        0 & \text{otherwise}
    \end{cases}$ \\
    *$E[X] = \frac{a + b}{2}$, $\text{Var}[X] = \frac{(b - a)^2}{12}$

    \textcolor{blue}{\textbf{Conditional Exp.}} $E[X|Y] = \int_{-\infty}^{\infty} x f_{X|Y} (x|y) \, dx$

    \textcolor{red}{\textbf{Binary Hyp. Testing:}} $H_0$: Null Hyp., $H_1$: Alt. Hyp. \\

    \textcolor{blue}{\textbf{TI Err. (False Rejection):}} Reject $H_0$ when $H_0$ is true. \\
    *$\alpha(R) = P[\underline{X} \in R \mid H_0]$ 

    \textcolor{blue}{\textbf{TII Err. (False Accept.):}} Accept $H_0$ when $H_1$ is true. \\
    *$\beta(R) = P[\underline{X} \in R^c \mid H_1]$ \\

    \customFigure[0.11]{../Images/TB_1.png}

    \textcolor{blue}{\textbf{Likelihood Ratio Test:}} For each value of $\underline{x}$, \\
    *$L(\underline{x}) = \frac{P_{\underline{X}} (\underline{x}|H_1)}{P_{\underline{X}} (\underline{x}|H_0)} \overset{H_1}{\underset{H_0}{\gtrless}} 1 \text{ or } \xi$ \\
    *\textbf{MLT:} $1$, \textbf{LRT:} $\xi$

    \textcolor{blue}{\textbf{Neyman-Pearson Lemma:}} Given a false rejection prob. ($\alpha$), the LRT offers the smallest possible false accept. prob. ($\beta$), and vice versa. \\
    *LRT produces ($\alpha,\beta$) pairs that lie on the efficient frontier.

    \customFigure[0.1]{../Images/L10_0.png}{}
    
    % Given $L(X), \xi$ so that \\ 
    % $P[L(X) > \xi \mid H_0] = \alpha$ and $P[L(X) \leq \xi \mid H_1] = \beta$,
    % then for any other test (rejection region) w/ $P[X \in R \mid H_0] \leq \alpha$, then $P[X \notin R \mid H_1] \geq \beta$.

    % \textcolor{darkgreen}{\textbf{Sig. Testing:}} Given $X_1, \ldots, X_n$, find a rejection reg. so a level of T1 err. is achieved: $P[\text{Reject } H_0 \mid H_0] = \alpha$. \\
    % *$\alpha$: Significance level, $1 - \alpha$: Confidence level.

    \textcolor{blue}{\textbf{Bayesian Hyp. Testing:}} \textcolor{darkgreen}{\textbf{MAP Rule:}} \\ 
    $L(\underline{x}) = \frac{p_{\underline{X}} (\underline{x} | H_1)}{p_{\underline{X}} (\underline{x} | H_0)} \overset{H_1}{\underset{H_0}{\gtrless}} \frac{P[H_0]}{P[H_1]}$ 
    
    % Selects hyp. w/ higher a posteriori prob, reject $H_0$ if: \\
    % $p(H_1 \mid \underline{x}) \overset{H_1}{\underset{H_0}{\gtrless}} p(H_0 \mid \underline{x}) \; \mid \; f(H_1 \mid \underline{x}) \overset{H_1}{\underset{H_0}{\gtrless}} f(H_0 \mid \underline{x})$ \\
    % $p_{\underline{X}} (\underline{x} | H_1) P[H_1] \overset{H_1}{\underset{H_0}{\gtrless}} p_{\underline{X}} (\underline{x} | H_0) P[H_0]$ \\
    
    % $p(\underline{x} \mid H_1)\pi_j \overset{H_1}{\underset{H_0}{\gtrless}} p(\underline{x} \mid H_0)\pi_0 \; \mid \; f(\underline{x} \mid H_1)\pi_j \overset{H_1}{\underset{H_0}{\gtrless}} f(\underline{x} \mid H_0)\,\pi_0$ \\
    % *$p(H_j \mid \underline{x}) \text{=} \frac{p_{\underline{X}} (\underline{x} \mid H_j) P [H_j]}{p_{\underline{X}} (\underline{x} \mid H_0) P[H_0] + p_{\underline{X}} (\underline{x} \mid H_1) P[H_1]}$: A posteriori 

    \textcolor{darkgreen}{\textbf{Min. Cost Bayes' Dec. Rule:}} $C_{ij}$ is cost of choosing $H_j$ when $H_i$ is true. Given obs. $\underline{X} = \underline{x}$, the exp. cost of choosing \\ 
    $H_j$ is $A_j (\underline{x}) = \sum_{i=0}^1 C_{ij} \, P[H_i | \underline{X} = \underline{x}]$. \\
    
    % $\text{Min. Cost Detection} = \sum_{i=0}^1 \sum_{j=0}^1 C_{ij} \, P[\text{decide } j \mid H_i] \, \pi_i$ \\
    % *$j=0$: Accept $H_0$, $j=1$: Reject $H_0$

    \textcolor{darkgreen}{\textbf{Min. Cost Dec. Rule:}} 
    $L(\underline{x}) = \frac{P_{\underline{X}} (\underline{x} \mid H_1)}{P_{\underline{X}} (\underline{x} \mid H_0)}\overset{H_1}{\underset{H_0}{\gtrless}} \frac{(C_{01} - C_{00}) P[H_0]}{(C_{10} - C_{11})P[H_1]}$. \\
    *$C_{01}$: False accept. cost, $C_{10}$: False reject. cost.

    \textcolor{blue}{\textbf{Naive Bayes Assumption:}} Assume $X_1 \ldots, X_n$ (features) are ind., then $p_{\underline{X} \mid \Theta} (\underline{x} \mid \theta) \Pi_{i=1}^n p_X (x_i \mid \theta)$.

    \textbf{Notation:} $P_{\underline{X} | \Theta} (\underline{x} | \theta)$, only put RVs in subscript, not values. $P_{\underline{X}} (\underline{x} | H_i)$, didn't put $H$ in subscript b/c it's not a RV.

    \textbf{Binomial} \# of successes in $n$ trials, each w/ prob. $p$ \\
    $ b(x \mid n, p) = \binom{n}{x} p^x (1 - p)^{n - x}, x = 0, 1, 2, \dots $ \\
    *$ E[X] = \mu = np \; \mid \; Var(X) = \sigma^2 = np(1 - p) $

    \textbf{Multinomial} \# of $x_i$ successes in $n$ trials, each w/ prob. $p_i$ \\
    $ f(x_i \mid p_i \forall i, n) = \frac{n!}{x_1! \dots x_m!} p_1^{x_1} \dots p_m^{x_m} $  \\
    *$ \sum_{i} x_i = n $, and $ \sum_{i=1}^{m} p_i = 1 $ \\
    *$ E[X_i] = \mu_i = np_i \; \mid \; Var(X_i) = \sigma^2_i = np_i(1 - p_i) $

    \textbf{Hypergeometric} \# of successes in $n$ draws from $N$ items, $k$ of which are successes \\
    $ h(x \mid N, n, k) = \frac{\binom{k}{x} \binom{N-k}{n-x}}{\binom{N}{n}}$ \\
    *$\max\{0, n - (N - k)\} \leq x \leq \min\{n, k\} $ \\
    *$E[X] = \mu = \frac{nk}{N} \; \mid \; Var(X) = \sigma^2 = \frac{N-n}{N-1} \cdot n \cdot \frac{k}{N} \cdot \left(1 - \frac{k}{N} \right) $

    \textbf{Negative Binomial} \# of trials until $k$ successes, each w/ prob. $p$ \\
    $ b^*(x \mid k, p) = \binom{x-1}{k-1} p^k (1 - p)^{x - k}$ \\
    *$x \geq k, x = k, k+1, \dots $ \\
    *$ E[X] = \mu = \frac{k}{p} \; \mid \; Var(X) = \sigma^2 = \frac{k(1 - p)}{p^2} $

    \textbf{Geometric} \# of trials until 1st success, each w/ prob. $p$ \\
    $ g(x \mid p) = p(1 - p)^{x - 1}$ \\
    *$x \geq 1, x = 1, 2, 3, \dots $ \\
    *$ E[X] = \mu = \frac{1}{p} \; \mid \; Var(X) = \sigma^2 = \frac{1 - p}{p^2} $

    \textbf{Poisson} \# of events in a fixed interval w/ rate $\lambda$ \\
    $ p(x \mid \lambda t) = \frac{e^{-\lambda t} (\lambda t)^x}{x!}$ \\
    *$x \geq 0, x = 0, 1, 2, \dots $ \\
    *$ E[X] = \mu = \lambda t \; \mid \; Var(X) = \sigma^2 = \lambda t $

    \textcolor{orange}{\textbf{Gaussian to Q Fcn:}} 1. Find $Q(x) = \frac{1}{\sqrt{2\pi}} \int_x^\infty e^{-t^2/2} \, dt$. \\
    2. Use table to find $Q(x)$ for $x \geq 0$. \\


    }
\end{paracol}

\end{document}