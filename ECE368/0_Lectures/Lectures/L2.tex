\begin{faq}

\end{faq}

\subsection{Total Probability}
\begin{definition}
    If $H_1,\ldots,H_n$ form a partition of $\Omega$, then
    \begin{equation}
        P[A] = \sum_{i=1}^{n} P[A|H_i]P[H_i]
    \end{equation}
    \customFigure[0.5]{../Images/L1_2.png}{Total Probability}
\end{definition}

\subsection{Bayes' Rule}
\begin{definition}
    \begin{equation}
        P[H_k|A] = \frac{P[H_k \cap A]}{P[A]} = \frac{P[A|H_k]P[H_k]}{\sum_{i=1}^{n} P[A|H_i]P[H_i]}
    \end{equation}
\end{definition}

\subsubsection{Posteriori Probability, Priori Probability (Prior), Likelihood}
\begin{definition}
    \begin{itemize}
        \item \textbf{Posteriori:} $P[H_k|A]$.
        \item \textbf{Priori:} $P[H_k]$.
        \item \textbf{Likelihood:} $P[A|H_k]$.
    \end{itemize}
\end{definition}

\begin{example}
    Suppose a lie detector is 95\% accurate, i.e. $P[\text{'out=truth'}|\text{'in=truth'}] = 0.95$ and $P[\text{'out=lie'}|\text{'in=lie'}] = 0.95$. It says that Mr. Ernst is lying. What is the probability Mr. Ernst is actually lying.
    \begin{itemize}
        \item \textbf{Observation:} $A = \text{'out=lie'}$.
        \item \textbf{Hypothesis:} $H_0 = \text{'in=lie'}$ and $H_1 = \text{'in=truth'}$.
        \item \textbf{Solution:} $P[H_0|A] = \frac{P[A|H_0]P[H_0]}{P[A|H_0]P[H_0] + P[A|H_1]P[H_1]} = \frac{0.95 \times P[H_0]}{0.95 \times P[H_0] + 0.05 \times (1 - P[H_0])}$.
        \item \textbf{$H_0 = 0.01$:} i.e. 1\% of the population are liars, then $P[H_0|A] = \frac{0.95 \times 0.01}{0.95 \times 0.01 + 0.05 \times 0.99} = 0.16$.
    \end{itemize}
\end{example}

\begin{warning}
    Need to know priori probability.
\end{warning}

\subsubsection{Interpretation of Bayes' Rule}
\begin{notes}
    Taking one component of the total probability and normalizing it by the sum of all components.
\end{notes}

\subsection{Random Variables}
\begin{motivation} \textbf{Coin Toss}
    Mapping of each outcome to a real number
    \begin{itemize}
        \item $w \in \Omega$ is the outcome of a coin toss, and $X$ is the RV, so $H \rightarrow 0$ and $T \rightarrow 1$.
    \end{itemize}
    \customFigure[0.5]{../Images/L1_3.png}{Random Variables}
    \begin{itemize}
        \item Mapping is deterministic function. RV is not random or variable.
    \end{itemize}
\end{motivation}

\begin{definition}
    Mapping from $\Omega$ to $\mathbb{R}$.
\end{definition}

\subsection{Distribution of RV}

\subsubsection{Cumulative Distribution Function (CDF) of RV}
\begin{definition}
    \begin{equation}
        F_X(x) \equiv P[X \leq x]
    \end{equation}
\end{definition}

\subsubsection{Discrete RV Probability Mass Function (PMF)}
\begin{definition}
    \begin{equation}
        P_X(x_j) \equiv P[X=x_j] \quad j=1,2,3,\ldots 
    \end{equation}
\end{definition}

\begin{example} \textbf{Binonmial RV w/ $(n,p)$}
    \begin{equation}
        P_X(k) = \binom{n}{k} p^k (1-p)^{n-k}
    \end{equation}
    \customFigure[0.5]{../Images/L1_4.png}{Binomial RV}
\end{example}

\subsubsection{Continuous RV Probability Density Function (PDF)}
\begin{definition}
    \begin{equation}
        f_X(x) \equiv \frac{d}{dx} F_X(x)
    \end{equation}
    \begin{equation}
        P[x < X < x+dx] = f_X (x) dx
    \end{equation}
\end{definition}

\begin{example} \textbf{Gaussian RV w/ ($\mu,\sigma^2$)}
    \begin{equation}
        f_X (x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
    \end{equation}
    \customFigure[0.5]{../Images/L1_5.png}{Gaussian RV}
    \begin{itemize}
        \item $P[X \in A] = \int_{A} f_X(x) dx$.
    \end{itemize}
\end{example}

\begin{notes}
    Discrete RV has pdf w/ $\delta$ functions.
\end{notes}

\subsubsection{Conditional PMF/PDF}
\begin{definition}
    \begin{equation}
        P_X (x|A) 
    \end{equation}
    \begin{equation}
        f_X (x|A)
    \end{equation}
\end{definition}

\begin{example} \textbf{Continuous}
    \begin{equation}
        f(x|X>a) = \begin{cases}
            \frac{f_X(x)}{P[X>a]} & \text{if } x > a \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation}
\end{example}

\begin{example} \textbf{Geometric RV}
    Geometric RV $X$ w/ success probability $p$ 
    \begin{equation}
        P_X(k) = (1-p)^{k-1}p
    \end{equation}

    \begin{itemize}
        \item \textbf{Memoryless Property:} $P_X[k|X > m] = \frac{p(1-p)^{k-1}}{(1-p)^m} = p(1-p)^{k-m-1}$. 
        \begin{itemize}
            \item So it only cares about the additional trials (i.e. same as resetting after $m$ trials).
        \end{itemize}
    \end{itemize}
    
\end{example}

\subsection{Expected Values}
\begin{definition}
    \begin{equation}
        E[X] = \int_{-\infty}^{\infty} x f_X(x) dx \overset{\text{If int. values}}{=} \sum_{k=-\infty}^{\infty} k f_X(k)
    \end{equation}
    \begin{equation}
        E[h(X)] = \int_{-\infty}^{\infty} h(x) f_X(x) dx \overset{\text{If int. values}}{=} \sum_{k=-\infty}^{\infty} h(k) f_X(k)
    \end{equation}
    \begin{equation}
        \text{Var}[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2
    \end{equation}
    \begin{equation}
        E[X|A] = \int_{-\infty}^{\infty} x f_{X}(x|A) dx
    \end{equation}
\end{definition}

\begin{example} \textbf{Lottery Ticket (Geometric RV)}
    \begin{enumerate}
        \item \textbf{Given:} Buying one lottery ticket per week
        \begin{itemize}
            \item Each ticket has $10^{-7} = p$ chance of winning the jackpot.
            \item $X = \text{'\# of weeks to win jackpot'}$.
        \end{itemize}
        \item \textbf{Problem:} What is the expected number of weeks to win the jackpot?
        \item \textbf{Solution:} $E[X] = \sum_{k=1}^{\infty} k(1 - p)^{k-1}p = \ldots = \frac{1}{p} = 10^7$ weeks.
        \item \textbf{Extension (Memoryless Property):} If I have already played for 999999 weeks, what is the expected number of weeks to win the jackpot? $E[X-999999|X>999999] = E[X] = 10^7$ weeks.
    \end{enumerate}
\end{example}