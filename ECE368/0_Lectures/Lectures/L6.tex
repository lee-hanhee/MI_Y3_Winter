\subsection{MLE for Categorical Random Variables}
\begin{example}
    \begin{enumerate}
        \item We say that $X \sim \text{Cat}(\underline{\theta})$ if 
        \[
        P[X = m] = \theta_m, \quad m = 1, 2, \dots, M.
        \]
        \begin{itemize}
            \item Going from 2 to $M$ categories is a generalization of the Bernoulli distribution.
        \end{itemize}
        The parameter $\underline{\theta}$ is a vector:
        \[
        \underline{\theta} = 
        \begin{bmatrix}
        \theta_1 \\
        \theta_2 \\
        \vdots \\
        \theta_M
        \end{bmatrix},
        \]
        such that $\theta_m \geq 0$ and $\sum_{m=1}^M \theta_m = 1.$
    
        \item Given $n$ i.i.d. observations $X_1, \dots, X_n$, we aim to find the maximum likelihood estimator (MLE) of $\underline{\theta}$.
    
        \item Define $n_m$ as the number of observations that equal $m$:
        \[
        n_m = \sum_{i=1}^n 1(x_i = m),
        \]
        where $1(x_i = m)$ is the indicator function. Note that $\sum_{m=1}^M n_m = n.$
    
        \item The likelihood function is:
        \[
        p_{\underline{X}}(\underline{x} \mid \underline{\theta}) = \prod_{m=1}^M \theta_m^{n_m}.
        \]
        \begin{itemize}
            \item Similar to the Bernoulli distribution, but with $M$ categories.
        \end{itemize}
        Taking the log, we get:
        \[
        \log p_{\underline{X}}(\underline{x} \mid \underline{\theta}) = \sum_{m=1}^M n_m \log \theta_m.
        \]
    
        \item To find the optimal $\underline{\theta}$, we minimize the negative log-likelihood:
        \[
        \min_{\underline{\theta}} -\sum_{m=1}^M n_m \log \theta_m,
        \]
        subject to the constraints $\theta_m \geq 0$ for $1 \leq m \leq M$ and $\sum_{m=1}^M \theta_m = 1.$
    
        \item Solving this optimization problem, the MLE is:
        \[
        \hat{\Theta}_m = \frac{N_m}{n} = \frac{\sum_{i=1}^{n} 1(X_i = n)}{n}, \quad \underline{\hat{\Theta}} = 
        \begin{bmatrix}
        \frac{N_1}{n} \\
        \vdots \\
        \frac{N_m}{n}
        \end{bmatrix}.
        \]
    \end{enumerate}
\end{example}
\newpage

\subsection{MLE for Gaussian Random Variables}
\begin{example}
    \begin{enumerate}
        \item Given $n$ i.i.d. observations $X_1, \dots, X_n$ of a Gaussian random variable with parameters $(\mu, \sigma^2)$, we aim to find the maximum likelihood estimators (MLEs) of $\mu$ and $\sigma^2$.

        \begin{align*}
        f_{\underline{X}}(\underline{x} | \mu, \sigma^2) &= \prod_{i=1}^n \frac{1}{\sqrt{2\pi} \sigma} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right), \\
        \log f_{\underline{X}}(\underline{x} | \mu, \sigma^2) &= \sum_{i=1}^n \left(-\frac{(x_i - \mu)^2}{2\sigma^2} - \log \sigma - \log \sqrt{2\pi}\right).
        \end{align*}
    
        \item To find $\mu$, take the derivative of the log-likelihood with respect to $\mu$ and set it to zero:
        \begin{align*}
        0 &= \frac{\partial}{\partial \mu} \sum_{i=1}^n \left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right), \\
        0 &= \frac{1}{n} \sum_{i=1}^n x_i - \mu, \\
        \mu &= \frac{1}{n} \sum_{i=1}^n x_i.
        \end{align*}
    
        \item To find $\sigma^2$, take the derivative of the log-likelihood with respect to $\sigma^2$ and set it to zero:
        \begin{align*}
        0 &= \frac{\partial}{\partial \sigma^2} \sum_{i=1}^n \left(-\frac{(x_i - \mu)^2}{2\sigma^2} - \frac{1}{2} \log \sigma^2\right), \\
        0 &= -\frac{1}{2} \sum_{i=1}^n \left(\frac{(x_i - \mu)^2}{\sigma^4}\right) + \frac{1}{2\sigma^2}, \\
        \sigma^2 &= \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2.
        \end{align*}
    
        \item Thus, the MLEs are:
        \begin{align*}
        \hat{\mu} &= \frac{1}{n} \sum_{i=1}^n X_i, \quad \text{(sample mean)} \\
        \hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n (X_i - \hat{\mu})^2. \quad \text{(sample variance)}
        \end{align*}
        \begin{itemize}
            \item Note: The sample variance is biased, so we often use $\frac{1}{n-1}$ instead of $\frac{1}{n}$ to make it unbiased.
            \item Note: The sample mean is unbiased.
        \end{itemize}
    \end{enumerate}
\end{example}

\subsection{Will the Sun Rise Tomorrow? (Laplace's Problem)}
\begin{example}
    \begin{itemize}
        \item Observation: The Sun has risen for $n$ consecutive days. Estimate the probability that it will rise tomorrow.
        \item Model: Assume $n$ i.i.d. Bernoulli random variables $X_1, \dots, X_n$ with $P[X_i = 1] = \theta$.
    \end{itemize}
\end{example}
\subsubsection{Frequentist Approach}
\begin{example}
    \begin{enumerate}
        \item The Maximum Likelihood Estimator (MLE) is:
        \[
        \hat{\theta} = \frac{K}{n} = \frac{\sum_{i=1}^n X_i}{n}.
        \]
        \item If $K = n$ (i.e., the Sun has risen every day so far), then:
        \[
        \hat{\theta} = \frac{n}{n} = 1.
        \]
        \item Conclusion: The Sun will rise tomorrow with probability $1$, regardless of what $n$ is, based on the Frequentist approach. 
        \begin{itemize}
            \item This doesn't make sense b/c if $n=1$ then we are assuming 100\% it will rise based on one observation.
        \end{itemize}
    \end{enumerate}
\end{example}

\subsubsection{Bayesian Approach}
\begin{example}
    \begin{enumerate}
        \item Assume that $\theta$ is not fixed but drawn from a uniform distribution in $[0, 1]$. This means that the probability of the Sun rising is based on a uniform distribution.
        \item We want to find the probability that the sun will rise tomorrow given that it has risen for $n$ consecutive days:
        \[
        P[X_{n+1} = 1 | X_1 = 1, \dots, X_n = 1].
        \]
        Using Bayes' Theorem:
        \[
        P[X_{n+1} = 1 | X_1 = 1, \dots, X_n = 1] = \frac{P[X_1 = 1, \dots, X_{n+1} = 1]}{P[X_1 = 1, \dots, X_n = 1]}.
        \]
        \item Compute $P[X_1 = 1, \dots, X_n = 1]$:
        \[
        P[X_1 = 1, \dots, X_n = 1] = \int_0^1 P[X_1 = 1, \dots, X_n = 1 | \Theta = \theta] f_\Theta(\theta) \, d\theta.
        \]
        \begin{itemize}
            \item The joint probability is calculated by integrating the product of the likelihood and the prior (i.e. marginalizing over $\theta$).
            \item The likilihood becomes $\theta^n$ because the observations are i.i.d.
            \item The prior is uniform, so $f_\Theta(\theta) = 1$.
        \end{itemize}
        Since $f_\Theta(\theta) = 1$ (uniform prior) and $P[X_1 = 1, \dots, X_n = 1 | \Theta = \theta] = \theta^n$, we have:
        \[
        P[X_1 = 1, \dots, X_n = 1] = \int_0^1 \theta^n \, d\theta = \frac{1}{n+1}.
        \]
        \item Compute $P[X_1 = 1, \dots, X_{n+1} = 1]$ similarly:
        \[
        P[X_1 = 1, \dots, X_{n+1} = 1] = \int_0^1 \theta^{n+1} \, d\theta = \frac{1}{n+2}.
        \]
        \item Combine results:
        \[
        P[X_{n+1} = 1 | X_1 = 1, \dots, X_n = 1] = \frac{\frac{1}{n+2}}{\frac{1}{n+1}} = \frac{n+1}{n+2}.
        \]
        \item Conclusion: As $n$ increases, the probability approaches $1$, providing more certainty with more data.
    \end{enumerate}
\end{example}

\subsection{Sample Mean is Not Always an ML Estimator}
\begin{example}
    Given an unknown voltage \( x \), we measure it using a voltmeter that outputs a random reading \( Y \) that is uniform in \( [0, x] \). Suppose we make \( n \) i.i.d. measurements \( Y_1, \dots, Y_n \) and wish to estimate \( \mu = \frac{x}{2} = \mathbb{E}[Y] \).
    \begin{enumerate}
        \item \textbf{Sample Mean:} 
        \[
        \hat{\mu} = \frac{1}{n} \sum_{i=1}^n Y_i \quad \cdots (1)
        \]

        \item \textbf{To Find the ML Estimator:}
        \begin{align*}
            f_{\mathbf{Y}}(\mathbf{y} \mid \mu) &= \prod_{i=1}^n f_Y(y_i \mid \mu) \\
            &= \prod_{i=1}^n \frac{1}{2\mu} \cdot 1(0 \leq y_i \leq 2\mu) \\
            &= \frac{1}{(2\mu)^n} \prod_{i=1}^n 1(0 \leq y_i \leq 2\mu).
        \end{align*}
        
        The likelihood is maximized for:
        \[
        \arg\max_{\mu} f_{\mathbf{Y}}(\mathbf{y} \mid \mu) = \max_{1 \leq i \leq n} \frac{1}{2} Y_i.
        \]

        Therefore:
        \[
        \hat{\mu} = \max_{1 \leq i \leq n} \frac{1}{2} Y_i \quad \cdots (2)
        \]

        \item Clearly, \( (1) \neq (2) \).
    \end{enumerate}
\end{example}

