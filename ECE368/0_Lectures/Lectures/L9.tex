
\begin{definition}
    \textbf{Assume prior:} $P_\Theta(\theta) \text{ or } f_\Theta(\theta)$ w/ observations: \( \bar{X} = x \).
    \begin{align*}
        \hat{\theta} &= g(\underline{x}) = \mathbb{E}[\Theta \mid \underline{X} = \underline{x}] \\
        \text{or } \hat{\Theta} &= g(\underline{X}) = \mathbb{E}[\Theta \mid \underline{X} ].
    \end{align*}
\end{definition}

\begin{notes}
    \begin{enumerate}
    \item \textbf{MAP vs. LMS Estimators:}
    \begin{itemize}
        \item \textbf{MAP:} Use the most probable \( \theta \) given \( x \).
        \item \textbf{LMS:} Use the expected value (conditional on \( \bar{X} = x \)) of \( \Theta \), i.e., the "Conditional Expectation Estimator."
    \end{itemize}

    \item \textbf{Unbiasedness of LMS Estimator:}
    \begin{align*}
        \mathbb{E}[\hat{\Theta}] &= \mathbb{E}[\mathbb{E}[\Theta \mid \underline{X}]] = \mathbb{E}[\Theta], \\
        \implies \mathbb{E}[\hat{\Theta} - \Theta] &= 0.
    \end{align*}

    \item \textbf{LMS Estimator Minimizes Conditional MSE:}
    \[
    \mathbb{E}\big[(\Theta - \hat{\Theta})^2 \mid \underline{X} = \underline{x} \big].
    \]
    \textbf{Proof:}
    \begin{enumerate}
        \item First, suppose no observations: \( \hat{\Theta} \) is a constant. So we want:
        \begin{align*}
            \hat{\Theta} &= \arg\min_c \mathbb{E}\big[(\Theta - c)^2\big], \\
            0 &= \frac{d}{dc} \big[ -2\mathbb{E}[\Theta] + 2c \big], \\
            c &= \mathbb{E}[\Theta].
        \end{align*}

        \item Alternate view:
        \begin{align*}
            \mathbb{E}\big[(\Theta - c)^2\big] &= \mathrm{Var}[\Theta - c] + \mathbb{E}[\Theta - c]^2, \\
            &= \mathrm{Var}[\Theta] + (\mathbb{E}[\Theta] - c)^2.
        \end{align*}
        To minimize: Set bias \( \mathbb{E}[\Theta] - c \) to zero, while for variance, we have no control.

        \item Now, with observations \( \underline{X} = \underline{x} \) (i.e. given): $\hat{\theta} = g(\underline{x})$
        \begin{align*}
            \mathbb{E}\big[(\Theta - g(\underline{x}))^2 \mid \underline{X} = \underline{x}\big] &= \mathrm{Var}[\Theta \mid \underline{X} = \underline{x}] + (\mathbb{E}[\Theta \mid \bar{X} = \underline{x}] - g(\underline{x}))^2.
        \end{align*}
        To minimize: Set \( (g(x) - \mathbb{E}[\Theta \mid \underline{X} = \underline{x}])^2 = 0 \) since we have no control over the variance.
        \item Conclusion:
        \[
        \hat{\theta} = g(x) = \mathbb{E}[\Theta \mid \underline{X} = \underline{x}].
        \]
    \end{enumerate}
    \end{enumerate}
\end{notes}

\subsection{Example: Prior Coin Toss Problem}
\begin{example}
    
    \begin{align*}
        \hat{\theta}_{\text{LMS}} &= \mathbb{E}[\Theta \mid X = k] \\
        &= \frac{k + \alpha}{n + \alpha + \beta}.
    \end{align*}
\end{example}

\subsection{Example: Prior Voltage Problem}
\begin{example}
    \begin{enumerate}
        \item \textbf{Setup:}
        \begin{itemize}
            \item Unknown voltage \( \Theta \).
            \item Prior: \( \Theta \sim \text{Uniform}[0, 1] \).
            \item Volt meter reading \( Y \) given \( \Theta = \theta \): \( Y \sim \text{Uniform}[0, \Theta] \).
            \item Independent measurements: \( Y_1, \dots, Y_n \) given \( \theta \).
        \end{itemize}

        \item \textbf{Likelihood:}
        \begin{align*}
            f_{\underline{Y} \mid \Theta}(\underline{y} \mid \theta) &= \prod_{i=1}^n f_{Y \mid \Theta}(y_i \mid \theta) \\
            &= \frac{1}{\theta^n} \cdot 1(\theta \geq \max_{1 \leq i \leq n} y_i).
        \end{align*}

        \item \textbf{Posterior:}
        \begin{align*}
            f_{\Theta \mid \underline{Y}}(\theta \mid \underline{y}) &= \frac{\frac{1}{\theta^n} \cdot \mathbf{1}(\theta \geq \max_{1 \leq i \leq n} y_i) \mathbf{1} (0 \leq \theta \leq 1)}{f_{\underline{Y}}(\underline{y})}.
        \end{align*}

        \item \textbf{Estimators:}
        \begin{itemize}
            \item ML/MAP:
            \[
            \hat{\theta} = \max_{1 \leq i \leq n} y_i.
            \]

            \item LMS:
            \[
            \hat{\theta} = \mathbb{E}[\Theta \mid \underline{Y} = \underline{y}] = \int_{-\infty}^\infty \theta f_{\Theta \mid \underline{Y}}(\theta \mid \underline{y}) d\theta.
            \]
        \end{itemize}

        \item \textbf{Derivation for LMS:}
        \begin{enumerate}
            \item We need $f_{\underline{Y}} (\underline{y}) = \int_{-\infty}^{\infty} \frac{1}{\theta^n} \cdot \mathbf{1}(\theta \geq \max_{1 \leq i \leq n} y_i) \mathbf{1} (0 \leq \theta \leq 1) d\theta $
            \item Compute \( f_{\underline{Y}} (\underline{y}) \) for \( n = 1 \):
            \begin{align*}
                f_{\underline{Y}} (\underline{y}) &= \int_y^1 \frac{1}{\theta} d\theta, \quad 0 \leq y \leq 1 \\
                &= \ln(\theta) \Big|_y^1 \\
                &= -\ln(y).
            \end{align*}

            \item Compute \( \hat{\theta} \) for \( n = 1 \):
            \begin{align*}
                \hat{\theta} &= \int_{-\infty}^{\infty} \theta \frac{\frac{1}{\theta} \mathbf{1}(\theta \geq y) \cdot \mathbf{1} (0 \leq \theta \leq 1)}{- \ln y} d\theta \\
                &= \frac{1}{-\ln(y)} \int_y^1 d\theta \\
                &= \frac{y-1}{ln(y)} 
            \end{align*}
        \end{enumerate}

        \item \textbf{Graphical Interpretation:}
        \customFigure[0.5]{../Images/L9_0.png}{}
    \end{enumerate}
\end{example}